[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aurélien Callens",
    "section": "",
    "text": "Je m’appelle Aurélien Callens, passionné par la science des données, le code… et la pêche ! Après une thèse en mathématiques appliquées dédiée à la modélisation des risques côtiers, j’ai poursuivi mon parcours en start-up AgTech, où j’ai dirigé des projets de R&D en fusion de données géospatiales et deep learning appliqué à la météorologie.\nEn tant que Data Scientist freelance, je mets mon expertise en analyse de données, Machine Learning et Deep Learning au service des entreprises. Qu’il s’agisse de données environnementales, géospatiales ou industrielles, j’accompagne mes clients de la recherche à l’industrialisation pour transformer leurs données en solutions concrètes et impactantes.\n\n\n Retour au sommet"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.html",
    "href": "projects/2022-04-01-my-fishing-application/index.html",
    "title": "Mon application de pêche",
    "section": "",
    "text": "Dans ce projet, j’ai créé une application Shiny pour enregistrer mes sessions de pêche dans le but de trouver les conditions météorologiques optimales pour attraper plus de poissons. J’ai rédigé plusieurs articles de blog liés à ce projet, n’hésitez pas à les consulter !"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.html#résumé",
    "href": "projects/2022-04-01-my-fishing-application/index.html#résumé",
    "title": "Mon application de pêche",
    "section": "",
    "text": "Dans ce projet, j’ai créé une application Shiny pour enregistrer mes sessions de pêche dans le but de trouver les conditions météorologiques optimales pour attraper plus de poissons. J’ai rédigé plusieurs articles de blog liés à ce projet, n’hésitez pas à les consulter !"
  },
  {
    "objectID": "projects/2022-04-01-my-fishing-application/index.html#productions",
    "href": "projects/2022-04-01-my-fishing-application/index.html#productions",
    "title": "Mon application de pêche",
    "section": "Productions",
    "text": "Productions\n\nArticles de blog :\n\nCréation d’une application Shiny pour stocker mes données de pêche\n\nAnalyse exploratoire de mes données de pêche\n\nMise à jour de mon application\n\nAnalyse exploratoire de mes données de pêche (Shiny et Plotly)\n\nApplications Shiny\n\nExemple simplifié de l’application de pêche que j’utilise\n\nApplication Shiny présentant les résultats de ma saison de pêche 2021"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.html",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.html",
    "title": "Apprentissage statistique pour l’évaluation des risques côtiers",
    "section": "",
    "text": "L’objectif de mon travail était de démontrer comment les méthodes d’apprentissage statistique peuvent contribuer à l’amélioration des outils d’évaluation des risques côtiers et au développement d’un système d’alerte précoce visant à réduire le risque d’inondation côtière. J’ai travaillé sur 4 sujets :\n\nRegroupement des régimes météorologiques\n\nMéthode d’apprentissage automatique pour corriger les prévisions de vagues\n\nApprentissage profond pour créer une base de données d’impacts de vagues à partir d’images de surveillance\n\nRéseau bayésien pour améliorer la prise de décision en matière de risques côtiers"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#résumé",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#résumé",
    "title": "Apprentissage statistique pour l’évaluation des risques côtiers",
    "section": "",
    "text": "L’objectif de mon travail était de démontrer comment les méthodes d’apprentissage statistique peuvent contribuer à l’amélioration des outils d’évaluation des risques côtiers et au développement d’un système d’alerte précoce visant à réduire le risque d’inondation côtière. J’ai travaillé sur 4 sujets :\n\nRegroupement des régimes météorologiques\n\nMéthode d’apprentissage automatique pour corriger les prévisions de vagues\n\nApprentissage profond pour créer une base de données d’impacts de vagues à partir d’images de surveillance\n\nRéseau bayésien pour améliorer la prise de décision en matière de risques côtiers"
  },
  {
    "objectID": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#productions",
    "href": "projects/2021-10-01-statistical-learning-coastal-risks/index.html#productions",
    "title": "Apprentissage statistique pour l’évaluation des risques côtiers",
    "section": "Productions",
    "text": "Productions\n\nMa thèse de doctorat :\n\nApprentissage statistique pour l’évaluation des risques côtiers\n\nPrésentation :\n\nPrésentation pour ma soutenance de thèse\n\nDeux articles publiés dans des revues à comité de lecture :\n\nCallens, A., Morichon, D., Liria, P., Epelde, I., & Liquet, B. (2021). Automatic Creation of Storm Impact Database Based on Video Monitoring and Convolutional Neural Networks. Remote Sensing, 13(10), 1933, (10.3390/rs13101933).\n\nCallens, A., Morichon, D., Abadie, S., Delpey, M., Liquet, B. (2020). Using Random forest and Gradient boosting trees to improve wave forecast at a specific location. Applied Ocean Research, 104, (10.1016/j.apor.2020.102339).\n\nArticle de blog :\n\nLa fin de mon parcours doctoral"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mon blog",
    "section": "",
    "text": "Trier par\n       Ordre par défaut\n         \n          Titre\n        \n         \n          Date - Le plus ancien\n        \n         \n          Date - Le plus récent\n        \n         \n          Auteur·rice\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPrécipitations avec une précision de 1 km² : un mythe qui devient réalité ?\n\n\n\n\n\n\nResearch\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n17 déc. 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nExplorer la valeur du télédétection en agriculture\n\n\n\n\n\n\nResearch\n\n\nEDA\n\n\n\n\n\n\n\n\n\n19 sept. 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nL’importance du controle de la qualité des données en Météorologie\n\n\n\n\n\n\nResearch\n\n\nAnomaly detection\n\n\n\n\n\n\n\n\n\n17 juil. 2024\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nOptimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\nEDA\n\n\nNLP\n\n\n\n\n\n\n\n\n\n21 sept. 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse des packages R les plus téléchargés\n\n\n\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n19 juil. 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nR et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 4\n\n\nAnalyse exploratoire de mes données (shiny et plotly)\n\n\n\nR\n\n\nShiny\n\n\nEDA\n\n\n\n\n\n\n\n\n\n12 avr. 2022\n\n\nAurélien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nLa fin de ma thèse\n\n\n\n\n\n\n\n\n\n\n\n2 oct. 2021\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nR et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 3\n\n\nMise à jour de l’application\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n1 juin 2021\n\n\nAurélien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping sur Aliexpress avec Rselenium\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n18 nov. 2020\n\n\nAurelien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nEst-ce que R et Shiny peuvent faire de moi un meilleur pêcheur ? Partie 2\n\n\nAnalyse exploratoire de mes données de pêche\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n25 sept. 2020\n\n\nAurélien Callens\n\n\n\n\n\n\n\n\n\n\n\n\nR et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 1\n\n\nCréer une application Shiny pour stocker mes données de pêche\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n12 sept. 2020\n\n\nAurélien Callens\n\n\n\n\n\n\nAucun article correspondant\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "",
    "text": "Il y a quelques semaines, j’ai commencé à chercher un poste de data scientist dans l’industrie. Mes premiers gestes ont été :\nAprès avoir lu de nombreuses annonces et travaillé plusieurs heures sur mon CV, je me suis demandé si je pouvais optimiser ces étapes avec R et Data Science. J’ai donc décidé de scraper Indeed et d’analyser les données des offres de data science pour :"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#chargement-des-bibliothèques",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#chargement-des-bibliothèques",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Chargement des bibliothèques",
    "text": "Chargement des bibliothèques\nLa première étape est d’importer plusieurs packages :\n\n# General\nlibrary(tidyverse)\n# Webscraping \nlibrary(rvest)\nlibrary(RSelenium)\n# Geo data\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(rnaturalearth)\nlibrary(sf)\n# NLP\nlibrary(udpipe)\nlibrary(textrank)\nlibrary(wordcloud)\n# Cleaning\nlibrary(stringr)\n# Additional functions presented at the end of the post \nsource('scraping_functions.R')"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#collecter-les-données-avec-le-web-scraping",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#collecter-les-données-avec-le-web-scraping",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Collecter les données avec le web scraping",
    "text": "Collecter les données avec le web scraping\nAu début de ce projet, j’utilisais read_html() de rvest pour accéder et télécharger la page web d’Indeed. Cependant, les pages Indeed sont protégées par un logiciel anti-scraping qui bloquait toutes mes demandes, même si le scraping n’est pas interdit sur les pages qui m’intéressent (j’ai vérifié la page robots.txt).\nC’est pourquoi j’ai décidé d’accéder aux pages avec Rselenium qui permet d’exécuter un navigateur sans tête. On commence par naviguer vers la page correspondant aux résultats de recherche des offres de Data Scientist en France :\n\nurl = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&from=searchOnHP\"\n\n# Headless Firefox browser\nexCap &lt;- list(\"moz:firefoxOptions\" = list(args = list('--headless')))\nrD &lt;- rsDriver(browser = \"firefox\", extraCapabilities = exCap, port=1111L,\n                verbose = F)\nremDr &lt;- rD$client\n\n# Navigate to the url\nremDr$navigate(url)\n\n# Store page source \nweb_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\nPour scraper une information spécifique sur une page web, voici les étapes à suivre :\n\nTrouver l’élément/texte/donnée que vous souhaitez scraper sur la page web.\nTrouver le xpath ou le sélecteur CSS associé en utilisant l’outil de développement de Chrome ou Firefox (tutoriel ici !).\nExtraire l’élément avec html_element() en indiquant le xpath ou le sélecteur CSS.\nTransformer les données en texte avec html_text2().\nNettoyer les données si nécessaire.\n\nVoici l’exemple avec le nombre d’offres de Data Scientist listées en France :\n\nweb_page %&gt;%\n  html_element(css = \"div.jobsearch-JobCountAndSortPane-jobCount\") %&gt;% # selecting with css \n  html_text2() %&gt;% # Transform to text\n  str_remove_all(\"[^0-9.-]\") %&gt;% # Clean the data to only get numbers\n  substr(start = 2, stop = 8) %&gt;% \n  as.numeric()\n\nPour l’instant, on ne peut scraper les données que de la première page. Cependant, je suis intéressé par toutes les offres d’emploi et j’ai besoin d’accéder aux autres pages ! Après avoir navigué sur les 3 premières pages des offres, j’ai remarqué un modèle dans l’URL (valide au moment de l’écriture), ce qui signifie qu’avec une seule ligne de code, je peux produire une liste contenant les URL des 40 premières pages.\nUne fois la liste obtenue, il ne reste plus qu’à boucler sur toutes les URL avec un délai (bonne pratique pour le web scraping), collecter les données et les nettoyer avec des fonctions personnalisées (à la fin de l’article) :\n\n# Creating URL link corresponding to the first 40 pages\nbase_url = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&start=\"\nurl_list &lt;- c(url, paste0(base_url, as.character(seq(from=10, to=400, by=10))))\n\n# Looping through the URL list\nres &lt;- list()\nfor(i in 1:length(url_list)){\n  # Navigate to the URL\n  remDr$navigate(url_list[i])\n  \n  # Store page source \n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\n  # Job title \n  job_title &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\") %&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_text2() %&gt;%\n    str_replace(\".css.*;\\\\}\", \"\")\n\n  # URL for job post \n  job_url &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_element(\"a\") %&gt;%\n    html_attr('href') %&gt;%\n    lapply(function(x){paste0(\"https://fr.indeed.com\", x)}) %&gt;%\n    unlist()\n  \n  # Data about company\n  company_info &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\")%&gt;%\n    html_element(css = \".company_location\")%&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_comploc) %&gt;% # Function to clean the textual data\n    do.call(rbind, .)\n\n  # Data about job description\n  job_desc &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_element(css =\".slider_container .jobCardShelfContainer\")%&gt;%\n    html_text2() %&gt;%\n    tidy_job_desc() # Function to clean the textual data related to job desc.\n\n  # Data about salary (when indicated)\n  salary_hour &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result .resultContent\")%&gt;%\n    html_element(css = \".salaryOnly\") %&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_salary) %&gt;% # Function to clean the data related to salary\n    do.call(rbind, .)\n  \n  # Job posts in the same format\n  final_df &lt;- cbind(job_title, company_info, salary_hour, job_desc, job_url)\n  colnames(final_df) &lt;- c(\"Job_title\", \"Company\", \"Location\", \"Rating\", \"Low_salary\", \"High_salary\", \"Contract_info\", \"Job_desc\", \"url\")\n  res[[i]] &lt;- final_df\n  \n  # Sleep 5 seconds, good practice for web scraping\n  Sys.sleep(5)\n}\n\n# Gather all the job post in a tibble\nfinal_df &lt;- as_tibble(do.call(\"rbind\", res))\n\n# Final data cleaning\nfinal_df &lt;- final_df %&gt;%\n  mutate_at(c(\"Rating\", \"Low_salary\", \"High_salary\"), as.numeric)\n\n# Clean job title\nfinal_df$Job_title_c &lt;- clean_job_title(final_df$Job_title)  \nfinal_df$Job_title_c &lt;- as.factor(final_df$Job_title_c)\n\nOn a maintenant un jeu de données propre ! Voici un exemple tronqué des 5 premières lignes :\n\n\n\n\n\n\nJob_title\nCompany\nLocation\nRating\nLow_salary\nHigh_salary\nContract_info\nJob_desc\nJob_type\nJob_title_c\n\n\n\n\nData Scientist junior (H/F)\nKea & Partners\n92240 Malakoff\nNA\n3750\n4583\nCDI +2 | Travail en journée +1\nPlusieurs postes à pourvoirMaitrise de Python et des packages de data science. 1er cabinet européen de conseil en stratégie à devenir Société à Mission, certifiés B-Corp depuis 2021*,…\nPrésentiel\ndata scientist junior\n\n\nData Scientist (F ou H)\nSNCF\nSaint-Denis (93)\n3.9\nNA\nNA\nCDI\nLe développement informatique (C, C++, Python, Azure, …). Valider et recetter les phases des projets. Travailler avec des méthodes agiles avec les équipes et…\nPrésentiel\ndata scientist\n\n\nData Scientist (H/F) (IT)\nYzee Services\nParis (75)\n3.3\n2916\n3750\nTemps plein\nRecueillir, structurer et analyser les données pertinentes pour l'entreprise (activité liée à la relation client, conseil en externe).\nPrésentiel\ndata scientist\n\n\nData Scientist H/F\nNatan (SSII)\nParis (75)\nNA\n4583\n5833\nCDI +1 | Travail en journée\nPlusieurs postes à pourvoirVous retrouverez une *ESN ambitieuse portée par le goût de l’excellence.*. Au sein du département en charge d'automatisation transverse des besoins de la…\nPrésentiel\ndata scientist\n\n\nData Scientist Junior H/F / Freelance\nkarma partners\nRoissy-en-Brie (77)\nNA\n400\n550\nTemps plein +1\nLe profil recherché est un profil junior (0-2 ans d'expérience) en data science, avec une appétence technique et des notions d'architecture logicielle et de…\nPrésentiel\ndata scientist junior\n\n\n\n\n\n\n\n\nVisualisation des salaires proposés\nVoyons si on peut obtenir quelques informations sur les offres de jobs en data science en réalisant quelques représentations graphiques. La première chose que je voulais savoir, c’était combien les entreprises étaient prêtes à payer pour recruter un candidat en data science. J’ai donc décidé de réaliser quelques graphiques sur la plage de salaires en fonction de l’entreprise et du titre du poste.\nAttention !\nLes graphiques suivants doivent être pris avec des pincettes, car ils affichent un petit échantillon des données. En effet, le salaire n’était listé que pour 14% des annonces. Les tendances ou informations dans ces graphiques peuvent ne pas être représentatives des entreprises n’ayant pas indiqué leur salaire proposé.\n\nSalaire par entreprise\nLe graphique suivant montre les revenus mensuels proposés par certaines entreprises (toutes les entreprises ne listent pas leur salaire proposé) :\n\n# Function to make euro X scale \neuro &lt;- scales::label_dollar(\n  prefix = \"\",\n  suffix = \"\\u20ac\",\n  big.mark = \".\",\n  decimal.mark = \",\"\n)\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% # To remove internships and freelance works\n  select(Company, Low_salary, High_salary) %&gt;%\n  group_by(Company) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n           Company = fct_reorder(Company, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Company)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  geom_hline(aes(yintercept = median(Mean_salary)), lty=2, col='red', alpha = 0.7) +\n  scale_y_continuous(labels = euro) +\n  ylab(\"Monthly income\") +\n  xlab(\"\") +\n  coord_flip() +\n  theme_bw(base_size = 8)\n\n\n\n\n\n\n\n\nLa médiane des salaires mensuels est d’environ 3700 euros. Comme vous pouvez le constater, les salaires peuvent varier considérablement selon l’entreprise. Cela est en partie dû au fait que je n’ai pas fait de distinction entre les différents types de postes en data science (data scientist, data analyst, data engineer, senior ou lead).\n\n\nSalaire par titre de poste\nOn peut tracer le même graphique, mais au lieu de regrouper par entreprise, on va regrouper par titre de poste :\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;%  # To remove internships and freelance works\n  select(Job_title_c, Low_salary, High_salary, Job_type) %&gt;%\n  group_by(Job_title_c) %&gt;%\n  summarize_if(is.numeric, ~ mean(.x, na.rm = TRUE)) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_title_c = fct_reorder(Job_title_c, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Job_title_c, y = Mean_salary)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  #geom_label(aes(label = n, Job_title_c, y = 1500), data = count_df) + \n  scale_y_continuous(labels = euro) +\n  theme_bw(base_size = 12) +\n  xlab(\"\") +\n  ylab(\"Monthly Income\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nOn remarque clairement les différences de salaires proposés en fonction du titre de poste : les data scientists semblent gagner légèrement plus en moyenne que les data analysts. Les entreprises semblent également proposer des salaires plus élevés pour les postes avec plus de responsabilités ou nécessitant plus d’expérience (senior, lead).\n\n\nSalaire en fonction de la localisation : télétravail complet, hybride, sur site ?\nEnfin, on peut tracer les salaires en fonction de la localisation (télétravail complet, hybride, sur site) pour voir si cela a un impact :\n\n# Tidy the types and locations of listed jobs\nfinal_df &lt;- tidy_location(final_df)\ncount_df &lt;- count(final_df %&gt;% filter(Low_salary &gt; 1600), Job_type)\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% \n  drop_na(Location) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_type = as.factor(Job_type)) %&gt;%\n    ggplot(aes(x = Job_type, y = Mean_salary)) +\n  geom_boxplot(na.rm = TRUE) +\n  geom_label(aes(label = n, Job_type, y = 5500), data = count_df) + \n  scale_y_continuous(labels = euro) + \n  theme_bw(base_size = 12) +\n  xlab(\"Job Type\") +\n  ylab(\"Income\")\n\n\n\n\n\n\n\n\nIl est à noter que la plupart des emplois proposés en France sont des emplois sur site. Le salaire médian pour ce type de postes est légèrement inférieur à celui des emplois hybrides. La distribution des salaires pour les emplois en télétravail complet et hybrides doit être interprétée avec prudence car elle ne concerne que 12 offres d’emploi.\n\n\n\nCartographie des lieux des emplois\nLors de ma recherche d’emploi, j’étais frustré de ne pas voir une carte géographique regroupant les lieux de tous les emplois proposés. Une telle carte pourrait m’aider considérablement dans ma recherche. Faisons-la !\nTout d’abord, on doit nettoyer et homogénéiser les lieux pour toutes les offres d’emploi. À cette fin, j’ai créé une fonction personnalisée (tidy_location()) qui inclut plusieurs fonctions de stringr. Vous pouvez trouver plus de détails sur cette fonction à la fin de ce post. Elle renvoie le lieu sous ce format : [Ville]([Code postal]). Même si tous les lieux ont été homogénéisés, ils ne peuvent pas être directement tracés sur une carte (on a besoin de la longitude et de la latitude). Pour obtenir la latitude et la longitude à partir du nom de la ville et du code postal, j’ai utilisé la fonction geocode() du package tidygeocoder.\n\n# Extract coordinates from town name\nfinal_df &lt;- final_df %&gt;%\n  mutate(Loc_tidy_fr = paste(Loc_tidy, 'France')) %&gt;%\n  geocode(Loc_tidy_fr, method = 'arcgis', lat = latitude , long = longitude) %&gt;%\n  select(- Loc_tidy_fr)\n\n\nDistribution des emplois en Data Science en France\nOn peut maintenant représenter le nombre d’emplois en Data Science par département :\n\n# Map of France from rnaturalearth package\nfrance &lt;- ne_states(country = \"France\", returnclass = \"sf\") %&gt;% \n  filter(!name %in% c(\"Guyane française\", \"Martinique\", \"Guadeloupe\", \"La Réunion\", \"Mayotte\"))\n\n# Transform location to st point \ntest &lt;- st_sf(final_df, geom= lapply(1:nrow(final_df), function(x){st_point(c(final_df$longitude[x],final_df$latitude[x]))}))\nst_crs(test) &lt;- 4326\n\n# St_join by departments \njoined &lt;- france %&gt;%\n  st_join(test, left = T)\n\n# Custom breaks for visual representation\nmy_breaks = c(0, 2, 5, 10, 30, 50, 100, 260)\n\njoined %&gt;% \n  mutate(region=as.factor(name)) %&gt;% \n  group_by(region) %&gt;% \n  summarize(Job_number=n()) %&gt;% \n  mutate(Job_number = cut(Job_number, my_breaks)) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill=Job_number), col='grey', lwd=0.2) + \n  scale_fill_brewer(\"Job number\",palette = \"GnBu\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nIl est vraiment intéressant de constater que la répartition des emplois est assez hétérogène en France. La majorité des emplois sont concentrés dans quelques départements qui abritent une grande ville. Cela est attendu, car la plupart des emplois sont proposés par de grandes entreprises souvent installées à proximité des grandes villes.\n\n\nCarte interactive\nOn peut assii aller plus loin et tracer une carte interactive avec leaflet, ça nous permet de rechercher dynamiquement une offre d’emploi :\n\n# Plot leaflet map\nfinal_df %&gt;%\n  mutate(pop_up_text = sprintf(\"&lt;b&gt;%s&lt;/b&gt; &lt;br/&gt; %s\",\n                                     Job_title, Company)) %&gt;% # Make popup text\n  leaflet() %&gt;%\n  setView(lng = 2.36, lat = 46.31, zoom = 5.2) %&gt;% # Center of France\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addMarkers(\n    popup = ~as.character(pop_up_text),\n    clusterOptions = markerClusterOptions()\n  )"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#analyser-les-descriptions-demploi",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#analyser-les-descriptions-demploi",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Analyser les descriptions d’emploi",
    "text": "Analyser les descriptions d’emploi\nDe nos jours, la plupart des CV sont scannés et interprétés par un système de suivi des candidatures (ATS). Pour faire simple, ce système recherche des mots-clés dans votre CV et évalue la correspondance avec l’offre d’emploi pour laquelle vous postulez. Il est donc important de décrire vos expériences avec des mots-clés spécifiques pour améliorer vos chances d’accéder à l’étape suivante du processus de recrutement.\nMais quels mots-clés devrait-on inclure dans mon CV ? Répondons à cette question en analysant les descriptions des offres d’emploi de data scientist.\n\nTélécharger et nettoyer chaque description d’emploi\nTout d’abord, on télécharge la description complète de chaque offre en naviguant à travers toutes les URL listées dans notre tableau. On nettoye et homogénéise la description avec une fonction personnalisée :\n\n# Loop through all the URLs\njob_descriptions &lt;- list()\npb &lt;- txtProgressBar(min = 1, max = length(final_df$url), style = 3)\nfor(i in 1:length(final_df$url)){\n  remDr$navigate(final_df$url[i])\n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n  job_descriptions[[i]] &lt;- web_page %&gt;%\n        html_elements(css = \".jobsearch-JobComponent-description\") %&gt;%\n      html_text2()\n  Sys.sleep(2)\n  setTxtProgressBar(pb, i)\n}\n# Gathering in dataframe\njob_descriptions &lt;- as.data.frame(do.call(\"rbind\", job_descriptions))\nnames(job_descriptions) &lt;- c(\"Description\")\n\n# Binding to same table:\nfinal_df &lt;- cbind(final_df, job_descriptions)\n\n# Homogenize with custom function\nfinal_df$Description_c &lt;- lapply(final_df$Description, function(x){clean_job_desc(x)[[2]]})\nfinal_df$Language &lt;- textcat::textcat(final_df$Description)\n\n\n\nProcédure d’annotation avec le package udpipe\nCette partie est inspirée de cet article.\nMaintenant que les descriptions de tous les emplois listés sont importées et pré-nettoyées, on peut annoter les données textuelles avec le package udpipe. Ce package contient des fonctions et des modèles qui permettent de réaliser la tokenisation, la lemmatisation et l’extraction de mots-clés.\nOn restreigne d’abord cette analyse aux offres d’emploi de data scientist rédigées en français, puis on annote toutes les descriptions :\n\n# Restricting the analysis to Data scientist post written in french\ndesc_data_scientist &lt;- final_df %&gt;%\n  filter((Job_title_c == \"data scientist\") & (Language == \"french\")) %&gt;%\n  select(Description_c)\n\nud_model &lt;- udpipe_download_model(language = \"french\") # Download the model if necessary\nud_model &lt;- udpipe_load_model(ud_model$file_model) \n\n# Annotate the descriptions \nx &lt;- udpipe_annotate(ud_model, x = paste(desc_data_scientist, collapse = \" \"))\nx &lt;- as.data.frame(x)\n\n\n\nLes noms les plus courants\nOn peut visualiser les mots les plus utilisés dans les offres d’emploi de data scientist rédigées en français :\n\nstats &lt;- subset(x, upos %in% \"NOUN\")\nstats &lt;- txt_freq(x = stats$lemma)\n\nstats %&gt;%\n  top_n(50, freq) %&gt;%\n  mutate(key = as.factor(key),\n         key = fct_reorder(key, freq)) %&gt;%\n  ggplot(aes(x = key, y = freq)) +\n  geom_bar(stat = 'identity') +\n  coord_flip() + \n  ylab(\"Most common nouns\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nBien que cela nous donne une idée des mots à inclure, ce n’est pas très informatif car les mots-clés sont souvent composés de deux mots ou plus.\n\n\nExtraction des mots-clés pour la rédaction de CV\nIl existe plusieurs méthodes implémentées dans udpipe pour extraire les mots-clés d’un texte. Après avoir testé plusieurs méthodes, j’ai sélectionné l’extraction automatique rapide des mots-clés (RAKE) qui me donne les meilleurs résultats :\n\nstats &lt;- keywords_rake(x = x,\n                       term = \"token\",# Search on token\n                       group = c(\"doc_id\", \"sentence_id\"), # On every post \n                       relevant = x$upos %in% c(\"NOUN\", \"ADJ\"),  # Only among noun and adj.\n                       ngram_max = 2, n_min = 2, sep = \" \")\n\nstats &lt;- subset(stats, stats$freq &gt;= 5 & stats$rake &gt; 3)\n\nstats %&gt;% \n  arrange(desc(rake)) %&gt;% \n  head()\n\n                    keyword ngram freq     rake\n1 intelligence artificielle     2    9 9.368889\n2             tableaux bord     2    5 8.504274\n3      formation supérieure     2    5 8.374725\n4        modèles prédictifs     2   15 7.581294\n5         force proposition     2    6 7.190238\n6        production échelle     2    5 7.034038\n\nwordcloud(words = stats$keyword, freq = stats$freq, min.freq = 3,\n          max.words=100, random.order=FALSE, rot.per=0.35,\n          colors=brewer.pal(8, \"Dark2\"), scale = c(2.5, .5))\n\n\n\n\n\n\n\n\nOn peut voir que cette méthode a sélectionné des mots-clés importants en français liés au poste de data scientist ! Dans les premières positions, on trouve les mots-clés : “intelligence artificielle”, “tableaux de bord”, “enseignement supérieur”, “modèle prédictif”. Il vaut mieux vérifier si ces mots apparaissent sur mon CV !"
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#conclusion",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#conclusion",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Conclusion",
    "text": "Conclusion\nJ’espère vous avoir convaincu qu’il est possible d’optimiser votre recherche d’emploi avec la Data Science !\nSi cet article vous a intéressé et que vous êtes à la recherche d’un nouveau Data Scientist, n’hésitez pas à me contacter par mail car je suis actuellement en recherche d’emploi en France (hybride, remote) ou en Europe (remote)."
  },
  {
    "objectID": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#fonctions-personnalisées-pour-nettoyer-les-données-extraites-de-la-page-web",
    "href": "posts/2022-09-21-webscraping-indeed-data-scientist-jobs/index.html#fonctions-personnalisées-pour-nettoyer-les-données-extraites-de-la-page-web",
    "title": "Optimiser ma recherche de postes de Data Scientist en scrappant Indeed avec R",
    "section": "Fonctions personnalisées pour nettoyer les données extraites de la page web",
    "text": "Fonctions personnalisées pour nettoyer les données extraites de la page web\nCes fonctions utilisent plusieurs méthodes telles que les expressions régulières, les mots vides et les instructions conditionnelles pour nettoyer les données textuelles.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(httr)\nlibrary(tidystopwords)\nlibrary(textcat)\n\n# Function to tidy the data related to the company\ntidy_comploc &lt;- function(text){\n  lst &lt;- str_split(text, pattern = \"\\n\", simplify =T)\n  ext_str &lt;- substr(lst[1], nchar(lst[1])-2, nchar(lst[1]))\n  res &lt;- suppressWarnings(as.numeric(gsub(',', '.', ext_str)))\n  lst[1] &lt;- ifelse(is.na(res), lst[1], substr(lst[1], 1, nchar(lst[1])-3))\n  lst[3] &lt;- res\n  t(as.matrix(lst))\n}\n\n# Function to tidy the short job description provided with the job post\ntidy_job_desc &lt;- function(text){\n  stopwords &lt;- c(\"Candidature facile\", \"Employeur réactif\")\n  text &lt;- str_remove_all(text, paste(stopwords, collapse = \"|\"))\n  stopwords_2 &lt;- \"(Posted|Employer).*\"\n  text &lt;- str_remove_all(text, stopwords_2)\n  text\n}\n\n# Function to tidy the salary data if provided\ntidy_salary &lt;- function(text){\n  if(is.na(text)){\n    others &lt;- NA\n    sal_low &lt;- NA\n    sal_high &lt;- NA\n  }else{\n    text &lt;- str_split(text, \"\\n\", simplify = T)\n    others &lt;- paste(text[str_detect(text, \"€\", negate = T)], collapse = \" | \")\n    sal &lt;- text[str_detect(text, \"€\", negate = F)]\n    if(rlang::is_empty(sal)){\n      sal_low &lt;- NA\n      sal_high &lt;- NA\n    }else{\n      range_sal &lt;- as.numeric(str_split(str_remove_all(str_replace(sal, \"à\", \"-\"), \"[^0-9.-]\"), \"-\", simplify = TRUE))\n      sal_low &lt;- sort(range_sal)[1]\n      sal_high &lt;- sort(range_sal)[2]\n\n      if(str_detect(sal, \"an\")){\n        sal_low &lt;- floor(sal_low/12)\n        sal_high &lt;- floor(sal_high/12)\n      }\n    }\n  }\n  return(c(as.numeric(sal_low), as.numeric(sal_high), others))\n}\n\n# Function to tidy the location of the job (Remote/Hybrid/Onsite) + homogenize \n# location and zip code\ntidy_location &lt;- function(final_df){\n  final_df$Job_type &lt;- ifelse(final_df$Location == \"Télétravail\", \"Full Remote\", ifelse(str_detect(final_df$Location, \"Télétravail\"), \"Hybrid\", \"On site\"))\n  final_df$Loc_possibility &lt;- ifelse(str_detect(final_df$Location, \"lieu\"), \"Plusieurs lieux\", NA)\n  stopwords &lt;- c(\"Télétravail à\", \"Télétravail\", \"à\", \"hybride\")\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Location, paste(stopwords, collapse = \"|\"))\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Loc_tidy, \"[+].*\")\n  final_df$Loc_tidy &lt;- str_trim(final_df$Loc_tidy)\n  final_df$Loc_tidy &lt;-  sapply(final_df$Loc_tidy,\n                               function(x){\n                                 if(!is.na(suppressWarnings(as.numeric(substr(x, 1, 5))))){\n                                   return(paste(substr(x, 7, 30), paste0('(', substr(final_df$Loc_tidy[2], 1, 2), ')')))\n                                 }else{\n                                   return(x)\n                                 }})\n  return(final_df)\n}\n\n# Function to keep only certain words in text\nkeep_words &lt;- function(text, keep) {\n  words &lt;- strsplit(text, \" \")[[1]]\n  txt &lt;- paste(words[words %in% keep], collapse = \" \")\n  return(txt)\n}\n\n# Homogenize the job title and class them in a few categories\nclean_job_title &lt;- function(job_titles){\n  job_titles &lt;- tolower(job_titles)\n  job_titles &lt;- gsub(\"[[:punct:]]\", \" \", job_titles, perl=TRUE)\n\n  words_to_keep &lt;- c(\"data\", \"scientist\", \"junior\", \"senior\", \"engineer\", \"nlp\",\n                     \"analyst\", \"analytics\", \"analytic\", \"science\", \"sciences\",\n                     \"computer\", \"vision\", \"ingenieur\", \"données\", \"analyste\",\n                     \"analyses\", \"lead\", \"leader\", \"dataminer\", \"mining\", \"chief\",\n                     \"miner\", \"analyse\", 'head')\n  job_titles_c &lt;- unlist(sapply(job_titles, function(x){keep_words(x, words_to_keep)}, USE.NAMES = F))\n  job_titles_c &lt;- unlist(sapply(job_titles_c, function(x){paste(unique(unlist(str_split(x, \" \"))), collapse = \" \")}, USE.NAMES = F))\n  table(job_titles_c)\n\n  data_analytics_ind &lt;-  job_titles_c %in% c(\"analyses data\", \"analyst data\", \"analyste data\", \"analyste data scientist\", \"data analyse\",\n                                             \"analyste données\", \"analytic data scientist\", \"analytics data\", \"analytics data engineer\", \"data analyst engineer\",\n                                             \"data analyst données\", \"data analyst scientist\", \"data analyst scientist données\", \"data analyste\", \"data analyst analytics\",\n                                             \"data analytics\", \"data analytics engineer\", \"data engineer analyst\", \"data scientist analyst\", \"data scientist analytics\")\n  job_titles_c[data_analytics_ind] &lt;- \"data analyst\"\n\n  data_analytics_j_ind &lt;-  job_titles_c %in% c(\"junior data analyst\", \"junior data analytics\", \"junior data scientist analyst\")\n  job_titles_c[data_analytics_j_ind] &lt;- \"data analyst junior\"\n\n  data_scientist_ind &lt;- job_titles_c %in% c(\"data computer science\", \"data science\", \"data science scientist\", \"data sciences\",\n                                            \"data sciences scientist\", \"data scientist données\", \"data scientist sciences\",\n                                            \"données data scientist\", \"scientist data\", \"science données\", \"scientist data\",\n                                            \"scientist data science\", \"computer data science\", \"data science données\", \"data scientist science\")\n  job_titles_c[data_scientist_ind] &lt;- \"data scientist\"\n\n  data_scientist_j_ind &lt;- job_titles_c %in% c(\"junior data scientist\")\n  job_titles_c[data_scientist_j_ind] &lt;- \"data scientist junior\"\n\n  data_engineer_ind &lt;- job_titles_c %in% c(\"data engineer scientist\", \"data science engineer\", \"data miner\", \"data scientist engineer\",\n                                           \"dataminer\", \"engineer data scientist\", \"senior data scientist engineer\", \"ingenieur data scientist\")\n  job_titles_c[data_engineer_ind] &lt;- \"data engineer\"\n\n  nlp_data_scientist_ind &lt;- job_titles_c %in% c(\"data scientist nlp\", \"nlp data science\",\n                                                \"nlp data scientist\", \"senior data scientist nlp\")\n  job_titles_c[nlp_data_scientist_ind] &lt;- \"data scientist NLP\"\n\n  cv_data_scientist_ind &lt;- job_titles_c %in% c(\"computer vision data scientist\", \"data science computer vision\",\n                                               \"data scientist computer vision\")\n  job_titles_c[cv_data_scientist_ind] &lt;- \"data scientist CV\"\n\n  lead_data_scientist_ind &lt;- job_titles_c %in% c(\"chief data\", \"chief data scientist\", \"data scientist leader\", \"lead data scientist\",\n                                                 \"data chief scientist\", \"lead data scientist senior\", \"head data science\")\n  job_titles_c[lead_data_scientist_ind] &lt;- \"data scientist lead or higher\"\n  senior_data_scientist_ind &lt;- job_titles_c %in% c(\"senior data scientist\")\n  job_titles_c[senior_data_scientist_ind] &lt;- \"data scientist senior\"\n\n  senior_data_analytics_ind &lt;- job_titles_c %in% c(\"senior analytics data scientist\", \"senior data analyst\", \"senior data scientist analytics\")\n  job_titles_c[senior_data_analytics_ind] &lt;- \"data analyst senior\"\n\n\n  lead_data_analyst_ind &lt;- job_titles_c %in% c(\"lead data analyst senior\", \"lead data analyst\")\n  job_titles_c[lead_data_analyst_ind] &lt;- \"data analyst lead\"\n  return(job_titles_c)\n}\n\n# Function to clean the full job description before word annotation\nclean_job_desc &lt;- function(text){\n  text &lt;- tolower(text)\n  text &lt;- str_replace_all(text, \"\\n\", \" \")\n  text &lt;- str_remove(text, pattern = \"dé.*du poste \")\n  text &lt;- str_remove(text, pattern = \"analyse de recr.*\")\n  text &lt;- gsub(\"(?!&)[[:punct:]+’+…+»+«]\", \" \", text, perl=TRUE)\n\n  language &lt;- textcat(text)\n\n  if(language == \"french\"){\n    text &lt;- str_replace_all(text, \"œ\", \"oe\")\n    stopwords &lt;- c(\"détails\", \"poste\", \"description\", \"informations\", \"complémentaires\", \"c\", generate_stoplist(language = \"French\"))\n  }else{\n    stopwords &lt;- c(\"description\", generate_stoplist(language = \"English\"))\n  }\n\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n\n  return(c(language, text))\n}"
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html",
    "title": "Webscraping sur Aliexpress avec Rselenium",
    "section": "",
    "text": "Aujourd’hui, je vais vous montrer comment récupérer les prix des produits sur le site Aliexpress."
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#quelques-mots-sur-le-web-scraping",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#quelques-mots-sur-le-web-scraping",
    "title": "Webscraping sur Aliexpress avec Rselenium",
    "section": "Quelques mots sur le web scraping",
    "text": "Quelques mots sur le web scraping\nAvant de plonger dans le sujet, vous devez savoir que le web scraping n’est pas autorisé sur certains sites web. Pour savoir si cela s’applique au site que vous souhaitez scraper, je vous invite à vérifier la page robots.txt qui devrait se trouver à la racine de l’adresse du site. Pour Aliexpress, cette page se trouve ici : www.aliexpress.com/robots.txt.\nCette page indique que le web scraping et le crawling ne sont pas autorisés sur plusieurs catégories de pages telles que /bin/*, /search/*, /wholesale* par exemple. Heureusement pour nous, la catégorie /item/*, où les pages des produits sont stockées, peut être scrappée."
  },
  {
    "objectID": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#rselenium",
    "href": "posts/2020-11-18-webscraping-aliexpress-rselenium/index.html#rselenium",
    "title": "Webscraping sur Aliexpress avec Rselenium",
    "section": "RSelenium",
    "text": "RSelenium\n\nInstallation pour Ubuntu 18.04 LTS\nL’installation de RSelenium n’a pas été aussi simple que prévu et j’ai rencontré deux erreurs.\nLa première erreur que j’ai obtenue après avoir installé le package et essayé la fonction Rsdriver était :\nError in curl::curl_fetch_disk(url, x$path, handle = handle) :\nUnrecognized content encoding type. libcurl understands deflate, gzip content encodings.\nGrâce à ce post, j’ai installé le package manquant : stringi.\nUne fois cette erreur corrigée, j’en ai rencontré une autre :\nError: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar\nCette fois-ci, le problème venait d’un fichier corrompu. Grâce à ce post, j’ai su que je devais simplement télécharger ce fichier selenium-server-standalone-4.0.0-alpha-2.jar depuis le site officiel de Selenium et remplacer le fichier corrompu par celui-ci.\nJ’espère que cela aidera certains d’entre vous à installer RSelenium sur Ubuntu 18.04 LTS !\n\n\nOuverture d’un navigateur web\nAprès avoir corrigé les erreurs ci-dessus, je peux maintenant ouvrir un navigateur Firefox :\n\nlibrary(RSelenium)\n\n#Open a firefox driver\nrD &lt;- rsDriver(browser = \"firefox\") \nremDr &lt;- rD[[\"client\"]]\n\n\n\nConnexion à Aliexpress\nLa première étape pour récupérer les prix des produits sur Aliexpress est de se connecter à son compte :\n\nlog_id &lt;- \"Your_mail_adress\"\npassword &lt;- \"Your_password\"\n\n# Navigate to aliexpress login page \nremDr$navigate(\"https://login.aliexpress.com/\")\n\n# Fill the form with mail address\nremDr$findElement(using = \"id\", \"fm-login-id\")$sendKeysToElement(list(log_id))\n\n# Fill the form with password\nremDr$findElement(using = 'id', \"fm-login-password\")$sendKeysToElement(list(password))\n\n#Submit the login form by clicking Submit button\nremDr$findElement(\"class\", \"fm-button\")$clickElement()\n\n\n\nNavigating through the URLs and scraping the prices\nMaintenant, on doit naviguer à travers un vecteur contenant les URL des produits Aliexpress qui nous intéressent. Ensuite, on extrait le prix du produit en utilisant le xpath du prix du produit sur la page web. Le xpath de l’élément que vous voulez scraper peut être trouvé en utilisant les outils de développement de Chrome ou Firefox (tutoriel ici !). Une fois le prix extrait, il faut s’assurer que ce prix soit sous un format numérique en supprimant tout caractère spécial (symbole euro ou dollar) et en remplaçant la virgule par un point pour le séparateur décimal. Voici le code R :\n\n  url_list &lt;- list(\"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m\") \n\n# Allocate a vector to store the price of the products \ncurrentp &lt;- c()\nfor(i in 1:length(url_list)){\n  \n  # Navigate to link [i]\n  remDr$navigate(url_list[i])\n  \n  # Find the price with an xpath selector and findElement.  \n  # Sometimes products can be removed and this could throw an error this is why we are using 'try' to handle the potential errors\n  \n  current &lt;- try(remDr$findElement(using = \"xpath\",'//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"product-price-value\", \" \" ))]'), silent = T)\n  \n  #If error : current price is NA \n  if(class(current) =='try-error'){\n    currentp[i] &lt;- NA\n  }else{\n    # Get the price \n    text &lt;- unlist(current$getElementText())\n    \n    #Remove euro sign\n    text &lt;- gsub(\"[^A-Za-z0-9,;._-]\",\"\",text)\n    \n    #Case when there is a range of price instead of one price + replace comma by point\n    if(grepl(\"-\", text)) {  \n      pe &lt;- sub(\"-.*\",\"\",text) %&gt;% sub(\",\", \".\", ., fixed = TRUE)\n      currentp[i] &lt;-  as.numeric(pe)\n    }else{\n      currentp[i] &lt;- as.numeric(sub(\",\", \".\", text, fixed = TRUE))\n  }\n  }\n  \nSys.sleep(4)\n}\n\nIl est conseillé d’attendre quelques secondes entre chaque lien avec Sys.sleep(4) afin d’éviter d’être mis sur liste noire par le site web.\n\n\nVersion Phantomjs\nSi vous exécutez le code ci-dessus, vous devriez voir un navigateur Firefox s’ouvrir et naviguer à travers la liste que vous avez fournie. Dans le cas où vous ne souhaitez pas une fenêtre active, vous pouvez remplacer Firefox par le navigateur phantomjs, qui est un navigateur sans interface graphique (headless).\nJe ne sais pas pourquoi, mais l’utilisation de rsDriver(browser = \"phantomjs\") ne fonctionne pas pour moi. J’ai trouvé cet article qui propose de démarrer le navigateur phantomjs avec le package wdman :\n\nlibrary(wdman)\nlibrary(RSelenium)\n# start phantomjs instance\nrPJS &lt;- wdman::phantomjs(port = 4680L)\n\n# is it alive?\nrPJS$process$is_alive()\n\n#connect selenium to it?\nremDr &lt;-  RSelenium::remoteDriver(browserName=\"phantomjs\", port=4680L)\n\n# open a browser\nremDr$open()\n\nremDr$navigate(\"http://www.google.com/\")\n\n# Screenshot of the headless browser to check if everything is working\nremDr$screenshot(display = TRUE)\n\n# Don't forget to close the browser when you are finished ! \nremDr$close()\n\n\n\nConclusion\nUne fois que l’on comprend les bases de RSelenium et comment sélectionner des éléments dans des pages HTML, c’est assez facile d’écrire un script pour extraire des données sur le web. Cet article est un exemple simple d’extraction du prix des produits sur les pages Aliexpress, mais le script peut être étendu pour extraire plus de données sur chaque page, telles que le nom de l’article, sa note, etc. Il est même possible d’automatiser ce script pour qu’il s’exécute quotidiennement afin de suivre l’évolution des prix au fil du temps. Les possibilités sont infinies !"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur pêcheur ? Partie 2",
    "section": "",
    "text": "Dans le précédent article de blog, j’ai décrit en détail comment j’ai créé une application Shiny qui stocke les données de mes sessions de pêche. Dans cet article, je vais explorer les données que j’ai collectées au cours de l’année dernière.\nPour résumer, mon application stocke les données dans deux fichiers csv. Le premier contient des variables liées aux conditions de pêche au début et à la fin de la session, telles que :\nLe second contient des informations sur mes prises :"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html#importation-et-nettoyage-de-mes-données-de-pêche",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html#importation-et-nettoyage-de-mes-données-de-pêche",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur pêcheur ? Partie 2",
    "section": "Importation et nettoyage de mes données de pêche",
    "text": "Importation et nettoyage de mes données de pêche\nLa première étape de cette analyse consiste à importer les deux fichiers csv et à effectuer quelques transformations.\n\n\nShow the code\n# Change character variables to factor\nsession_data %&lt;&gt;% \nmutate_at(vars(Weather, Tide_status), as.factor)\n\n# Change character variables to factor\ncatch_data %&lt;&gt;% \nmutate_at(vars(species, lure, colour, length_lure), as.factor)\n\n\n \nAprès avoir nettoyé et réarrangé les données (le code est caché ci-dessous), on peut explorer graphiquement les données !\n\n\nShow the code\n# Compute mean conditions (between beg and end session) \n\nmean_weather_cond &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(-c(Long, Lat, Water_level, Tide_time)) %&gt;% \nsummarise_if(is.numeric, mean) \n\n\n# Extract fixed conditions and comments + join with mean cond \n\nfixed_cond_com &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(Session, Comments, Long, Lat, Weather) %&gt;% \nmutate(Comments_parsed = paste(na.omit(Comments), collapse = \"\")) %&gt;% \nselect(-Comments) %&gt;% \nslice(1) %&gt;% \ninner_join(mean_weather_cond, by = \"Session\")\n\n# Create end and beg variables for WL, Time , Tide_time, Tide_status\n\nbeg_end_vars &lt;- session_data %&gt;% \nselect(Session, Status, Water_level, Time, Tide_time, Tide_status) %&gt;% \npivot_wider(names_from = Status,\nvalues_from = c(Time, Water_level,  Tide_time, Tide_status))\n\n\n# Assemble both file and calculate duration\n\ndat_ses &lt;-  inner_join(beg_end_vars,\nfixed_cond_com,\nby = \"Session\")\n\n# Calculate duration of the sessions\n\ndat_ses %&lt;&gt;% \nmutate(duration = round(difftime(Time_end,  Time_beg,  units = \"hours\"),\ndigits = 1))\n\ncatch_cond &lt;- full_join(dat_ses,\ncatch_data, by = c( \"Session\" = \"n_ses\" )) %&gt;% \nmutate(Session = factor(Session, levels = 1:length(dat_ses$Session)))\n\ncatch_cond %&lt;&gt;%\nmutate(Tide_status_ses = paste0(Tide_status_beg, \"_\", Tide_status_end))\n\n# Simplify the Tide status variable\n\ncatch_cond$Tide_status_ses &lt;- sapply(catch_cond$Tide_status_ses , function(x){switch(x, \n\"Up_Dead\" = \"Up\",\n\"Up_Up\" = \"Up\",\n\"Up_Down\" = \"Dead\",\n\"Down_Dead\" = \"Down\",\n\"Down_Up\" = \"Dead\",\n\"Down_Down\"  = \"Down\",\n\"Dead_Dead\" = \"Dead\",\n\"Dead_Up\" = \"Up\",\n\"Dead_Down\" = \"Down\"\n)}, USE.NAMES = F)"
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html#exploration-graphique",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html#exploration-graphique",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur pêcheur ? Partie 2",
    "section": "Exploration graphique",
    "text": "Exploration graphique\n\nMes lieux de pêche\nOn peut visualiser les endroits où j’ai le plus pêché en utilisant le package leaflet :\n\n\nShow the code\n# Calculate the number of fish caught by session \nfish_number &lt;-  catch_cond  %&gt;% na.omit() %&gt;% group_by(Session) %&gt;%  summarise(nb = length(Session))\n\n# Dataframe with variables we want to show on the map\nmap_data &lt;- catch_cond %&gt;% \ngroup_by(Session) %&gt;%\nselect(Session, Time_beg, Time_end, Long,\nLat, Water_level_beg, Tide_status_beg, Tide_time_beg, duration) \n\nmap_data &lt;- full_join(map_data, fish_number)\n\nmap_data$nb[is.na(map_data$nb)] &lt;- 0\n\n# Interactive map with Popup for each session\nlibrary(leaflet)\n\nleaflet(map_data, width = \"100%\") %&gt;% addTiles() %&gt;%\naddPopups(lng = ~Long, lat = ~Lat, \nwith(map_data, sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  fish &lt;br/&gt; Water level: %.0f m, %s, %.0f min since last peak\",                                         Session, duration,  Time_beg, nb, Water_level_beg, Tide_status_beg, Tide_time_beg)), \noptions = popupOptions(maxWidth = 100, minWidth = 50))\n\n\n\n\n\n\nComme vous pouvez le voir, je pêche principalement dans la Nive, une rivière qui traverse la ville de Bayonne.\n\n\nQuel est le meilleur moment pour pêcher ?\n\nPériode de l’année\nLe graphique suivant montre le nombre de poissons capturés en fonction de la période de l’année :\n\n\nShow the code\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\nggplot(aes(y = n_catch, x =Time_beg)) +\ngeom_point( size = 2) + \n  theme_minimal() + labs(x = \"Date\", y = \"Number of catch\") + scale_x_datetime(date_labels = \"%d/%m/%y\", date_breaks = \"3 months\") \n\n\n\n\n\n\n\n\n\nAvec de ce graphique, on constate que je ne suis pas allé pêcher durant l’automne et l’hiver 2019, je n’ai donc aucune donnée pour ces saisons. Et c’est bête pour moi car l’automne est réputé être une excellente période pour la pêche au bar! Je dois aller pêcher cette année pour compenser ce manque de données. En hiver, la pêche est vraiment compliquée, car la grande majorité des bars retournent vers l’océan.\n\n\nHeure de la journée\nCe graphique montre le nombre de poissons capturés en fonction de l’heure de la journée :\n\n\nShow the code\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch ), \nhour = format(Time_beg, \"%H\")) %&gt;%\nggplot(aes(y = n_catch, x =hour)) +\ngeom_point( size = 2)  + labs(x = \"Hour\", y = \"Number of catch\")+\ntheme_minimal()\n\n\n\n\n\n\n\n\n\nJe pêche principalement après le travail ou en soirée. Pour tirer des conclusions pertinentes sur l’influence de l’heure de pêche, je dois aller pêcher à différents moments de la journée (le matin, par exemple).\n\n\nLa marée\nLa marée est un paramètre important pour la pêche en estuaire. Voyons l’effet du courant de marée sur mes prises :\n\n\nShow the code\nlibrary(ggpubr)\n\ngg1 &lt;- catch_cond %&gt;% \n  group_by(Session, Tide_status_ses, .drop = F)  %&gt;%  \n  drop_na() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Tide_status_ses\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\n  ggplot(aes(y = n_catch, x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot() +\n  labs(x = \"Status of tide current\", y = \"Number of catch\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg2 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length,x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot()+\n  labs(x = \"Status of tide current\", y = \"Length of the fish\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\nggarrange(gg1, gg2)\n\n\n\n\n\n\n\n\n\nIl semble que l’état du courant de marée n’influence pas le nombre de prises, mais qu’il affecte la taille des poissons. J’ai tendance à attraper des poissons plus gros lorsque le courant descend.\n\n\n\nLa lune affecte-t-elle mes résultats de pêche ?\nUne croyance largement répandue chez les pêcheurs est que la lune influence fortement le comportement des poissons. Les données sur la phase lunaire étaient disponibles grâce à l’API météo, j’ai donc décidé d’enregistrer cette variable pour vérifier si cette croyance était fondée.\nLes deux graphiques ci-dessous montrent le nombre et la taille des poissons en fonction de la phase de la lune (0 correspondant à la nouvelle lune et 1 à la pleine lune) :\n\n\nShow the code\ngg3 &lt;- catch_cond %&gt;% \n  group_by(Session, Moon, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Moon\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Moon)) +\n  geom_point( size = 2) +\n  labs(x = \"Moon phase\", y = \"Number of catch\")+\n  theme_minimal()\n\ngg4 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Moon)) +\n  geom_point( size = 2) +\n  geom_smooth(method=\"lm\", se=T) + \n  labs(x = \"Moon phase\", y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg3, gg4)\n\n\n\n\n\n\n\n\n\nLa phase de la lune ne semble pas influencer le nombre de poissons que j’attrape lors d’une session. Cependant, j’ai tendance à attraper des poissons plus gros à mesure que l’on se rapproche de la pleine lune. Pour confirmer cette observation, je dois continuer à pêcher afin de collecter plus de données !\n\n\nLa météo influence-t-elle mes résultats de pêche ?\nOn peut examiner le nombre de poissons capturés dans différentes conditions météorologiques :\n\n\nShow the code\n# precipitation probability \n\ngg5 &lt;- catch_cond %&gt;% \n  group_by(Session, Preci_prob, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Preci_prob\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Preci_prob)) +\n  geom_point()+\n  labs(x = \"Precipitation prob.\", y = \"Number of catch\")+\n  theme_minimal()\n\n# Atm pressure \n\ngg6 &lt;- catch_cond %&gt;% \n  group_by(Session, Atm_pres, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Atm_pres\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Atm_pres)) +\n  geom_point() +\n  labs(x = \"Atm. pressure\", y = \"Number of catch\")+\n  theme_minimal()\n\n#Air temp\n\ngg7 &lt;- catch_cond %&gt;% \n  group_by(Session, Air_temp, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Air_temp\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Air_temp)) +\n  geom_point() +\n  labs(x = \"Air temp.\", y = \"Number of catch\")+\n  theme_minimal()\n\n\n#Cloud cover\n\ngg8 &lt;- catch_cond %&gt;% \n  group_by(Session, Cloud_cover, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Cloud_cover\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Cloud_cover)) +\n  geom_point() +\n  labs(x = \"Cloud cover\", y = \"Number of catchh\")+\n  theme_minimal()\n\nggarrange(gg5, gg6, gg7, gg8)\n\n\n\n\n\n\n\n\n\nEn fonction de leur taille:\n\n\nShow the code\ngg15 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Preci_prob)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg16 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Atm_pres)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg17 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Air_temp)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\n\ngg18  &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Cloud_cover)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg15, gg16, gg17, gg18)\n\n\n\n\n\n\n\n\n\nÉtant donné que j’ai des données limitées et que toutes les conditions météorologiques ne sont pas couvertes, il est difficile de tirer des conclusions.\n\n\nQuels sont les meilleurs leurres pour attraper du poisson ?\nÀ chaque prise, je remplis un petit formulaire dans mon application Shiny afin d’enregistrer les caractéristiques du leurre utilisé. Il existe différents types de leurres ayant des nages spécifiques, des couleurs et des tailles variées. On peut représenter le nombre de poissons capturés en fonction des caractéristiques du leurre :\n\n\nShow the code\nlevels(catch_cond$colour) &lt;- c(\"clear\", \"natural\", \"dark\")\nlevels(catch_cond$length_lure) &lt;- c(\"large\", \"medium\", \"small\")\n\ngg9 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=lure, fill = lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ \n  theme(legend.position=\"None\")\n\ngg10 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=colour, fill = colour)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ \n  theme(legend.position=\"None\")\n\ngg11 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot( aes(x=length_lure, fill = length_lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Size of the lure\", y = \"\")+\n    scale_fill_brewer(palette=\"Dark2\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg9, gg10, gg11, ncol = 3),\n                left = text_grob(\"Number of catch\", rot = 90)\n)\n\n\n\n\n\n\n\n\n\nOn peut faire de même pour la longueur des poissons capturés :\n\n\nShow the code\ngg12 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length, x = lure, fill=lure)) +\n  geom_boxplot()+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg13 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = colour, fill= colour)) +\n  geom_boxplot()+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ theme(legend.position=\"None\")\n\ngg14 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = length_lure, fill=length_lure)) +\n  geom_boxplot()+\n  labs(x = \"Size of the lure\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"Dark2\")+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg12, gg13, gg14, ncol = 3),\n                left = text_grob(\"Length of fish\", rot = 90)\n)\n\n\n\n\n\n\n\n\n\nAvec ces 6 graphiques, on peut voir que les leurres les plus efficaces pour moi sont les types shad et slug. Mention honorable jerkbait : je n’ai attrapé que 2 poissons avec, mais 2 gros (médiane autour de 47 cm). Les couleurs qui ont le mieux fonctionné sont les couleurs claires et naturelles. Pour la taille des leurres, les plus grands ont tendance à attraper des poissons plus gros en moyenne. Ces conclusions doivent être prises avec des pincettes, car je n’ai pas enregistré le temps passé avec chaque leurre avant d’attraper un poisson. De plus, j’ai tendance à utiliser les mêmes types et couleurs de leurres (par habitude), je vais me forcer à varier davantage."
  },
  {
    "objectID": "posts/2020-09-25-datascience-fishing-part2/index.html#conclusion",
    "href": "posts/2020-09-25-datascience-fishing-part2/index.html#conclusion",
    "title": "Est-ce que R et Shiny peuvent faire de moi un meilleur pêcheur ? Partie 2",
    "section": "Conclusion",
    "text": "Conclusion\nL’analyse de mes données de pêche a été très intéressante et m’a apporté des insights sur mon style de pêche ! J’ai compris que je pêchais presque toujours de la même manière, avec les mêmes habitudes. Bien que cela semble fonctionner pour moi, j’ai une vision biaisée de la façon d’attraper le bar européen. Je dois utiliser des leurres plus grands pour attraper de plus gros poissons et varier les types de leurres utilisés. En effet, je pêche la plupart du temps avec des leurres slug ou shad, d’où le plus grand nombre de prises avec ces types de leurres.\nJe vais continuer à utiliser l’application pour collecter plus de données et mieux comprendre mes sessions de pêche. Je vous tiendrai au courant des résultats ! :wink:"
  },
  {
    "objectID": "posts/2024-07-17-sencrop-data-quality/index.html",
    "href": "posts/2024-07-17-sencrop-data-quality/index.html",
    "title": "L’importance du controle de la qualité des données en Météorologie",
    "section": "",
    "text": "Ça fait longtemps qu’on ne s’est pas vus ! J’ai été assez occupé sur mon nouveau poste, mais je reviens avec un nouvel article sur Medium. 🚀\nLorsque l’on travaille avec des données météorologiques, assurer la qualité des données n’est pas juste un “plus” — c’est une nécessité. Les prévisions météorologiques, les outils d’aide à la décision et les analyses climatiques reposent sur des mesures précises. Mais que se passe-t-il lorsqu’un capteur tombe en panne, qu’une station est installée de manière incorrecte, ou — croyez-le ou non — qu’un oiseau décide de nidifier dans un collecteur de pluie ? 🐦\nChez Sencrop, le réseau de stations météorologiques alimente une variété de processus en aval, des agrégations simples aux outils complexes d’aide à la décision en agriculture. Sans une détection robuste des anomalies, ces processus pourraient être perturbés par des mesures erronées, entraînant des insights inexactes.\nDans cet article, j’explore :\n\nPourquoi le contrôle de la qualité des données est crucial en météorologie\nLes méthodes classiques de contrôle de la qualité des données et pourquoi elles ne sont pas adaptées à notre cas\nComment nous mettons en œuvre une détection des anomalies innovante pour maintenir la fiabilité de nos données\n\nSi vous êtes curieux de savoir comment garder vos données propres et significatives, lisez l’article complet ici : Sencrop’s data quality control: Beyond the Z-score\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2024-12-17-rainfall-interpolation-deep-learning/index.html",
    "href": "posts/2024-12-17-rainfall-interpolation-deep-learning/index.html",
    "title": "Précipitations avec une précision de 1 km² : un mythe qui devient réalité ?",
    "section": "",
    "text": "Encore une fois, j’ai publié un nouvel article sur Medium ! Cette fois, j’explore un défi passionnant en agriculture et en météorologie : Pouvons-nous atteindre la précision des pluviomètres tout en bénéficiant de la large couverture du télédétection ? 🌧📡\n\nPourquoi les données spatiales sur les précipitations sont-elles importantes ?\nLes précipitations influencent tout, de la planification de l’irrigation à la gestion de la santé des cultures. Alors que les pluviomètres traditionnels fournissent des mesures locales très précises, ils ne captent pas les modèles de précipitations sur de plus grandes régions, créant ainsi des zones aveugles pour la prise de décision. Les radars météorologiques et les satellites aident à combler ces lacunes, mais leurs estimations des précipitations au niveau du sol manquent de précision.\n\n\nLe défi et l’approche\nChez Sencrop, nous relevons ce défi avec l’apprentissage profond. Dans cet article, je présente une méthodologie innovante appelée densification, qui fusionne : - Des observations de précipitations rares mais précises provenant de notre réseau de stations météorologiques - Des estimations de précipitations denses mais moins précises provenant des radars et des satellites\nL’objectif ? Fournir des données de précipitations à haute résolution (1 km²) partout en Europe, avec une précision égale ou supérieure à celle de notre réseau de stations.\nCurieux de voir comment nous rendons cela possible ? Découvrez l’article complet ici : Rainfall with a Precision of 1km²—A Myth Becoming Reality?\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.html",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.html",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 4",
    "section": "",
    "text": "Dans ce post, j’explore les données que j’ai collectées au cours de l’année dernière avec la version mise à jour de l’application (présentée ici). Cette rapide analyse exploratoire est réalisée avec deux packages que j’apprécie particulièrement : Plotly et shiny.\nPour rappel, ma nouvelle application stocke les données dans trois fichiers csv. Le premier contient les variables liées aux conditions de pêche. Le deuxième contient des informations sur mes prises et enfin le troisième contient des informations sur les caractéristiques des leurres que j’ai utilisés pendant la session."
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.html#shiny-pour-explorer-les-données-de-pêche-par-session",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.html#shiny-pour-explorer-les-données-de-pêche-par-session",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 4",
    "section": "Shiny pour explorer les données de pêche par session",
    "text": "Shiny pour explorer les données de pêche par session\nJ’ai codé une petite application shiny qui fournit un résumé des conditions de marée et de débit de la rivière, des changements de leurres et des prises pour chaque session. N’hésitez pas à explorer mes données de pêche !"
  },
  {
    "objectID": "posts/2022-04-12-datascience-fishing-part4/index.html#code-de-lapplication-shiny",
    "href": "posts/2022-04-12-datascience-fishing-part4/index.html#code-de-lapplication-shiny",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 4",
    "section": "Code de l’application shiny",
    "text": "Code de l’application shiny\nVoici le code des graphiques plotly dans l’application :\n\nlibrary(plotly)\nlibrary(tidyverse)\n\n\n#' For the tide plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param temporal_range number of hours to display (before and after the session)\n#' @return A plotly object\nplot_tide_ses &lt;- function(dat, n_ses, temporal_range = 4){\n\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly(data = dat_tide, \n          x = ~ hour, \n          y = ~ Water, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type = 'line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1)),\n      list(type = 'line',\n           x0 =  as.POSIXct(dat_t$End),\n           x1 = as.POSIXct(dat_t$End),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*temporal_range ,\n                                        as.POSIXct(dat_t$End) + 3600*temporal_range )),\n                   title = \"\"),\n      yaxis = list(title = \"Tide level\"))\n}\n\n#' For the river flow plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param past_days number of previous to display (before the session)\n#' @return A plotly object\nplot_flow_ses &lt;- function(dat, n_ses, past_days = 4){\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Flow_ts = list(eval(parse(text = Ts_flow))))\n  \n  dat_flow &lt;- as.data.frame(dat_t$Flow_ts)\n  dat_flow$Date &lt;- as.POSIXct(dat_flow$Date, origin = \"1970-01-01\")\n  dat_flow$Nive &lt;- as.numeric(as.character(dat_flow$Nive))\n  dat_flow$Adour &lt;- as.numeric(as.character(dat_flow$Adour))\n  \n  \n  dat_flow &lt;- dat_flow %&gt;% \n    pivot_longer(cols = c(Nive, Adour), \n                 names_to = \"River\",\n                 values_to = \"Flow\")\n  \n  plot_ly(data = dat_flow, \n          x = ~ Date,\n          y = ~ Flow, \n          color = ~ River, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type='line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_flow$Flow),\n           y1 = max(dat_flow$Flow),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*24*past_days,\n                                        as.POSIXct(dat_t$End) )),\n                   title = \"\"))\n}\n\n#' Function to prepare the dataset for the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param session first dataframe with session characteristics\n#' @param ses_n the id (number) of the session\n#' @return A dataframe\nstart_end_fonction &lt;- function(lure, session, ses_n){\n  dat_ses &lt;- session %&gt;% \n    filter(Session == ses_n)\n  \n  dat_lure &lt;- lure %&gt;% \n    filter(n_ses == ses_n)\n  \n  startdates &lt;- dat_lure$time\n  enddates &lt;- c(startdates[-1], dat_ses$End)\n  \n  data.frame(change = length(startdates):1, \n             start = as.POSIXct(startdates),\n             end = as.POSIXct(enddates),\n             type = dat_lure$type_lure,\n             text = paste(dat_lure$color_lure, dat_lure$length_lure))\n}\n\n#' For the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param caught second dataframe with fish caught characteristics\n#' @param session first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @return A plotly object\nlure_change &lt;- function(lure, caught, dat, n_ses){\n  \n  df &lt;- start_end_fonction(lure, dat, n_ses)\n  \n  catch &lt;- caught %&gt;% \n    filter(n_ses == n_ses)\n  \n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly() %&gt;% \n    add_segments(data = df,\n                 x = ~ start,\n                 xend = ~ end,\n                 y = ~ change,\n                 yend = ~ change,\n                 color = ~ type,\n                 #text = ~ text,\n                 size = I(5),\n                 alpha = 0.8) %&gt;%\n    add_segments(x = as.POSIXct(catch$time),\n                 xend = as.POSIXct(catch$time),\n                 y = min(df$change),\n                 yend = max(df$change),\n                 line = list(color = \"red\", dash = \"dash\"),\n                 name = 'Fish caught') %&gt;%\n    add_trace(data = dat_tide, \n              x = ~ hour, \n              y = ~ Water, \n              mode = 'lines', \n              yaxis = \"y2\",\n              name = \"Water level\",\n              alpha = 0.4,\n              hoverinfo = 'skip'\n    ) %&gt;% \n    layout(xaxis = list(range = c(df$start[1] - 1000 , df$end[nrow(df)] + 1000),\n                        title = \"\"),\n           yaxis = list(title = \"\", zeroline = FALSE, showline = FALSE,\n                        showticklabels = FALSE, showgrid = FALSE ),\n           yaxis2 = list(overlaying = \"y\", side = \"right\"))\n}\n\nVoici le code de cette application :\n\nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(shinydashboard)\nlibrary(plotly)\nlibrary(tidyverse)\nsource('plot_functions.R')\ndat &lt;- read_csv(\"session1.csv\")\ncaught &lt;- read_csv(\"catch1.csv\")\nlure &lt;- read_csv(\"lure.csv\")\n\n# In order to save the tide and flow time series I parse the data in the dataframe\n# The following line is used to transform the parsed text into usable values\ndat_t &lt;- dat %&gt;% \n  mutate(Tide_ts = list(eval(parse(text = Ts_tide))),\n         Flow_ts = list(eval(parse(text = Ts_flow))))\n\nbody &lt;- dashboardBody(fluidPage(\n  # Application title\n  h1(\"Exploratory analysis of fishing data\",\n  align = \"center\",\n  style = \"padding: 40px;  text-align: center;  background: #605ca8;  color: white;  font-size: 40px;\"),\n  br(),\n  # Dropdown menu to select the fishing session\n  fluidRow(align = \"center\",\n           pickerInput(inputId = 'Ses',\n                       label = h3('Select a fishing session:'),\n                       choices = unique(dat$Session[-1]),\n                       options = list(\n                         style = \"btn-primary\"),\n                       choicesOpt = list(\n                         style = rep_len(\"font-size: 75%; line-height: 1.6;\", 4)\n                       ))),\n  br(),\n  br(),\n  # Key figures of the session\n  fluidRow(\n    valueBoxOutput(\"progressD\", width = 4),\n    valueBoxOutput(\"progressF\", width = 4),\n    valueBoxOutput(\"progressL\", width = 4)),\n  br(),\n  \n  br(),\n  # Graphs of the tide and river flow of recent days\n  fluidRow(\n    box(title = \"Tidal water level\", status = \"primary\", \n        plotlyOutput(\"TidePlot\"), width = 6),\n    box(title = \"River flow\", status = \"primary\",\n        plotlyOutput(\"FlowPlot\"), width = 6)),\n  br(),\n  # Graph lure changes during the session + catch\n  fluidRow(\n    box(title = \"Lures tested and fish capture\", status = \"warning\", \n        plotlyOutput(\"LurePlot\"), width=12))\n))\n\nui &lt;- dashboardPage(\n  \n  dashboardHeader(disable = TRUE),\n  \n  dashboardSidebar(disable = TRUE),\n  \n  body\n)\n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  # Duration\n  output$progressD &lt;- renderValueBox({\n    Duration = as.integer(difftime(as.POSIXct(dat$End[dat$Session == input$Ses]), as.POSIXct(dat$Beg[dat$Session == input$Ses]), units = 'mins'))\n    valueBox(tags$p(\"Duration\", style = \"font-size: 80%;\"),\n             tags$p(paste(Duration, \"min\"), style = \"font-size: 150%; font-weight: bold;\"),\n             icon = icon(\"clock\"), color = \"purple\")\n  })\n  \n  # Number of fish\n  \n  output$progressF &lt;- renderValueBox({\n    fish_caught = as.integer(caught %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Fish caught\", style = \"font-size: 80%;\"), tags$p(fish_caught, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"trophy\"), color = \"purple\")\n  })\n  \n  # Number of lures tried\n  \n  output$progressL &lt;- renderValueBox({\n    Lure = as.integer(lure %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Lure tried\", style = \"font-size: 80%;\"), tags$p(Lure, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"fish\"), color = \"purple\")\n  })\n  \n  output$TidePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_tide_ses(dat, input$Ses, 4)\n  })\n  output$FlowPlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_flow_ses(dat_t, input$Ses, 4)\n  })\n  output$LurePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    lure_change(lure, caught, dat, input$Ses)\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2022-07-19-top-r-packages/index.html",
    "href": "posts/2022-07-19-top-r-packages/index.html",
    "title": "Analyse des packages R les plus téléchargés",
    "section": "",
    "text": "Après un moment à coder en Python tous les jours pour mon travail, j’avais besoin de faire une pause et d’effectuer quelques analyses en R ! Depuis le début de mon postdoc, je n’avais pas suivi les dernières tendances concernant les packages R. Dans cet article, je vais analyser des données sur les packages R pour voir quels sont les packages les plus téléchargés au cours des dernières semaines. Je vais aussi visualiser toutes les relations entre les packages R en examinant leurs dépendances requises.\nCommençons par importer les packages nécessaires pour cette analyse :\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cranlogs)\nlibrary(igraph)\nlibrary(visNetwork)\n\n\nComment trouver les packages R les plus populaires ?\nLa première étape consiste à collecter les données sur le nombre de téléchargements pour chaque package. Heureusement pour nous, il existe un package appelé cranlogs qui fait exactement ce dont on a besoin ! Avec une simple ligne de commande, on peut collecter les données sur les 50 packages ayant eu le plus de téléchargements au cours du dernier mois, puis on peut tracer le résultat :\n\npopular_pckg &lt;- cran_top_downloads(\"last-month\", 50)\n\npopular_pckg %&gt;%\n  mutate(package = fct_reorder(package, desc(count))) %&gt;% \n  ggplot(aes(x = package, y = count)) +\n  geom_bar(stat=\"identity\") + \n  scale_x_discrete(expand = expansion(mult = c(0, 0.02))) +\n  theme_bw() +\n  xlab(\"\") +\n  ylab(\"Downloads\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 14),\n        legend.position = \"none\") +labs(x = NULL)\n\n\n\n\n\n\n\n\nJe pensais que ce graphique serait plus difficile à réaliser à cause de la disponibilité des données, mais avec le bon package, c’est facile !\n\n\nVisualisation des dépendances entre les packages\nUne fois que j’ai vu le graphique ci-dessus, je me suis demandé quelles étaient toutes les dépendances entre ces packages et je voulais savoir lequel était le plus “connecté”. Pour répondre à cette question, j’avais besoin de plus de données, notamment sur les dépendances requises de chaque package. Après quelques recherches, j’ai découvert que les données sur les packages (y compris la description et les dépendances) pouvaient être extraites avec une fonction du package tools :\n\ndf_pkg &lt;- tools::CRAN_package_db()[, c('Package', 'Imports')]\ndf_pkg &lt;- df_pkg %&gt;% filter(if_any(\"Package\", ~.x %in% popular_pckg[[\"package\"]]))\n\nCependant, cette fonction extrait les données pour tous les packages, et je souhaite effectuer l’analyse uniquement sur les 50 packages les plus populaires. J’ai donc décidé de coupler la fonction du package cranlog avec la base de données que j’ai collectée avec la fonction CRAN_package_db() :\n\n# Can be quite long hence the parallel map\n#plan(multisession, workers = 12)\nmonthly_dl &lt;- map(list(df_pkg$Package), function(x){sum(cran_downloads(x, 'last-month')$count)})\ndf_pkg$monthly_dl &lt;- unlist(monthly_dl)\n# write_csv(df_pkg, 'R_pkg_dl.csv')\n\nOn peut ensuite filtrer par nombre de téléchargements :\n\ndf_pkg &lt;- df_pkg %&gt;% \n  distinct(Package, .keep_all= TRUE) %&gt;% \n  arrange(\"monthly_dl\") \n\nMaintenant, il est temps de préparer les données pour une visualisation graphique. Pour créer un graphique, on a besoin de deux tableaux. Le premier doit contenir toutes les relations entre les nœuds (dans notre cas, les nœuds sont des packages), il a deux colonnes : ‘from’ et ‘to’. Le deuxième tableau contient une seule colonne avec les noms des nœuds.\n\nimport_cleaning &lt;- function(text){\n  text &lt;- gsub('\\\\s*\\\\([^\\\\)]+\\\\)', '', text)\n  text &lt;- gsub('\\\\n', ' ', text)\n  text &lt;- gsub(' ', '', text, fixed = TRUE)\n  text &lt;- str_split(text, ',')\n  return(text)\n}\n\nimport_cleaning(df_pkg$Imports[2])\n\ntest &lt;- df_pkg %&gt;% \n  mutate(cleaned_imports = import_cleaning(Imports))\n\ndf_target &lt;- function(x,y){\n  df &lt;- expand.grid(from=x, to=unlist(y))\n  return(df)}\n\nfor(i in 1:nrow(test)){\n  if(i == 1){\n    df_res = df_target(test$Package[i], test$cleaned_imports[i])\n  }else{\n      df_res = rbind(df_res, df_target(test$Package[i], test$cleaned_imports[i]))\n  }\n}\n\nlinks &lt;- df_res %&gt;% \n  filter(!is.na(to) | (to == \"\"))\n\nnodes &lt;- tibble(id=as.character(unique(unlist(df_res))))\n\nUne fois les deux matrices créées, on peut visualiser interactivement le réseau graphique avec le package visNetwork :\n\nvisNetwork(nodes, links) %&gt;%\n    visIgraphLayout(type = \"full\") %&gt;%\n    visNodes(\n        shape = \"dot\",\n        color = list(\n            background = \"#0085AF\",\n            border = \"#013848\",\n            highlight = \"#FF8000\"\n        ),\n        scaling = list(min=2,\n                       max = 10),\n        shadow = list(enabled = TRUE, size = 10)\n    ) %&gt;%\n    visEdges(\n      arrows='to',\n        shadow = FALSE,\n        color = list(color = \"#0085AF\", highlight = \"#C62F4B\")\n    ) %&gt;%\n    visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T)) %&gt;% \n    visLayout(randomSeed = 11)\n\n\n\n\n\nN’hésitez pas à déplacer, zoomer ou sélectionner des packages pour voir leurs dépendances !\n\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 1",
    "section": "",
    "text": "ℹ️ Note: Lorsque j’ai développé cette application, j’étais débutant en développement web et en gestion des données. J’ai choisi Shiny car c’était une solution simple pour moi à l’époque.\nAvec le recul, si je devais refaire ce projet aujourd’hui, j’opterais plutôt pour une application Django avec une base de données dédiée, ce qui offrirait plus de flexibilité et de robustesse.\nMon passe-temps favori, en plus de R bien sûr, est la pêche. La plupart du temps, je pêche le bar (Dicentrarchus labrax) dans les estuaires. Le bar est un poisson prédateur qui a un large éventail de proies : crabes, lançons, crevettes, gambas et autres poissons. Pour capturer ces prédateurs, je n’utilise pas d’appâts vivants, je préfère utiliser des leurres artificiels qui imitent une proie spécifique.\nEn théorie, attraper un poisson est assez simple :\nEn pratique, c’est une autre histoire ! En effet, l’activité alimentaire, la position du bar européen dans l’estuaire et ses proies varient en fonction de plusieurs paramètres :\nComme vous l’avez compris, de nombreux paramètres peuvent potentiellement influencer les résultats de mes sessions de pêche. C’est pourquoi j’ai décidé de créer une application Shiny pour augmenter le nombre et la taille des poissons capturés durant mes sessions. Pour atteindre cet objectif, je dois mieux comprendre l’activité, la position et les proies du bar en fonction des paramètres décrits ci-dessus."
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#exigences-de-mon-application",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#exigences-de-mon-application",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 1",
    "section": "Exigences de mon application",
    "text": "Exigences de mon application\n\nElle doit stocker les données de mes sessions de pêche :\n\n\n\n\n\n\n\n\n\nInformations nécessaires\nDescription des variables\nSource des données\n\n\n\n\nTemps\nHeure à laquelle un poisson est capturé, durée écoulée depuis le début de la session\nR\n\n\nPrise\nEspèce et taille du poisson capturé\nGéolocalisation via smartphone ?\n\n\nLeurres\nType, longueur, couleur du leurre utilisé\nAPI météo\n\n\n\n\nElle doit enregistrer les données sur mes prises et les leurres artificiels utilisés :\n\n\n\n\n\n\n\n\n\nInformations nécessaires\nDescription des variables\nSource des données\n\n\n\n\nTemps\nHeure à laquelle un poisson est capturé, durée écoulée depuis le début de la session\nR\n\n\nPrise\nEspèce et taille du poisson capturé\nSaisie utilisateur\n\n\nLeurres\nType, longueur, couleur du leurre utilisé\nSaisie utilisateur\n\n\n\n\nElle doit être adaptée aux petits écrans, car je l’utiliserai toujours sur mon téléphone.\nElle doit rester gratuite."
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#collecte-des-données",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#collecte-des-données",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 1",
    "section": "Collecte des données",
    "text": "Collecte des données\n\nRécupération de ma position GPS\nMa position GPS est collectée grâce à un peu de code Javascript intégré dans l’en-tête de l’application Shiny. Ce code a été développé par AugusT et est disponible sur son dépôt GitHub.\n\n\nAPI météo\nPour les données météorologiques, j’ai trouvé une API gratuite appelée Dark Sky. J’ai développé une fonction qui prend en entrée les coordonnées d’un lieu ainsi que la clé utilisateur de l’API et retourne les conditions météorologiques actuelles sous forme de dataframe :\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\n\nweather &lt;- function(x, API_key){\n  url &lt;- paste0(\"https://api.darksky.net/forecast/\",API_key,\n                \"/\", x[1], \",\", x[2],\n                \"?units=ca&exclude=hourly,alerts,flags\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  current.weather.info &lt;- with(table,\n                               data.frame(Air_temp = currently$temperature,\n                                     Weather = currently$summary,\n                                     Atm_pres = currently$pressure,\n                                     Wind_str = currently$windSpeed,\n                                     Wind_dir = currently$windBearing,\n                                     Cloud_cover = currently$cloudCover,\n                                     PrecipProb = currently$precipProbability,\n                                     PrecipInt = currently$precipIntensity,  \n                                     Moon = daily$data$moonPhase[1]))\n  return(current.weather.info)\n}\n\n\n\nWeb scraping des données de marée\nJ’ai créé une fonction pour récupérer des informations sur les marées à partir d’un site web français. La fonction suivante ne prend aucun argument et retourne le niveau d’eau actuel, l’état de la marée (montante ou descendante) ainsi que le temps écoulé depuis le dernier pic de marée pour le lieu où je pêche.\n\ntide &lt;- function(){\n  \n  # Set the current time and time zone \n  Sys.setenv(TZ=\"Europe/Paris\")\n  time &lt;- as.POSIXct(Sys.time())\n  url &lt;- \"https://services.data.shom.fr/hdm/vignette/grande/BOUCAU-BAYONNE?locale=en\"\n  \n  # Read the web page that contains the tide data \n  text &lt;- url %&gt;% \n    read_html() %&gt;%\n    html_text()\n  \n  # Clean the html data to get a dataframe  with two cols Time and water level: \n\n  text &lt;- as.character(sub(\".*var data = *(.*?) *\\\\;.*\", \"\\\\1\", text))\n  text &lt;- unlist(str_split( substr(text, 1, nchar(text)-2), \"\\\\],\"))\n  tidy_df &lt;- data.frame(hour=NA,Water=NA)\n  \n  for(i in 1:length(text)){\n    text_dat &lt;- unlist(str_split(text[i], '\"'))[c(2,3)]\n    text_dat[1] &lt;- substr(text_dat[1], 1, nchar(text_dat[1])-1)\n    text_dat[2] &lt;- as.numeric(substr(text_dat[2], 2, nchar(text_dat[2])))\n    tidy_df[i,] &lt;- text_dat\n  }\n  \n  tidy_df$hour &lt;- as.POSIXct(paste(format(Sys.time(),\"%Y-%m-%d\"), tidy_df$hour))\n  \n  # Some lines to get the tide status (going down or up) : \n  \n  n_closest &lt;- which(abs(tidy_df$hour - time) == min(abs(tidy_df$hour - time)))\n  \n  water_level &lt;- as.numeric(tidy_df[n_closest, 2])\n  \n  all_decrea &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummin(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  all_increa &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummax(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  maree &lt;- ifelse(all_decrea, \"Down\", ifelse(all_increa, \"Up\", \"Dead\"))\n  \n  \n  # Compute time since the last peak :\n  \n  last_peak &lt;- max(cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt; 0)$lengths)\n                   [cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt;0)$lengths) &lt; n_closest])\n  \n  \n  time_after &lt;- as.numeric(difftime(tidy_df$hour[n_closest], tidy_df$hour[last_peak], units = \"mins\"))\n  \n  \n  # Return the list with the results :\n  \n  return(list(Water_level = water_level,\n              Maree = maree,\n              Time_peak = time_after))\n  \n}"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#lapplication-shiny",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#lapplication-shiny",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 1",
    "section": "L’application Shiny",
    "text": "L’application Shiny\nLe principal problème que j’ai rencontré lors du développement de cette application était le stockage des données. Shinyapps.io héberge gratuitement votre application Shiny, mais j’ai rencontré des problèmes lorsque j’ai utilisé l’application pour modifier les fichiers CSV.\nLa solution que j’ai trouvée a été de stocker les données sur mon compte Dropbox. Vous pouvez trouver ici plus de détails sur le sujet ainsi que des solutions alternatives. J’ai utilisé le package rdrop2 pour accéder et modifier les données via l’application Shiny.\nVoici les principales étapes de cette application :\n\nAu démarrage de l’application, un fichier CSV stocké sur mon Dropbox est lu afin de vérifier si une session de pêche est en cours ou non. Si ce n’est pas le cas, l’utilisateur peut démarrer une session de pêche.\nLors du démarrage d’une nouvelle session, une ligne contenant les coordonnées, les conditions météorologiques et les conditions de marée est ajoutée au fichier CSV mentionné précédemment.\nSi un poisson est pêché, l’utilisateur peut remplir un formulaire pour enregistrer les données dans un second fichier CSV. Ce fichier contient : l’heure, l’espèce et la longueur du poisson ainsi que des informations sur le leurre utilisé (type, couleur, longueur).\nL’utilisateur peut mettre fin à la session de pêche en appuyant sur un bouton. Cela enregistre l’heure de fin, les conditions météorologiques et les conditions de marée dans le premier fichier CSV.\n\nUn schéma simplifié est présenté ci-dessous :\n\n\n\nSimplified workflow of the application\n\n\n\nCôté interface utilisateur (UI)\nL’interface utilisateur de l’application est construite en utilisant le package miniUI. Ce package permet aux utilisateurs de R de développer des applications Shiny adaptées aux petits écrans.\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(\n  # Javascript that give user location (input$lat,input$long)\n  tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n                           \n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n                           \n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n  \n  gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n  \n  miniTabstripPanel(\n    #First panel depends if a fishing session is started or not \n    miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                 miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                  uiOutput(\"UI\", align = \"center\"))\n    ),\n    # Second panel displays the location of the previous fishing session with the number of fish caught \n    miniTabPanel(\"Map\", icon = icon(\"map-o\"),\n                 miniContentPanel(scrollable = FALSE,padding = 0,\n                                  div(style=\"text-align:center\",\n                                      prettyRadioButtons(\"radio\", inline = TRUE, label = \"\",\n                                                         choices = list(\"3 dernières sessions\" = 1,\n                                                                        \"3 Meilleures Sessions\" = 2,\n                                                                        \"Tout afficher\" = 3), \n                                                         selected = 1)),\n                                  leafletOutput(\"map\", height = \"93%\")\n                 ))\n  )\n  \n)\n\n\n\nCôté serveur\nLe côté serveur est principalement composé de fonctions observeEvent. L’utilité de chaque observeEvent est indiquée dans le script sous forme de commentaires.\n\nserver &lt;- function(input, output, session){\n  source(\"api_functions.R\")\n  \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session.\n  \n  observeEvent(input$go ,{\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    output$UI&lt;- renderUI({\n      tagList(\n        if(rev(dat$Status)[1] == \"end\"){\n          actionButton(\"go\",\"Start session\")}\n        else{\n          actionButton(\"go\",\"End session\") \n        }\n      )\n    })\n    \n    output$UI_sess&lt;- renderUI({\n      if(rev(dat$Status)[1] == \"end\"){\n        tagList(textInput(\"comments\", label = h3(\"Commentaires\"), value = \"NA\"))\n      }else{\n        input$catch\n        \n        tagList(\n          selectInput(\"species\", label = h3(\"Espèces\"), \n                      choices = list(\"Bar\" = \"bar\", \n                                     \"Bar moucheté\" = \"bar_m\", \n                                     \"Alose\" = \"alose\",\n                                     \"Alose Feinte\" = \"alose_f\",\n                                     \"Maquereau\" = \"maquereau\", \n                                     \"Chinchard\" = \"chinchard\"), selected = \"bar\"),\n          \n          sliderInput(\"length\",label = h3(\"Taille du poisson\"),value=25,min=0,max=80, step=1),\n          \n          selectInput(\"lure\", label = h3(\"Type de leurre\"), \n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"), selectize = FALSE),\n          \n          selectInput(\"color_lure\", label = h3(\"Couleur du leurre\"), \n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ), selectize = FALSE),\n          \n          selectInput(\"length_lure\", label = h3(\"Taille du leurre\"), \n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"), selectize = FALSE),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          textInput(\"comments1\", label = h3(\"Commentaire avant la fin ?\"), value = \"NA\")\n          \n          \n        )\n        \n        \n      }\n      \n    })  \n    \n    \n  }, ignoreNULL = F)\n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    #Tide + geoloc + Weather\n    c_tide &lt;- unlist(tide())\n    geoloc &lt;- c(input$lat,input$long)\n    current.weather.info &lt;- weather(geoloc) \n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(rev(dat$Status)[1] == \"end\"){\n      \n      n_ses &lt;- c(rev(dat$Session)[1]+1)\n      stat_ses &lt;- c(\"beg\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n      \n    }else{\n      \n      n_ses &lt;- c(rev(dat$Session)[1])\n      stat_ses &lt;- c(\"end\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment1 &lt;- input$comments1\n      dat.f&lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment1)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(a), \"session.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    n_ses &lt;- c(rev(dat$Session)[1])\n    time &lt;- as.POSIXct(Sys.time())\n    time_after_beg &lt;- round(as.numeric(difftime(time, rev(dat$Time)[1], units = \"mins\")), digits = 0)\n    \n    catch &lt;- data.frame(n_ses, \n                        time = as.character(time),\n                        min_fishing = as.character(time_after_beg),\n                        species = input$species,\n                        length = input$length,\n                        lure = input$lure,\n                        colour = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(b), \"catch.csv\")\n    # Upload it to dropbox account \n    drop_upload(\"catch.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Create the map with the results of previous session depending on the choice of the user :\n  \n  observeEvent(input$radio,{\n    \n    output$map &lt;- renderLeaflet({\n      map_data &lt;- map_choice(input$radio)\n      leaflet(map_data) %&gt;% addTiles() %&gt;%\n        addPopups(lng = ~Long,\n                  lat = ~Lat, \n                  with(map_data,\n                       sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  poissons &lt;br/&gt; hauteur d'eau: %.0f m, %s, %.0f min après l'étal\",\n                               n_ses,\n                               duration,\n                               Time,\n                               nb,\n                               Water_level,\n                               Tide_status,\n                               Tide_time)),\n                  options = popupOptions(maxWidth = 100, minWidth = 50))\n    })\n    \n  })\n  \n}"
  },
  {
    "objectID": "posts/2020-09-12-datascience-fishing-part1/index.html#conclusion-et-améliorations-futures",
    "href": "posts/2020-09-12-datascience-fishing-part1/index.html#conclusion-et-améliorations-futures",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 1",
    "section": "Conclusion et améliorations futures",
    "text": "Conclusion et améliorations futures\nVous pouvez trouver un exemple de démonstration de cette application (non connectée au compte Dropbox)\nici.\nJ’utilise cette application depuis un an sans aucun problème ! Les données que j’ai collectées seront présentées dans le prochain article.\nDans les mois à venir, je dois trouver une nouvelle API gratuite pour remplacer l’actuelle. En effet, l’API météo que j’utilise a été rachetée par Apple et les requêtes gratuites seront arrêtées l’année prochaine."
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.html",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.html",
    "title": "La fin de ma thèse",
    "section": "",
    "text": "Il y a deux semaines, j’ai défendu ma thèse intitulée ‘Apprentissage statistique pour l’évaluation des risques côtiers’ devant un jury académique. La soutenance s’est très bien passée et le jury ainsi que l’auditoire étaient réellement intéressés par le travail que j’ai accompli durant ma thèse. Cette thèse a été un voyage incroyable et enrichissant tant en termes de connaissances et de recherche, que de collaboration et d’échanges. Je suis maintenant impatient de commencer un nouveau chapitre et de découvrir de nouveaux sujets de travail !\nVous pouvez trouver le manuscrit de ma thèse ici :  http://www.theses.fr/2021PAUU3016. Si vous n’avez pas le courage de lire le manuscrit entier (je comprends :wink:), vous trouverez ci-dessous un court résumé de ma thèse ainsi que les diapositives que j’ai présentées lors de ma soutenance de doctorat."
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.html#résumé-de-ma-thèse",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.html#résumé-de-ma-thèse",
    "title": "La fin de ma thèse",
    "section": "Résumé de ma thèse",
    "text": "Résumé de ma thèse\nAu cours des dernières décennies, la quantité de données liées aux risques côtiers a considérablement augmenté avec l’installation de nombreux réseaux de surveillance. À cette époque de big data, l’utilisation des méthodes d’apprentissage statistique (MAS) dans le développement de modèles prédictifs locaux devient de plus en plus légitime et justifiée. L’objectif de cette thèse est de démontrer comment les MAS peuvent contribuer à l’amélioration des outils d’évaluation des risques côtiers et au développement d’un système d’alerte précoce visant à réduire le risque d’inondation côtière.\nTrois méthodologies ont été développées et testées sur des sites d’étude réels. La première méthodologie vise à améliorer la prévision locale des vagues réalisée par un modèle de vagues spectrales avec des méthodes d’apprentissage automatique et des données provenant des réseaux de surveillance. Nous avons montré que l’assimilation de données avec des méthodes d’apprentissage automatique améliore significativement la prévision des paramètres de vagues, notamment la hauteur et la période des vagues. La deuxième méthodologie concerne la création de bases de données sur les impacts des tempêtes. Bien que ces bases de données soient essentielles pour le processus de réduction des risques de catastrophe, elles sont rares et dispersées. Nous avons donc proposé une méthodologie basée sur une méthode d’apprentissage profond (réseaux neuronaux convolutifs) pour générer automatiquement des données qualitatives sur l’impact des tempêtes à partir d’images fournies par des stations de surveillance vidéo installées sur la côte. La dernière méthodologie concerne le développement d’un modèle d’impact des tempêtes avec une méthode statistique (réseau bayésien) basé exclusivement sur les données acquises par divers réseaux de surveillance. Grâce à cette méthodologie, nous avons pu prédire qualitativement l’impact des tempêtes sur notre site d’étude, la Grande Plage de Biarritz.\n\n\n\nOrganisation du manuscrit de ma thèse"
  },
  {
    "objectID": "posts/2021-10-02-the-end-of-a-journey/index.html#ma-présentation-pour-la-soutenance-de-thèse",
    "href": "posts/2021-10-02-the-end-of-a-journey/index.html#ma-présentation-pour-la-soutenance-de-thèse",
    "title": "La fin de ma thèse",
    "section": "Ma présentation pour la soutenance de thèse",
    "text": "Ma présentation pour la soutenance de thèse\nVous pouvez trouver ma présentation ici"
  },
  {
    "objectID": "posts/2024-09-19-exploring-value-remote-sensing-agriculture/index.html",
    "href": "posts/2024-09-19-exploring-value-remote-sensing-agriculture/index.html",
    "title": "Explorer la valeur du télédétection en agriculture",
    "section": "",
    "text": "Je viens de publier un nouvel article sur Medium explorant le rôle du télédétection en agriculture, spécifiquement comment les données radar peuvent aider à estimer les précipitations plus efficacement ! 📡🌾🌧\nSavoir exactement combien de pluie est tombée sur un champ est crucial pour les agriculteurs. Cela influence les décisions d’irrigation, de prévention des maladies et de gestion de la santé des cultures. Traditionnellement, les pluviomètres ont été la référence pour mesurer les précipitations, mais leur portée est limitée à l’endroit où ils sont physiquement installés.\nVoici le télédétection — les radars météorologiques et les satellites fournissent des estimations de précipitations continuellement dans l’espace, offrant une perspective plus large que ce que les stations au sol seules peuvent offrir. Mais dans quelle mesure sont-elles fiables pour les applications agricoles spécifiques ? Chez Sencrop, nous étudions si les données radar peuvent :\n\nCompléter notre réseau de stations météorologiques, surtout dans les zones moins denses\nAméliorer la qualité des données de précipitations par remplissage des lacunes et détection d’anomalies\nRépondre à la précision requise pour les estimations horaires des précipitations, essentielles pour l’irrigation et la prévision des maladies\n\nDans cet article, je compare les données radar avec les mesures traditionnelles des stations météorologiques pour voir dans quelle mesure elles répondent aux besoins agricoles.\nCurieux de savoir comment le radar peut améliorer la prise de décision basée sur la météo en agriculture ? Lisez l’article complet ici : Exploring the Value of Remote Sensing in Agriculture: Making Sense of Radar Data\n\n\n\n Retour au sommet"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 3",
    "section": "",
    "text": "Dans cet article précédent, j’ai présenté l’application Shiny que j’ai développée pour enregistrer les données de mes sessions de pêche. Dans cet article, je vais présenter brièvement les modifications et les mises à jour que j’ai apportées pour améliorer l’application. Voici les principaux changements :"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#api-météo",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#api-météo",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 3",
    "section": "API Météo",
    "text": "API Météo\nDe petites modifications ont été apportées pour adapter la fonction météo précédente à la nouvelle API météo. Comme cette nouvelle API ne fournit pas de données sur la phase de la lune, j’ai décidé de calculer la phase de la lune avec le package oce :\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(oce)\n\nweather &lt;- function(lat, lon, API_key){\n  url &lt;- paste0(\"api.openweathermap.org/data/2.5/weather?lat=\", lat, \"&lon=\", lon, \"&appid=\", API_key, \"&units=metric\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  # The weather API don't provide moon phase so I compute it with Oce package\n  moon_phase &lt;- round(moonAngle(t = Sys.Date(),\n                                longitude = as.numeric(lon),\n                                latitude = as.numeric(lat))$illuminatedFraction,\n                      3)\n  \n  \n  current.weather.info &lt;- data.frame(Air_temp = table$main$temp,\n                                     Weather = table$weather$main,\n                                     Atm_pres = table$main$pressure,\n                                     Wind_str = table$wind$speed,\n                                     Wind_dir = table$wind$deg,\n                                     Cloud_cover = table$clouds$all,\n                                     PrecipInt = ifelse(is.null(table$rains$`1h`), 0, table$rains$`1h`),  \n                                     Moon = moon_phase)\n  return(current.weather.info)\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#débit-de-la-rivière",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#débit-de-la-rivière",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 3",
    "section": "Débit de la rivière",
    "text": "Débit de la rivière\nJ’ai écrit des fonctions pour scrapper des informations sur les débits des rivières dans lesquelles je pêche le plus, sur un site web :\n\n# Get and prepare the flow data\nget_Qdata &lt;- function(link){\n  table &lt;- fromJSON(content(GET(link), \"text\"))\n  table &lt;- table$Serie$ObssHydro\n  table &lt;- as.data.frame(table)\n  table$DtObsHydro &lt;- sub(\"T\", \" \", table$DtObsHydro)\n  table$DtObsHydro &lt;- substr(table$DtObsHydro, start = 1, stop = 19)\n  ts &lt;- data.frame(Date = seq.POSIXt(as.POSIXct(range(table$DtObsHydro)[1],'%m/%d/%y %H:%M:%S'), \n                                     as.POSIXct(range(table$DtObsHydro)[2],'%m/%d/%y %H:%M:%S'), by=\"hour\"))\n  \n  table$DtObsHydro &lt;- as.POSIXct(table$DtObsHydro, format = \"%Y-%m-%d %H:%M:%S\")\n  \n  table &lt;- full_join(table, ts, by = c(\"DtObsHydro\" = \"Date\")) %&gt;% arrange(DtObsHydro)\n  return(table)\n}\n\n# Main function to collect river flow \n\nriver_flow &lt;- function(){\n  # Url of website to scrap:\n  url_index &lt;- \"https://www.vigicrues.gouv.fr/services/station.json/index.php\"\n  \n  rep &lt;- GET(url_index)\n  \n  table_index &lt;- fromJSON(content(rep, \"text\"))$Stations%&gt;% \n    na.omit()\n  \n  # I need to add the flow of several rivers to get the flow of the rivers I am interested in:\n  stations &lt;- table_index %&gt;% \n    filter(LbStationHydro %in% c(\"Pontonx-sur-l'Adour\", \"St-Pandelon\", \"Artiguelouve\", \"Escos\",\n                                 \"Aïcirits [St-Palais]\", \"Cambo-les-Bains\"))\n  \n  base_url &lt;- \"http://www.vigicrues.gouv.fr/services/observations.json?CdStationHydro=\"\n  height_url &lt;- \"&FormatDate=iso\"\n  Q_url &lt;- \"&GrdSerie=Q\"\n  \n  stations &lt;- stations %&gt;% \n    mutate(WL_link = paste0(base_url, CdStationHydro, height_url),\n           Q_link = paste0(WL_link, Q_url))\n  \n  data_Q &lt;- lapply(stations$Q_link, \n                   function(x){get_Qdata(x)})\n  \n  data_Q &lt;- suppressWarnings(Reduce(function(...) merge(..., all = TRUE, by = \"DtObsHydro\"),\n                   data_Q))\n  \n  names(data_Q) &lt;- c(\"Date\", stations$LbStationHydro) \n  \n  data_Q &lt;- data_Q  %&gt;% \n    mutate(hour_of_day = format(Date, \"%Y-%m-%d %H\"))\n  \n  \n  data_Q &lt;- aggregate(.~hour_of_day, data = data_Q, mean, na.rm = TRUE, na.action = na.pass)\n  \n  data_Q &lt;- imputeTS::na_interpolation(data_Q, option = \"linear\")\n  \n  final_data &lt;- data_Q %&gt;% \n    mutate(Adour = `Pontonx-sur-l'Adour` +  `Aïcirits [St-Palais]` + Artiguelouve + Escos + `St-Pandelon`,\n           Date = as.POSIXct(hour_of_day, tryFormats = \"%Y-%m-%d %H\")) %&gt;% \n    select(Date, `Cambo-les-Bains`, Adour) %&gt;% \n    rename(Nive = `Cambo-les-Bains`)\n  \n  Cur_flow &lt;- data.frame(\"Nive_c\" = final_data[nrow(final_data), 2],\n                         \"Adour_c\" = final_data[nrow(final_data), 3])\n  \n  \n  final_data &lt;- cbind(Cur_flow, final_data) %&gt;% \n    nest(Ts_flow = c(Date, Nive, Adour)) %&gt;% \n    mutate(Ts_flow = paste(Ts_flow))\n\n  return(final_data)\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#application-shiny",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#application-shiny",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 3",
    "section": "Application Shiny",
    "text": "Application Shiny\nUn graphique simplifié de la nouvelle application est montré ci-dessous :\n\n\n\nGraphique simplifié de la nouvelle application\n\n\n\nCôté UI\nLe côté UI n’a pas beaucoup changé, j’ai seulement supprimé l’onglet qui affichait les données de pêche sur une carte car je n’utilisais pas beaucoup cette fonctionnalité :\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n\n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n\n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n               \n               gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n               \n               miniTabstripPanel(\n                 \n                 miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                              \n                              miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                               uiOutput(\"UI\", align = \"center\"))\n                              \n                 )\n               )\n               \n)\n\n\n\nCôté serveur\nPlusieurs changements ont été apportés du côté serveur pour collecter des données sur les leurres que j’utilise. Désormais, chaque fois que je change de leurre, je remplis un petit formulaire pour collecter les caractéristiques du leurre et cela ajoute une ligne dans un troisième fichier csv :\n\nserver &lt;- function(input, output, session){\n  \n  observeEvent(input$go ,{\n    \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session and small survey on lure characteristics.\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    # Reactive UI\n    \n    output$UI &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        # We now indicate what type of lure we use at the beginning of the session:\n        tagList(\n          selectInput(\"lure1\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure1\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure1\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"go\",\"Commencer session !\"))\n      }else{\n        \n        tagList(actionButton(\"go\",\"End session\"))\n      }\n      \n    })\n    \n    output$UI_sess &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        \n        tagList(textInput(\"comments\", label = \"Commentaire avant le début?\", value = \"NA\"))\n        \n      }else{\n        input$catch\n        input$lure\n        tagList(\n          \n          selectInput(\"lure_type\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"lure\",\n                       label = \"Changer de leurre!\"),\n          \n          br(), \n          br(), \n          \n          h4(\"Ajouter une capture\"),\n          \n          selectInput(\"species\", \n                      label = \"Espèces\",\n                      choices = list(\"Bar\" = \"bar\",\n                                     \"Bar moucheté\" = \"bar_m\",\n                                     \"Alose\" = \"alose\",\n                                     \"Maquereau\" = \"maquereau\",\n                                     \"Chinchard\" = \"chinchard\"),\n                      selected = \"bar\"),\n          \n          sliderInput(\"length\",\n                      label = \"Taille du poisson\",\n                      value = 25, \n                      min = 0, \n                      max = 80, \n                      step = 1),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          br(), \n          br(), \n          \n          textInput(\"comments1\", label = h4(\"Commentaire avant la fin ?\"), value = \"NA\")\n        )\n      }\n    })\n  }, ignoreNULL = F)\n  \n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(!is.na(rev(dat$End)[1])){\n      \n      #Tide + geoloc + Weather\n      c_tide &lt;- tide()\n      geoloc &lt;- c(input$lat,input$long)\n      current.weather.info &lt;- weather(lat = geoloc[1], lon = geoloc[2])\n      river.flow &lt;- river_flow()\n      \n      n_ses &lt;- c(rev(dat$Session)[1] + 1)\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;&lt;- cbind(data.frame(n_ses,\n                                 time_beg,\n                                 NA,\n                                 geoloc[2],\n                                 geoloc[1]),\n                      current.weather.info,\n                      c_tide,\n                      river.flow,\n                      comment)\n      names(dat.f) &lt;- names(dat)\n      print(dat.f)\n      final_dat &lt;- rbind(dat, dat.f)\n      \n      lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                            header = T,\n                            stringsAsFactors = F,\n                            dtoken = token)\n      \n      new_lure &lt;- data.frame(n_ses = n_ses,\n                             time = as.character(as.POSIXct(Sys.time())),\n                             type_lure = input$lure1,\n                             color_lure = input$color_lure1,\n                             length_lure = input$length_lure1)\n      \n      new_df &lt;- rbind(lure, \n                      new_lure)\n      \n      write_csv(as.data.frame(new_df), \"lure.csv\")\n      drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n      \n\n    }else{\n      \n      dat$End[nrow(dat)] &lt;- as.character(as.POSIXct(Sys.time()))\n      dat$Comments[nrow(dat)] &lt;- paste(dat$Comments[nrow(dat)], \"/\", input$comments1)\n      final_dat &lt;- dat \n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(final_dat), \"session1.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    catch &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        species = input$species,\n                        length = input$length)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    write_csv(as.data.frame(b), \"catch1.csv\")\n    drop_upload(\"catch1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  observeEvent(input$lure,{\n    lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                          header = T,\n                          stringsAsFactors = F,\n                          dtoken = token)\n    \n    new_lure &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        type_lure = input$lure_type,\n                        color_lure = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    new_df &lt;- rbind(lure, \n               new_lure)\n    \n    write_csv(as.data.frame(new_df), \"lure.csv\")\n    drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n}"
  },
  {
    "objectID": "posts/2021-06-01-datascience-fishing-part3/index.html#conclusion",
    "href": "posts/2021-06-01-datascience-fishing-part3/index.html#conclusion",
    "title": "R et Shiny peuvent-ils faire de moi un meilleur pêcheur ? Partie 3",
    "section": "Conclusion",
    "text": "Conclusion\nJ’ai testé cette nouvelle application lors de deux sessions de pêche et elle fonctionne à merveille. J’ai hâte de vous présenter mes résultats à la fin de cette saison de pêche !"
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html",
    "title": "Analyse des forçages physiques et climatiques impactant le transport des micropolluants dans l’estuaire de l’Adour",
    "section": "",
    "text": "L’objectif de ce travail était de fournir une synthèse descriptive des forçages physiques et climatiques de la plume de l’Adour (un fleuve français), à savoir la houle, le vent et le débit. Cette synthèse vise à estimer les paramètres initiaux d’un modèle hydrodynamique de la plume de l’Adour, qui sera utilisé pour mieux comprendre le transport des micropolluants dans cet estuaire."
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#résumé",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#résumé",
    "title": "Analyse des forçages physiques et climatiques impactant le transport des micropolluants dans l’estuaire de l’Adour",
    "section": "",
    "text": "L’objectif de ce travail était de fournir une synthèse descriptive des forçages physiques et climatiques de la plume de l’Adour (un fleuve français), à savoir la houle, le vent et le débit. Cette synthèse vise à estimer les paramètres initiaux d’un modèle hydrodynamique de la plume de l’Adour, qui sera utilisé pour mieux comprendre le transport des micropolluants dans cet estuaire."
  },
  {
    "objectID": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#productions",
    "href": "projects/2017-07-01-analyzing-environmental-data-micropollutants/index.html#productions",
    "title": "Analyse des forçages physiques et climatiques impactant le transport des micropolluants dans l’estuaire de l’Adour",
    "section": "Productions",
    "text": "Productions\n\nMon mémoire de Master :\n\nAnalyse des forçages physiques et climatiques impactant le transport des micropolluants dans l’estuaire de l’Adour (version française).\n\nApplication Shiny :\n\nDécouvrez ici ma première application Shiny (impressionnante mais lente), qui résume une partie de mon premier stage.\n\nUn article publié dans une revue à comité de lecture :\n\nMorichon, D., de Santiago, I., Delpey, M., Somdecoste, T., Callens, A., Liquet, B., … & Arnould, P. (2018). Assessment of flooding hazards at an engineered beach during extreme events: Biarritz, SW France. Journal of Coastal Research, 85(sp1), 801-805, (10.2112/SI85-161.1)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.html",
    "href": "projects/2018-09-01-robust-regression-time-series/index.html",
    "title": "Régression robuste pour les séries temporelles présentant de l’hétéroscédasticité",
    "section": "",
    "text": "Lors de ce stage, j’ai travaillé sur une nouvelle méthode statistique permettant d’effectuer une régression robuste pour des séries temporelles présentant de l’hétéroscédasticité. Nous avons développé et testé cette méthode sur un jeu de données contenant des mesures de concentration en chlorophylle dans un petit affluent de la Tamise (Royaume-Uni)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.html#résumé",
    "href": "projects/2018-09-01-robust-regression-time-series/index.html#résumé",
    "title": "Régression robuste pour les séries temporelles présentant de l’hétéroscédasticité",
    "section": "",
    "text": "Lors de ce stage, j’ai travaillé sur une nouvelle méthode statistique permettant d’effectuer une régression robuste pour des séries temporelles présentant de l’hétéroscédasticité. Nous avons développé et testé cette méthode sur un jeu de données contenant des mesures de concentration en chlorophylle dans un petit affluent de la Tamise (Royaume-Uni)."
  },
  {
    "objectID": "projects/2018-09-01-robust-regression-time-series/index.html#productions",
    "href": "projects/2018-09-01-robust-regression-time-series/index.html#productions",
    "title": "Régression robuste pour les séries temporelles présentant de l’hétéroscédasticité",
    "section": "Productions",
    "text": "Productions\n\nMon mémoire de Master 2 :\n\nRégression robuste pour les séries temporelles présentant de l’hétérogénéité\n\nDéveloppement dans le package rlmDataDriven :\n\nrlmDD_het.R : cette fonction réalise une régression robuste prenant en compte les corrélations temporelles et l’hétérogénéité.\n\nwhm.R : cette fonction est l’implémentation en R de l’estimation M pondérée.\n\nUn article publié dans une revue à comité de lecture :\n\nCallens, A., Wang, Y., Fu, L. et al. (2020). Robust Estimation Procedure for Autoregressive Models with Heterogeneity. Environmental Modeling & Assessment, (10.1007/s10666-020-09730-w)"
  }
]