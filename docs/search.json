[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Aurélien Callens, I am passionate about data science, coding and fishing ! Professionally, I am a data scientist with a PhD in Applied Mathematics. My doctoral thesis focused on the development of solutions based on machine and deep learning algorithms to solve problems related to coastal risks."
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About me",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n02/2023 - Nowadays: Research data scientist @ Sencrop, Lille, France\n10/2021 - 10/2022: Deep learning research engineer @ SUEZ and University of Pau and Pays de l’Adour, Bidart, France\n09/2018 – 09/2021: PhD Student in Applied Mathematics @ University of Pau and Pays de l’Adour, Anglet, France\n03/2018 – 09/2018: Research intern in Applied Statistics @ Queensland University of Technology, Brisbane, Australia\n03/2017 – 08/2017: Research intern in Applied Statistics @ University of Pau and Pays de l’Adour, Anglet, France"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\n\nPhD. Applied Mathematics, University of Pau and Pays de l’Adour, 2018-2021.\nMaster of Science in Aquatic ecosystem dynamics, University of Pau and Pays de l’Adour, 2016-2018.\nBachelor of Science in Organismal biology and ecology, University of Montpelier, France, 2013-2016."
  },
  {
    "objectID": "posts/2022/ds-fishing-part4.en.html",
    "href": "posts/2022/ds-fishing-part4.en.html",
    "title": "Can R and Shiny make me a better fisherman? Part 4",
    "section": "",
    "text": "In this post, I explore the data I have collected during the last year with the updated version of the application (presented here). This quick exploratory analysis is performed with two packages I really enjoy: Plotly and shiny.\nFor reminder, my new application store the data in three csv files. The first one contains variables related to the fishing conditions. The second one contains information about my catches and finally the third one contains information about the characteristics of the lures I used during the session."
  },
  {
    "objectID": "posts/2022/ds-fishing-part4.en.html#shiny-to-explore-fishing-data-by-session",
    "href": "posts/2022/ds-fishing-part4.en.html#shiny-to-explore-fishing-data-by-session",
    "title": "Can R and Shiny make me a better fisherman? Part 4",
    "section": "Shiny to explore fishing data by session",
    "text": "Shiny to explore fishing data by session\nI coded a small shiny application that provide a summary of the tide and river flow conditions, the lure changes and catches for each session. Don’t hesitate to explore my fishing data!"
  },
  {
    "objectID": "posts/2022/ds-fishing-part4.en.html#code-of-the-shiny-application",
    "href": "posts/2022/ds-fishing-part4.en.html#code-of-the-shiny-application",
    "title": "Can R and Shiny make me a better fisherman? Part 4",
    "section": "Code of the shiny application",
    "text": "Code of the shiny application\nHere is the code of the plotly graphs in the application:\n\nlibrary(plotly)\nlibrary(tidyverse)\n\n\n#' For the tide plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param temporal_range number of hours to display (before and after the session)\n#' @return A plotly object\nplot_tide_ses &lt;- function(dat, n_ses, temporal_range = 4){\n\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly(data = dat_tide, \n          x = ~ hour, \n          y = ~ Water, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type = 'line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1)),\n      list(type = 'line',\n           x0 =  as.POSIXct(dat_t$End),\n           x1 = as.POSIXct(dat_t$End),\n           y0 = min(dat_tide$Water),\n           y1 = max(dat_tide$Water),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*temporal_range ,\n                                        as.POSIXct(dat_t$End) + 3600*temporal_range )),\n                   title = \"\"),\n      yaxis = list(title = \"Tide level\"))\n}\n\n#' For the river flow plot\n#' \n#' @param dat first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @param past_days number of previous to display (before the session)\n#' @return A plotly object\nplot_flow_ses &lt;- function(dat, n_ses, past_days = 4){\n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Flow_ts = list(eval(parse(text = Ts_flow))))\n  \n  dat_flow &lt;- as.data.frame(dat_t$Flow_ts)\n  dat_flow$Date &lt;- as.POSIXct(dat_flow$Date, origin = \"1970-01-01\")\n  dat_flow$Nive &lt;- as.numeric(as.character(dat_flow$Nive))\n  dat_flow$Adour &lt;- as.numeric(as.character(dat_flow$Adour))\n  \n  \n  dat_flow &lt;- dat_flow %&gt;% \n    pivot_longer(cols = c(Nive, Adour), \n                 names_to = \"River\",\n                 values_to = \"Flow\")\n  \n  plot_ly(data = dat_flow, \n          x = ~ Date,\n          y = ~ Flow, \n          color = ~ River, \n          mode = 'lines') %&gt;%  \n    layout(shapes = list(\n      list(type='line',\n           x0 = as.POSIXct(dat_t$Beg),\n           x1 = as.POSIXct(dat_t$Beg),\n           y0 = min(dat_flow$Flow),\n           y1 = max(dat_flow$Flow),\n           line = list(dash = 'dot', width = 1))),\n      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*24*past_days,\n                                        as.POSIXct(dat_t$End) )),\n                   title = \"\"))\n}\n\n#' Function to prepare the dataset for the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param session first dataframe with session characteristics\n#' @param ses_n the id (number) of the session\n#' @return A dataframe\nstart_end_fonction &lt;- function(lure, session, ses_n){\n  dat_ses &lt;- session %&gt;% \n    filter(Session == ses_n)\n  \n  dat_lure &lt;- lure %&gt;% \n    filter(n_ses == ses_n)\n  \n  startdates &lt;- dat_lure$time\n  enddates &lt;- c(startdates[-1], dat_ses$End)\n  \n  data.frame(change = length(startdates):1, \n             start = as.POSIXct(startdates),\n             end = as.POSIXct(enddates),\n             type = dat_lure$type_lure,\n             text = paste(dat_lure$color_lure, dat_lure$length_lure))\n}\n\n#' For the plot of lure change and catch\n#' \n#' @param lure third dataframe with lure changes (hours) and characteristics\n#' @param caught second dataframe with fish caught characteristics\n#' @param session first dataframe with session characteristics\n#' @param n_ses the id (number) of the session\n#' @return A plotly object\nlure_change &lt;- function(lure, caught, dat, n_ses){\n  \n  df &lt;- start_end_fonction(lure, dat, n_ses)\n  \n  catch &lt;- caught %&gt;% \n    filter(n_ses == n_ses)\n  \n  dat_t &lt;- dat %&gt;% \n    filter(Session == n_ses) %&gt;% \n    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))\n  dat_tide &lt;- as.data.frame(dat_t$Tide_ts)\n  dat_tide$hour &lt;- as.POSIXct(dat_tide$hour, origin = \"1970-01-01\")\n  dat_tide$Water &lt;- as.numeric(as.character(dat_tide$Water))\n  \n  plot_ly() %&gt;% \n    add_segments(data = df,\n                 x = ~ start,\n                 xend = ~ end,\n                 y = ~ change,\n                 yend = ~ change,\n                 color = ~ type,\n                 #text = ~ text,\n                 size = I(5),\n                 alpha = 0.8) %&gt;%\n    add_segments(x = as.POSIXct(catch$time),\n                 xend = as.POSIXct(catch$time),\n                 y = min(df$change),\n                 yend = max(df$change),\n                 line = list(color = \"red\", dash = \"dash\"),\n                 name = 'Fish caught') %&gt;%\n    add_trace(data = dat_tide, \n              x = ~ hour, \n              y = ~ Water, \n              mode = 'lines', \n              yaxis = \"y2\",\n              name = \"Water level\",\n              alpha = 0.4,\n              hoverinfo = 'skip'\n    ) %&gt;% \n    layout(xaxis = list(range = c(df$start[1] - 1000 , df$end[nrow(df)] + 1000),\n                        title = \"\"),\n           yaxis = list(title = \"\", zeroline = FALSE, showline = FALSE,\n                        showticklabels = FALSE, showgrid = FALSE ),\n           yaxis2 = list(overlaying = \"y\", side = \"right\"))\n}\n\nHere is the code of this simple yet informative application:\n\nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(shinydashboard)\nlibrary(plotly)\nlibrary(tidyverse)\nsource('plot_functions.R')\ndat &lt;- read_csv(\"session1.csv\")\ncaught &lt;- read_csv(\"catch1.csv\")\nlure &lt;- read_csv(\"lure.csv\")\n\n# In order to save the tide and flow time series I parse the data in the dataframe\n# The following line is used to transform the parsed text into usable values\ndat_t &lt;- dat %&gt;% \n  mutate(Tide_ts = list(eval(parse(text = Ts_tide))),\n         Flow_ts = list(eval(parse(text = Ts_flow))))\n\nbody &lt;- dashboardBody(fluidPage(\n  # Application title\n  h1(\"Exploratory analysis of fishing data\",\n  align = \"center\",\n  style = \"padding: 40px;  text-align: center;  background: #605ca8;  color: white;  font-size: 40px;\"),\n  br(),\n  # Dropdown menu to select the fishing session\n  fluidRow(align = \"center\",\n           pickerInput(inputId = 'Ses',\n                       label = h3('Select a fishing session:'),\n                       choices = unique(dat$Session[-1]),\n                       options = list(\n                         style = \"btn-primary\"),\n                       choicesOpt = list(\n                         style = rep_len(\"font-size: 75%; line-height: 1.6;\", 4)\n                       ))),\n  br(),\n  br(),\n  # Key figures of the session\n  fluidRow(\n    valueBoxOutput(\"progressD\", width = 4),\n    valueBoxOutput(\"progressF\", width = 4),\n    valueBoxOutput(\"progressL\", width = 4)),\n  br(),\n  \n  br(),\n  # Graphs of the tide and river flow of recent days\n  fluidRow(\n    box(title = \"Tidal water level\", status = \"primary\", \n        plotlyOutput(\"TidePlot\"), width = 6),\n    box(title = \"River flow\", status = \"primary\",\n        plotlyOutput(\"FlowPlot\"), width = 6)),\n  br(),\n  # Graph lure changes during the session + catch\n  fluidRow(\n    box(title = \"Lures tested and fish capture\", status = \"warning\", \n        plotlyOutput(\"LurePlot\"), width=12))\n))\n\nui &lt;- dashboardPage(\n  \n  dashboardHeader(disable = TRUE),\n  \n  dashboardSidebar(disable = TRUE),\n  \n  body\n)\n\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  # Duration\n  output$progressD &lt;- renderValueBox({\n    Duration = as.integer(difftime(as.POSIXct(dat$End[dat$Session == input$Ses]), as.POSIXct(dat$Beg[dat$Session == input$Ses]), units = 'mins'))\n    valueBox(tags$p(\"Duration\", style = \"font-size: 80%;\"),\n             tags$p(paste(Duration, \"min\"), style = \"font-size: 150%; font-weight: bold;\"),\n             icon = icon(\"clock\"), color = \"purple\")\n  })\n  \n  # Number of fish\n  \n  output$progressF &lt;- renderValueBox({\n    fish_caught = as.integer(caught %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Fish caught\", style = \"font-size: 80%;\"), tags$p(fish_caught, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"trophy\"), color = \"purple\")\n  })\n  \n  # Number of lures tried\n  \n  output$progressL &lt;- renderValueBox({\n    Lure = as.integer(lure %&gt;% filter(n_ses == input$Ses) %&gt;% nrow())\n    valueBox(tags$p(\"Lure tried\", style = \"font-size: 80%;\"), tags$p(Lure, style = \"font-size: 150%;font-weight: bold;\"),\n             icon = icon(\"fish\"), color = \"purple\")\n  })\n  \n  output$TidePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_tide_ses(dat, input$Ses, 4)\n  })\n  output$FlowPlot &lt;- renderPlotly({\n    # generate plot depending on session\n    plot_flow_ses(dat_t, input$Ses, 4)\n  })\n  output$LurePlot &lt;- renderPlotly({\n    # generate plot depending on session\n    lure_change(lure, caught, dat, input$Ses)\n  })\n  \n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2022/web-scraping-indeed-with-r.html",
    "href": "posts/2022/web-scraping-indeed-with-r.html",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "",
    "text": "A few weeks ago, I started looking for a data scientist position in industry. My first moves were:\nAfter reading numerous job posts and work several hours on my resume, I wondered if I could optimize these steps with R and Data Science. I therefore decided to scrape Indeed and analyze the data about data science jobs to:"
  },
  {
    "objectID": "posts/2022/web-scraping-indeed-with-r.html#loading-libraries",
    "href": "posts/2022/web-scraping-indeed-with-r.html#loading-libraries",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Loading libraries",
    "text": "Loading libraries\nThe first step is to import several packages:\n\n# General\nlibrary(tidyverse)\n# Webscraping \nlibrary(rvest)\nlibrary(RSelenium)\n# Geo data\nlibrary(tidygeocoder)\nlibrary(leaflet)\nlibrary(rnaturalearth)\nlibrary(sf)\n# NLP\nlibrary(udpipe)\nlibrary(textrank)\nlibrary(wordcloud)\n# Cleaning\nlibrary(stringr)\n# Additional functions presented at the end of the post \nsource('scraping_functions.R')"
  },
  {
    "objectID": "posts/2022/web-scraping-indeed-with-r.html#collect-the-data-with-web-scraping",
    "href": "posts/2022/web-scraping-indeed-with-r.html#collect-the-data-with-web-scraping",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Collect the data with web scraping",
    "text": "Collect the data with web scraping\nIn the beginning of this project, I was using read_html() from rvest to access and download the webpage from Indeed. However, Indeed pages are protected by an anti-scrapping software that blocked any of my requests even though scraping is not forbidden on the pages I am interested in (I checked the robots.txt page).\nThis is why I decided to access the pages with Rselenium which allows to run an headless browser. We first navigate to the page corresponding to the search results of data scientist jobs in France:\n\nurl = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&from=searchOnHP\"\n\n# Headless Firefox browser\nexCap &lt;- list(\"moz:firefoxOptions\" = list(args = list('--headless')))\nrD &lt;- rsDriver(browser = \"firefox\", extraCapabilities = exCap, port=1111L,\n                verbose = F)\nremDr &lt;- rD$client\n\n# Navigate to the url\nremDr$navigate(url)\n\n# Store page source \nweb_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\nTo scrape a specific information on a webpage you need to follow these steps:\n\nFind on the web page the element/text/data you want to scrape\nFind the associated xpath or css selector with the developer tool of chrome or firefox ( tutorial here ! )\nExtract the element with hmtl_element() by indicating the xpath or css selector\nTransform the data to text with html_text2()\nClean the data if necessary\n\nHere is the example with the number of listed data science jobs in France:\n\nweb_page %&gt;%\n  html_element(css = \"div.jobsearch-JobCountAndSortPane-jobCount\") %&gt;% # selecting with css \n  html_text2() %&gt;% # Transform to text\n  str_remove_all(\"[^0-9.-]\") %&gt;% # Clean the data to only get numbers\n  substr(start = 2, stop = 8) %&gt;% \n  as.numeric()\n\nFor now, we can only scrape the data from the first page. However, I am interested in all the job posts and I need to access the other pages ! After navigating through the first 3 pages of listed jobs, I remarked a pattern in the URL address (valid at the time of writing), this means that with a line of code, I can produce a list containing the URLs for the first 40 pages.\nOnce I have the list, the only thing left is to loop over all the URLs with some delay (good practice for web-scraping), collect the data and clean it with custom functions (at the end of the post):\n\n# Creating URL link corresponding to the first 40 pages\nbase_url = \"https://fr.indeed.com/jobs?q=data%20scientist&l=France&start=\"\nurl_list &lt;- c(url, paste0(base_url, as.character(seq(from=10, to=400, by=10))))\n\n# Looping through the URL list\nres &lt;- list()\nfor(i in 1:length(url_list)){\n  # Navigate to the URL\n  remDr$navigate(url_list[i])\n  \n  # Store page source \n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n\n  # Job title \n  job_title &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\") %&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_text2() %&gt;%\n    str_replace(\".css.*;\\\\}\", \"\")\n\n  # URL for job post \n  job_url &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\") %&gt;%\n    html_element(\"h2\") %&gt;%\n    html_element(\"a\") %&gt;%\n    html_attr('href') %&gt;%\n    lapply(function(x){paste0(\"https://fr.indeed.com\", x)}) %&gt;%\n    unlist()\n  \n  # Data about company\n  company_info &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_elements(css = \".resultContent\")%&gt;%\n    html_element(css = \".company_location\")%&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_comploc) %&gt;% # Function to clean the textual data\n    do.call(rbind, .)\n\n  # Data about job description\n  job_desc &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result\")%&gt;%\n    html_element(css =\".slider_container .jobCardShelfContainer\")%&gt;%\n    html_text2() %&gt;%\n    tidy_job_desc() # Function to clean the textual data related to job desc.\n\n  # Data about salary (when indicated)\n  salary_hour &lt;- web_page %&gt;%\n    html_elements(css = \".mosaic-provider-jobcards .result .resultContent\")%&gt;%\n    html_element(css = \".salaryOnly\") %&gt;%\n    html_text2() %&gt;%\n    lapply(FUN = tidy_salary) %&gt;% # Function to clean the data related to salary\n    do.call(rbind, .)\n  \n  # Job posts in the same format\n  final_df &lt;- cbind(job_title, company_info, salary_hour, job_desc, job_url)\n  colnames(final_df) &lt;- c(\"Job_title\", \"Company\", \"Location\", \"Rating\", \"Low_salary\", \"High_salary\", \"Contract_info\", \"Job_desc\", \"url\")\n  res[[i]] &lt;- final_df\n  \n  # Sleep 5 seconds, good practice for web scraping\n  Sys.sleep(5)\n}\n\n# Gather all the job post in a tibble\nfinal_df &lt;- as_tibble(do.call(\"rbind\", res))\n\n# Final data cleaning\nfinal_df &lt;- final_df %&gt;%\n  mutate_at(c(\"Rating\", \"Low_salary\", \"High_salary\"), as.numeric)\n\n# Clean job title\nfinal_df$Job_title_c &lt;- clean_job_title(final_df$Job_title)  \nfinal_df$Job_title_c &lt;- as.factor(final_df$Job_title_c)\n\nWe have now a tidy data set! Here is a truncated example of the 5 first rows:\n\n\n\n\n\n\nJob_title\nCompany\nLocation\nRating\nLow_salary\nHigh_salary\nContract_info\nJob_desc\nJob_type\nJob_title_c\n\n\n\n\nData Scientist junior (H/F)\nKea & Partners\n92240 Malakoff\nNA\n3750\n4583\nCDI +2 | Travail en journée +1\nPlusieurs postes à pourvoirMaitrise de Python et des packages de data science. 1er cabinet européen de conseil en stratégie à devenir Société à Mission, certifiés B-Corp depuis 2021*,…\nPrésentiel\ndata scientist junior\n\n\nData Scientist (F ou H)\nSNCF\nSaint-Denis (93)\n3.9\nNA\nNA\nCDI\nLe développement informatique (C, C++, Python, Azure, …). Valider et recetter les phases des projets. Travailler avec des méthodes agiles avec les équipes et…\nPrésentiel\ndata scientist\n\n\nData Scientist (H/F) (IT)\nYzee Services\nParis (75)\n3.3\n2916\n3750\nTemps plein\nRecueillir, structurer et analyser les données pertinentes pour l'entreprise (activité liée à la relation client, conseil en externe).\nPrésentiel\ndata scientist\n\n\nData Scientist H/F\nNatan (SSII)\nParis (75)\nNA\n4583\n5833\nCDI +1 | Travail en journée\nPlusieurs postes à pourvoirVous retrouverez une *ESN ambitieuse portée par le goût de l’excellence.*. Au sein du département en charge d'automatisation transverse des besoins de la…\nPrésentiel\ndata scientist\n\n\nData Scientist Junior H/F / Freelance\nkarma partners\nRoissy-en-Brie (77)\nNA\n400\n550\nTemps plein +1\nLe profil recherché est un profil junior (0-2 ans d'expérience) en data science, avec une appétence technique et des notions d'architecture logicielle et de…\nPrésentiel\ndata scientist junior\n\n\n\n\n\n\n\n\nVisualization of the proposed salaries\nLet’s see if we can get some insights about data science jobs by making some graphical representations. The first thing I wanted to know is how much the companies are willing to pay in order to recruit a data science candidate. I therefore decided to make some figures about the salary range depending on the company and the job title.\nBeware!\nThe following graphs must be taken with a grain of salt as they display a small sample of the data. Indeed, the salary was listed for only 14% of the job post. The insights or trends in these graphs may not be representative of companies that have not listed their proposed salary.\n\nSalary by company\nThe following graphic shows the monthly income listed by some companies (not all the companies list their proposed salary):\n\n# Function to make euro X scale \neuro &lt;- scales::label_dollar(\n  prefix = \"\",\n  suffix = \"\\u20ac\",\n  big.mark = \".\",\n  decimal.mark = \",\"\n)\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% # To remove internships and freelance works\n  select(Company, Low_salary, High_salary) %&gt;%\n  group_by(Company) %&gt;%\n  summarize_if(is.numeric, mean) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n           Company = fct_reorder(Company, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Company)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  geom_hline(aes(yintercept = median(Mean_salary)), lty=2, col='red', alpha = 0.7) +\n  scale_y_continuous(labels = euro) +\n  ylab(\"Monthly income\") +\n  xlab(\"\") +\n  coord_flip() +\n  theme_bw(base_size = 8)\n\n\n\n\nThe median monthly salary is around 3700 euros. As you can see the salaries can vary a lot depending on the company. This is partly due because I didn’t make distinction between the different data science jobs (data scientist, data analyst, data engineer, senior or lead).\n\n\nSalary by job title\nWe can plot the same graph but instead of grouping by company we can group by job title:\n\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;%  # To remove internships and freelance works\n  select(Job_title_c, Low_salary, High_salary, Job_type) %&gt;%\n  group_by(Job_title_c) %&gt;%\n  summarize_if(is.numeric, ~ mean(.x, na.rm = TRUE)) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_title_c = fct_reorder(Job_title_c, desc(-Mean_salary))) %&gt;%\n  ggplot(aes(x = Job_title_c, y = Mean_salary)) +\n  geom_point(aes(y = Mean_salary), colour = \"#267266\") +\n  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +\n  #geom_label(aes(label = n, Job_title_c, y = 1500), data = count_df) + \n  scale_y_continuous(labels = euro) +\n  theme_bw(base_size = 12) +\n  xlab(\"\") +\n  ylab(\"Monthly Income\") +\n  coord_flip()\n\n\n\n\nWe clearly see the differences in proposed salaries depending on the job title: data scientists seem to earn slightly more in average than data analysts. The companies also seem to propose higher salaries for jobs with more responsibilities or requiring more experiences (senior, lead).\n\n\nSalary depending on location: full remote, hybrid, on site ?\nFinally we can plot the salaries depending on the location (full remote, hybrid, on site) to see if it has an impact:\n\n# Tidy the types and locations of listed jobs\nfinal_df &lt;- tidy_location(final_df)\ncount_df &lt;- count(final_df %&gt;% filter(Low_salary &gt; 1600), Job_type)\nfinal_df %&gt;%\n  filter(Low_salary &gt; 1600) %&gt;% \n  drop_na(Location) %&gt;%\n  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),\n         Job_type = as.factor(Job_type)) %&gt;%\n    ggplot(aes(x = Job_type, y = Mean_salary)) +\n  geom_boxplot(na.rm = TRUE) +\n  geom_label(aes(label = n, Job_type, y = 5500), data = count_df) + \n  scale_y_continuous(labels = euro) + \n  theme_bw(base_size = 12) +\n  xlab(\"Job Type\") +\n  ylab(\"Income\")\n\n\n\n\nIt is worth noting that most of the jobs proposed in France are on site jobs. The median salary for this type of jobs is slightly lower than hybrid jobs. The salary distribution of full remote and hybrid jobs must be taken with care as it is only represented by 12 job posts.\n\n\n\nMapping job locations\nDuring my job search, I was frustrated not to see a geographical map regrouping the locations of all the proposed jobs. Such map could help me greatly in my search. Let’s do it !\nFirst, we must tidy and homogenize the locations for all the job posts. To this end, I made a custom function (tidy_location()) which includes some stringr functions, you can find more details about this function at the end of this post. It outputs the location in this format [Town]([Zip code]). Even though all the locations have been homogenized, it can not be plotted on a map (we need the longitude and latitude). To get the latitude and longitude with the town name and zip code I used the geocode() function from tidygeocoder package.\n\n# Extract coordinates from town name\nfinal_df &lt;- final_df %&gt;%\n  mutate(Loc_tidy_fr = paste(Loc_tidy, 'France')) %&gt;%\n  geocode(Loc_tidy_fr, method = 'arcgis', lat = latitude , long = longitude) %&gt;%\n  select(- Loc_tidy_fr)\n\n\nDistribution of Data Science jobs in France\nWe can now represent the number of Data Science jobs by departments:\n\n# Map of France from rnaturalearth package\nfrance &lt;- ne_states(country = \"France\", returnclass = \"sf\") %&gt;% \n  filter(!name %in% c(\"Guyane française\", \"Martinique\", \"Guadeloupe\", \"La Réunion\", \"Mayotte\"))\n\n# Transform location to st point \ntest &lt;- st_sf(final_df, geom= lapply(1:nrow(final_df), function(x){st_point(c(final_df$longitude[x],final_df$latitude[x]))}))\nst_crs(test) &lt;- 4326\n\n# St_join by departments \njoined &lt;- france %&gt;%\n  st_join(test, left = T)\n\n# Custom breaks for visual representation\nmy_breaks = c(0, 2, 5, 10, 30, 50, 100, 260)\n\njoined %&gt;% \n  mutate(region=as.factor(name)) %&gt;% \n  group_by(region) %&gt;% \n  summarize(Job_number=n()) %&gt;% \n  mutate(Job_number = cut(Job_number, my_breaks)) %&gt;% \n  ggplot() +\n  geom_sf(aes(fill=Job_number), col='grey', lwd=0.2) + \n  scale_fill_brewer(\"Job number\",palette = \"GnBu\") + \n  theme_bw()\n\n\n\n\nIt is really interesting to see that the distribution of jobs is quite heterogeneous in France. The majority of the jobs are concentrated in a few departments that include a large city. It is expected as most of the jobs are proposed by large company that are often installed in the proximity of important cities.\n\n\nInteractive map\nWe can go further and plot an interactive map with leaflet which allows us to search dynamically for a job post:\n\n# Plot leaflet map\nfinal_df %&gt;%\n  mutate(pop_up_text = sprintf(\"&lt;b&gt;%s&lt;/b&gt; &lt;br/&gt; %s\",\n                                     Job_title, Company)) %&gt;% # Make popup text\n  leaflet() %&gt;%\n  setView(lng = 2.36, lat = 46.31, zoom = 5.2) %&gt;% # Center of France\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addMarkers(\n    popup = ~as.character(pop_up_text),\n    clusterOptions = markerClusterOptions()\n  )"
  },
  {
    "objectID": "posts/2022/web-scraping-indeed-with-r.html#analyzing-job-descriptions",
    "href": "posts/2022/web-scraping-indeed-with-r.html#analyzing-job-descriptions",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Analyzing job descriptions",
    "text": "Analyzing job descriptions\nNowadays most of the resumes are scanned and interpreted by an applicant tracking system (ATS). To make things simple, this system looks for key words in your resume and assess the match with the job you are applying for. It is therefore important to describe your experiences with specific key words to improve the chances of getting to the next step of the hiring process.\nBut what key words should I include in my resume ? Let’s answer this question by analyzing the job descriptions of data scientist jobs.\n\nDownloading and cleaning each job description\nFirst we download the full description of each job by navigating through all the URL listed in our table. We then clean and homogenize the description with a custom function:\n\n# Loop through all the URLs\njob_descriptions &lt;- list()\npb &lt;- txtProgressBar(min = 1, max = length(final_df$url), style = 3)\nfor(i in 1:length(final_df$url)){\n  remDr$navigate(final_df$url[i])\n  web_page &lt;- remDr$getPageSource(header = TRUE)[[1]] %&gt;% read_html()\n  job_descriptions[[i]] &lt;- web_page %&gt;%\n        html_elements(css = \".jobsearch-JobComponent-description\") %&gt;%\n      html_text2()\n  Sys.sleep(2)\n  setTxtProgressBar(pb, i)\n}\n# Gathering in dataframe\njob_descriptions &lt;- as.data.frame(do.call(\"rbind\", job_descriptions))\nnames(job_descriptions) &lt;- c(\"Description\")\n\n# Binding to same table:\nfinal_df &lt;- cbind(final_df, job_descriptions)\n\n# Homogenize with custom function\nfinal_df$Description_c &lt;- lapply(final_df$Description, function(x){clean_job_desc(x)[[2]]})\nfinal_df$Language &lt;- textcat::textcat(final_df$Description)\n\n\n\nAnnotation procedure with udpipe Package\nThis part is inspired from this post.\nNow that the descriptions of all the listed jobs are imported and pre-cleaned, we can annotate the textual data with udpipe package. This package contains functions and models which can perform tokenisation, lemmatisation and key word extraction.\nWe first restrict this analysis to data scientist job post written in french, then we annotate all the descriptions:\n\n# Restricting the analysis to Data scientist post written in french\ndesc_data_scientist &lt;- final_df %&gt;%\n  filter((Job_title_c == \"data scientist\") & (Language == \"french\")) %&gt;%\n  select(Description_c)\n\nud_model &lt;- udpipe_download_model(language = \"french\") # Download the model if necessary\nud_model &lt;- udpipe_load_model(ud_model$file_model) \n\n# Annotate the descriptions \nx &lt;- udpipe_annotate(ud_model, x = paste(desc_data_scientist, collapse = \" \"))\nx &lt;- as.data.frame(x)\n\n\n\nMost common nouns\nWe can visualize the most employed word throughout the data scientist job posts written in french:\n\nstats &lt;- subset(x, upos %in% \"NOUN\")\nstats &lt;- txt_freq(x = stats$lemma)\n\nstats %&gt;%\n  top_n(50, freq) %&gt;%\n  mutate(key = as.factor(key),\n         key = fct_reorder(key, freq)) %&gt;%\n  ggplot(aes(x = key, y = freq)) +\n  geom_bar(stat = 'identity') +\n  coord_flip() + \n  ylab(\"Most common nouns\") + \n  theme_bw()\n\n\n\n\nEven though, it gives us an idea of words to include it is not very informative as key words are often composed by two or more words.\n\n\nExtracting key words for resume writing\nThere are several methods implemented in udpipe to extract key words from a text. After testing several methods, I selected the Rapid Automatic Keyword Extraction (RAKE) which gives me the best results:\n\nstats &lt;- keywords_rake(x = x,\n                       term = \"token\",# Search on token\n                       group = c(\"doc_id\", \"sentence_id\"), # On every post \n                       relevant = x$upos %in% c(\"NOUN\", \"ADJ\"),  # Only among noun and adj.\n                       ngram_max = 2, n_min = 2, sep = \" \")\n\nstats &lt;- subset(stats, stats$freq &gt;= 5 & stats$rake &gt; 3)\n\nstats %&gt;% \n  arrange(desc(rake)) %&gt;% \n  head()\n\n                    keyword ngram freq     rake\n1 intelligence artificielle     2    9 9.368889\n2             tableaux bord     2    5 8.504274\n3      formation supérieure     2    5 8.374725\n4        modèles prédictifs     2   15 7.581294\n5         force proposition     2    6 7.190238\n6        production échelle     2    5 7.034038\n\nwordcloud(words = stats$keyword, freq = stats$freq, min.freq = 3,\n          max.words=100, random.order=FALSE, rot.per=0.35,\n          colors=brewer.pal(8, \"Dark2\"), scale = c(2.5, .5))\n\n\n\n\nWe can see that this method has selected important french key words related to the data scientist job ! In the first positions, we find the key words: “artificial intelligence”, “dashboards”, “higher education”, “predictive model”. I’d better check if these words appear on my resume !"
  },
  {
    "objectID": "posts/2022/web-scraping-indeed-with-r.html#conclusion",
    "href": "posts/2022/web-scraping-indeed-with-r.html#conclusion",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Conclusion",
    "text": "Conclusion\nI hope I convinced you that it is possible to optimize your job search with Data Science!\nIf this post has caught your interest and you are looking for a new Data Scientist, do not hesitate to contact me on my mail as I am currently looking for a job in France (Hybrid, Remote) or in Europe (Remote)."
  },
  {
    "objectID": "posts/2022/web-scraping-indeed-with-r.html#custom-functions-to-clean-data-extracted-from-the-webpage",
    "href": "posts/2022/web-scraping-indeed-with-r.html#custom-functions-to-clean-data-extracted-from-the-webpage",
    "title": "Optimizing my search for Data scientist jobs by scraping Indeed with R",
    "section": "Custom functions to clean data extracted from the webpage",
    "text": "Custom functions to clean data extracted from the webpage\nThese functions use several methods such as regular expressions, stop words and conditional statements to clean the textual data.\n\nlibrary(rvest)\nlibrary(stringr)\nlibrary(httr)\nlibrary(tidystopwords)\nlibrary(textcat)\n\n# Function to tidy the data related to the company\ntidy_comploc &lt;- function(text){\n  lst &lt;- str_split(text, pattern = \"\\n\", simplify =T)\n  ext_str &lt;- substr(lst[1], nchar(lst[1])-2, nchar(lst[1]))\n  res &lt;- suppressWarnings(as.numeric(gsub(',', '.', ext_str)))\n  lst[1] &lt;- ifelse(is.na(res), lst[1], substr(lst[1], 1, nchar(lst[1])-3))\n  lst[3] &lt;- res\n  t(as.matrix(lst))\n}\n\n# Function to tidy the short job description provided with the job post\ntidy_job_desc &lt;- function(text){\n  stopwords &lt;- c(\"Candidature facile\", \"Employeur réactif\")\n  text &lt;- str_remove_all(text, paste(stopwords, collapse = \"|\"))\n  stopwords_2 &lt;- \"(Posted|Employer).*\"\n  text &lt;- str_remove_all(text, stopwords_2)\n  text\n}\n\n# Function to tidy the salary data if provided\ntidy_salary &lt;- function(text){\n  if(is.na(text)){\n    others &lt;- NA\n    sal_low &lt;- NA\n    sal_high &lt;- NA\n  }else{\n    text &lt;- str_split(text, \"\\n\", simplify = T)\n    others &lt;- paste(text[str_detect(text, \"€\", negate = T)], collapse = \" | \")\n    sal &lt;- text[str_detect(text, \"€\", negate = F)]\n    if(rlang::is_empty(sal)){\n      sal_low &lt;- NA\n      sal_high &lt;- NA\n    }else{\n      range_sal &lt;- as.numeric(str_split(str_remove_all(str_replace(sal, \"à\", \"-\"), \"[^0-9.-]\"), \"-\", simplify = TRUE))\n      sal_low &lt;- sort(range_sal)[1]\n      sal_high &lt;- sort(range_sal)[2]\n\n      if(str_detect(sal, \"an\")){\n        sal_low &lt;- floor(sal_low/12)\n        sal_high &lt;- floor(sal_high/12)\n      }\n    }\n  }\n  return(c(as.numeric(sal_low), as.numeric(sal_high), others))\n}\n\n# Function to tidy the location of the job (Remote/Hybrid/Onsite) + homogenize \n# location and zip code\ntidy_location &lt;- function(final_df){\n  final_df$Job_type &lt;- ifelse(final_df$Location == \"Télétravail\", \"Full Remote\", ifelse(str_detect(final_df$Location, \"Télétravail\"), \"Hybrid\", \"On site\"))\n  final_df$Loc_possibility &lt;- ifelse(str_detect(final_df$Location, \"lieu\"), \"Plusieurs lieux\", NA)\n  stopwords &lt;- c(\"Télétravail à\", \"Télétravail\", \"à\", \"hybride\")\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Location, paste(stopwords, collapse = \"|\"))\n  final_df$Loc_tidy &lt;- str_remove_all(final_df$Loc_tidy, \"[+].*\")\n  final_df$Loc_tidy &lt;- str_trim(final_df$Loc_tidy)\n  final_df$Loc_tidy &lt;-  sapply(final_df$Loc_tidy,\n                               function(x){\n                                 if(!is.na(suppressWarnings(as.numeric(substr(x, 1, 5))))){\n                                   return(paste(substr(x, 7, 30), paste0('(', substr(final_df$Loc_tidy[2], 1, 2), ')')))\n                                 }else{\n                                   return(x)\n                                 }})\n  return(final_df)\n}\n\n# Function to keep only certain words in text\nkeep_words &lt;- function(text, keep) {\n  words &lt;- strsplit(text, \" \")[[1]]\n  txt &lt;- paste(words[words %in% keep], collapse = \" \")\n  return(txt)\n}\n\n# Homogenize the job title and class them in a few categories\nclean_job_title &lt;- function(job_titles){\n  job_titles &lt;- tolower(job_titles)\n  job_titles &lt;- gsub(\"[[:punct:]]\", \" \", job_titles, perl=TRUE)\n\n  words_to_keep &lt;- c(\"data\", \"scientist\", \"junior\", \"senior\", \"engineer\", \"nlp\",\n                     \"analyst\", \"analytics\", \"analytic\", \"science\", \"sciences\",\n                     \"computer\", \"vision\", \"ingenieur\", \"données\", \"analyste\",\n                     \"analyses\", \"lead\", \"leader\", \"dataminer\", \"mining\", \"chief\",\n                     \"miner\", \"analyse\", 'head')\n  job_titles_c &lt;- unlist(sapply(job_titles, function(x){keep_words(x, words_to_keep)}, USE.NAMES = F))\n  job_titles_c &lt;- unlist(sapply(job_titles_c, function(x){paste(unique(unlist(str_split(x, \" \"))), collapse = \" \")}, USE.NAMES = F))\n  table(job_titles_c)\n\n  data_analytics_ind &lt;-  job_titles_c %in% c(\"analyses data\", \"analyst data\", \"analyste data\", \"analyste data scientist\", \"data analyse\",\n                                             \"analyste données\", \"analytic data scientist\", \"analytics data\", \"analytics data engineer\", \"data analyst engineer\",\n                                             \"data analyst données\", \"data analyst scientist\", \"data analyst scientist données\", \"data analyste\", \"data analyst analytics\",\n                                             \"data analytics\", \"data analytics engineer\", \"data engineer analyst\", \"data scientist analyst\", \"data scientist analytics\")\n  job_titles_c[data_analytics_ind] &lt;- \"data analyst\"\n\n  data_analytics_j_ind &lt;-  job_titles_c %in% c(\"junior data analyst\", \"junior data analytics\", \"junior data scientist analyst\")\n  job_titles_c[data_analytics_j_ind] &lt;- \"data analyst junior\"\n\n  data_scientist_ind &lt;- job_titles_c %in% c(\"data computer science\", \"data science\", \"data science scientist\", \"data sciences\",\n                                            \"data sciences scientist\", \"data scientist données\", \"data scientist sciences\",\n                                            \"données data scientist\", \"scientist data\", \"science données\", \"scientist data\",\n                                            \"scientist data science\", \"computer data science\", \"data science données\", \"data scientist science\")\n  job_titles_c[data_scientist_ind] &lt;- \"data scientist\"\n\n  data_scientist_j_ind &lt;- job_titles_c %in% c(\"junior data scientist\")\n  job_titles_c[data_scientist_j_ind] &lt;- \"data scientist junior\"\n\n  data_engineer_ind &lt;- job_titles_c %in% c(\"data engineer scientist\", \"data science engineer\", \"data miner\", \"data scientist engineer\",\n                                           \"dataminer\", \"engineer data scientist\", \"senior data scientist engineer\", \"ingenieur data scientist\")\n  job_titles_c[data_engineer_ind] &lt;- \"data engineer\"\n\n  nlp_data_scientist_ind &lt;- job_titles_c %in% c(\"data scientist nlp\", \"nlp data science\",\n                                                \"nlp data scientist\", \"senior data scientist nlp\")\n  job_titles_c[nlp_data_scientist_ind] &lt;- \"data scientist NLP\"\n\n  cv_data_scientist_ind &lt;- job_titles_c %in% c(\"computer vision data scientist\", \"data science computer vision\",\n                                               \"data scientist computer vision\")\n  job_titles_c[cv_data_scientist_ind] &lt;- \"data scientist CV\"\n\n  lead_data_scientist_ind &lt;- job_titles_c %in% c(\"chief data\", \"chief data scientist\", \"data scientist leader\", \"lead data scientist\",\n                                                 \"data chief scientist\", \"lead data scientist senior\", \"head data science\")\n  job_titles_c[lead_data_scientist_ind] &lt;- \"data scientist lead or higher\"\n  senior_data_scientist_ind &lt;- job_titles_c %in% c(\"senior data scientist\")\n  job_titles_c[senior_data_scientist_ind] &lt;- \"data scientist senior\"\n\n  senior_data_analytics_ind &lt;- job_titles_c %in% c(\"senior analytics data scientist\", \"senior data analyst\", \"senior data scientist analytics\")\n  job_titles_c[senior_data_analytics_ind] &lt;- \"data analyst senior\"\n\n\n  lead_data_analyst_ind &lt;- job_titles_c %in% c(\"lead data analyst senior\", \"lead data analyst\")\n  job_titles_c[lead_data_analyst_ind] &lt;- \"data analyst lead\"\n  return(job_titles_c)\n}\n\n# Function to clean the full job description before word annotation\nclean_job_desc &lt;- function(text){\n  text &lt;- tolower(text)\n  text &lt;- str_replace_all(text, \"\\n\", \" \")\n  text &lt;- str_remove(text, pattern = \"dé.*du poste \")\n  text &lt;- str_remove(text, pattern = \"analyse de recr.*\")\n  text &lt;- gsub(\"(?!&)[[:punct:]+’+…+»+«]\", \" \", text, perl=TRUE)\n\n  language &lt;- textcat(text)\n\n  if(language == \"french\"){\n    text &lt;- str_replace_all(text, \"œ\", \"oe\")\n    stopwords &lt;- c(\"détails\", \"poste\", \"description\", \"informations\", \"complémentaires\", \"c\", generate_stoplist(language = \"French\"))\n  }else{\n    stopwords &lt;- c(\"description\", generate_stoplist(language = \"English\"))\n  }\n\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n  text &lt;- str_replace_all(text, paste(stopwords, collapse = \" | \"), \" \")\n\n  return(c(language, text))\n}"
  },
  {
    "objectID": "posts/2020/ds-fishing-part1.en.html",
    "href": "posts/2020/ds-fishing-part1.en.html",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "",
    "text": "My favorite hobby, in addition to R coding of course, is fishing. Most of the time, I fish European sea bass (Dicentrarchus labrax) in estuaries. The sea bass is a predatory fish that has a broad range of preys: crabs, sand eels, prawns, shrimps and other fish. To catch these predators, I don’t use live baits, I prefer to use artificial lures that imitate a specific prey.\nIn theory, it is quite easy to catch a fish:\nIn practice, it is an other story ! Indeed, the feeding activity, the position of the European sea bass in the estuary and their preys will vary depending on different parameters:\nAs you understand, there are many parameters potentially influencing the results of my fishing session. This is why I decided to create a shiny application to augment the number and the length of the fish caught during my sessions. To reach this objective, I need to better understand the activity, the position and the prey of the sea bass depending on the parameters described above."
  },
  {
    "objectID": "posts/2020/ds-fishing-part1.en.html#requirements-of-my-application",
    "href": "posts/2020/ds-fishing-part1.en.html#requirements-of-my-application",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "Requirements of my application",
    "text": "Requirements of my application\n\nIt must store data about my fishing session:\n\n\n\n\n\n\n\n\n\nInformation needed\nDescription of the variables\nWhere do I get the data ?\n\n\n\n\nTime\nTime when a fish is caught, time since the beginning of the session\nR\n\n\nCatch\nSpecies and length of the fish caught\nGeolocation from smartphone?\n\n\nLures\nType, length, color of lure used\nWeather API\n\n\n\n\nIt must record data about my catch and the artificial lures used:\n\n\n\n\n\n\n\n\n\nInformation needed\nDescription of the variables\nWhere do I get the data ?\n\n\n\n\nTime\nTime when a fish is caught, time since the beginning of the session\nR\n\n\nCatch\nSpecies and length of the fish caught\nUser input\n\n\nLures\nType, length, color of lure used\nUser input\n\n\n\n\nIt must be adapted to small screens because I will always use the application on my phone.\nIt must remain free."
  },
  {
    "objectID": "posts/2020/ds-fishing-part1.en.html#collecting-the-data",
    "href": "posts/2020/ds-fishing-part1.en.html#collecting-the-data",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "Collecting the data",
    "text": "Collecting the data\n\nGetting my gps location\nMy gps location is collected by using a bit of Javascript in the header of the shiny application. This code has been developed by AugusT and is available on his github repository.\n\n\nWeather API\nFor the weather data, I found a free API called Dark Sky. I made a function that takes as input the coordinates of a place and the API user key and returns the current weather conditions in a dataframe:\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\n\nweather &lt;- function(x, API_key){\n  url &lt;- paste0(\"https://api.darksky.net/forecast/\",API_key,\n                \"/\", x[1], \",\", x[2],\n                \"?units=ca&exclude=hourly,alerts,flags\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  current.weather.info &lt;- with(table,\n                               data.frame(Air_temp = currently$temperature,\n                                     Weather = currently$summary,\n                                     Atm_pres = currently$pressure,\n                                     Wind_str = currently$windSpeed,\n                                     Wind_dir = currently$windBearing,\n                                     Cloud_cover = currently$cloudCover,\n                                     PrecipProb = currently$precipProbability,\n                                     PrecipInt = currently$precipIntensity,  \n                                     Moon = daily$data$moonPhase[1]))\n  return(current.weather.info)\n}\n\n\n\nWeb scrapping for Tide data\nI created a function to scrap information about the tide on a french website. The following function takes no argument and return the current water level, the tide status (going up or down) and time since the tide peak for the location I fish.\n\ntide &lt;- function(){\n  \n  # Set the current time and time zone \n  Sys.setenv(TZ=\"Europe/Paris\")\n  time &lt;- as.POSIXct(Sys.time())\n  url &lt;- \"https://services.data.shom.fr/hdm/vignette/grande/BOUCAU-BAYONNE?locale=en\"\n  \n  # Read the web page that contains the tide data \n  text &lt;- url %&gt;% \n    read_html() %&gt;%\n    html_text()\n  \n  # Clean the html data to get a dataframe  with two cols Time and water level: \n\n  text &lt;- as.character(sub(\".*var data = *(.*?) *\\\\;.*\", \"\\\\1\", text))\n  text &lt;- unlist(str_split( substr(text, 1, nchar(text)-2), \"\\\\],\"))\n  tidy_df &lt;- data.frame(hour=NA,Water=NA)\n  \n  for(i in 1:length(text)){\n    text_dat &lt;- unlist(str_split(text[i], '\"'))[c(2,3)]\n    text_dat[1] &lt;- substr(text_dat[1], 1, nchar(text_dat[1])-1)\n    text_dat[2] &lt;- as.numeric(substr(text_dat[2], 2, nchar(text_dat[2])))\n    tidy_df[i,] &lt;- text_dat\n  }\n  \n  tidy_df$hour &lt;- as.POSIXct(paste(format(Sys.time(),\"%Y-%m-%d\"), tidy_df$hour))\n  \n  # Some lines to get the tide status (going down or up) : \n  \n  n_closest &lt;- which(abs(tidy_df$hour - time) == min(abs(tidy_df$hour - time)))\n  \n  water_level &lt;- as.numeric(tidy_df[n_closest, 2])\n  \n  all_decrea &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummin(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  all_increa &lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==\n                      cummax(tidy_df$Water[(n_closest-6):(n_closest+6)] ))\n  \n  maree &lt;- ifelse(all_decrea, \"Down\", ifelse(all_increa, \"Up\", \"Dead\"))\n  \n  \n  # Compute time since the last peak :\n  \n  last_peak &lt;- max(cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt; 0)$lengths)\n                   [cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &gt;0)$lengths) &lt; n_closest])\n  \n  \n  time_after &lt;- as.numeric(difftime(tidy_df$hour[n_closest], tidy_df$hour[last_peak], units = \"mins\"))\n  \n  \n  # Return the list with the results :\n  \n  return(list(Water_level = water_level,\n              Maree = maree,\n              Time_peak = time_after))\n  \n}"
  },
  {
    "objectID": "posts/2020/ds-fishing-part1.en.html#the-shiny-application",
    "href": "posts/2020/ds-fishing-part1.en.html#the-shiny-application",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "The shiny application",
    "text": "The shiny application\nThe main problem I encountered while developing this application was data storage. Shinyapps.io host freely your shiny application but there were some problems when I used the shiny application to modify the csv files. The solution I found was to store the data in my dropbox account, you can find here more details on the subject and alternatives solutions. I used the package rdrop2 to access and modify the data with the shiny application.\nHere are the main steps of this application :\n\nWhen the application is started, it reads a csv file stored on my dropbox to see if a fishing session is running or not. If not the user can start a fishing session.\nWhen starting a new session, a line with coordinates, weather conditions, and tide condition is added to the csv file previously mentioned.\nIf a fish is caught, the user can fill out a form to store the data in a second csv file. This file contains : the time, the species and length of the fish and information about the fishing lure used (type, color, length).\nThe user can end the fishing session by pushing a button. This will register the ending time, weather conditions, and tide condition in the first csv file.\n\nA simplified graph is showed below:\n\n\n\nSimplified workflow of the application\n\n\n\nUI side\nThe user interface of the application is built using the miniUI package. This package allows R user to develop shiny application adapted to small screens.\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(\n  # Javascript that give user location (input$lat,input$long)\n  tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n                           \n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n                           \n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n  \n  gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n  \n  miniTabstripPanel(\n    #First panel depends if a fishing session is started or not \n    miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                 miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                  uiOutput(\"UI\", align = \"center\"))\n    ),\n    # Second panel displays the location of the previous fishing session with the number of fish caught \n    miniTabPanel(\"Map\", icon = icon(\"map-o\"),\n                 miniContentPanel(scrollable = FALSE,padding = 0,\n                                  div(style=\"text-align:center\",\n                                      prettyRadioButtons(\"radio\", inline = TRUE, label = \"\",\n                                                         choices = list(\"3 dernières sessions\" = 1,\n                                                                        \"3 Meilleures Sessions\" = 2,\n                                                                        \"Tout afficher\" = 3), \n                                                         selected = 1)),\n                                  leafletOutput(\"map\", height = \"93%\")\n                 ))\n  )\n  \n)\n\n\n\nServer side\nThe server side is mainly composed by observeEvent functions. The utility of each observeEvent is provided in the script as commentary.\n\nserver &lt;- function(input, output, session){\n  source(\"api_functions.R\")\n  \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session.\n  \n  observeEvent(input$go ,{\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    output$UI&lt;- renderUI({\n      tagList(\n        if(rev(dat$Status)[1] == \"end\"){\n          actionButton(\"go\",\"Start session\")}\n        else{\n          actionButton(\"go\",\"End session\") \n        }\n      )\n    })\n    \n    output$UI_sess&lt;- renderUI({\n      if(rev(dat$Status)[1] == \"end\"){\n        tagList(textInput(\"comments\", label = h3(\"Commentaires\"), value = \"NA\"))\n      }else{\n        input$catch\n        \n        tagList(\n          selectInput(\"species\", label = h3(\"Espèces\"), \n                      choices = list(\"Bar\" = \"bar\", \n                                     \"Bar moucheté\" = \"bar_m\", \n                                     \"Alose\" = \"alose\",\n                                     \"Alose Feinte\" = \"alose_f\",\n                                     \"Maquereau\" = \"maquereau\", \n                                     \"Chinchard\" = \"chinchard\"), selected = \"bar\"),\n          \n          sliderInput(\"length\",label = h3(\"Taille du poisson\"),value=25,min=0,max=80, step=1),\n          \n          selectInput(\"lure\", label = h3(\"Type de leurre\"), \n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"), selectize = FALSE),\n          \n          selectInput(\"color_lure\", label = h3(\"Couleur du leurre\"), \n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ), selectize = FALSE),\n          \n          selectInput(\"length_lure\", label = h3(\"Taille du leurre\"), \n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"), selectize = FALSE),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          textInput(\"comments1\", label = h3(\"Commentaire avant la fin ?\"), value = \"NA\")\n          \n          \n        )\n        \n        \n      }\n      \n    })  \n    \n    \n  }, ignoreNULL = F)\n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    #Tide + geoloc + Weather\n    c_tide &lt;- unlist(tide())\n    geoloc &lt;- c(input$lat,input$long)\n    current.weather.info &lt;- weather(geoloc) \n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(rev(dat$Status)[1] == \"end\"){\n      \n      n_ses &lt;- c(rev(dat$Session)[1]+1)\n      stat_ses &lt;- c(\"beg\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n      \n    }else{\n      \n      n_ses &lt;- c(rev(dat$Session)[1])\n      stat_ses &lt;- c(\"end\")\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment1 &lt;- input$comments1\n      dat.f&lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment1)\n      names(dat.f)&lt;-names(dat)\n      a &lt;- rbind(dat,dat.f)\n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(a), \"session.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch.csv\", header = T, stringsAsFactors = F, dtoken = token) \n    \n    n_ses &lt;- c(rev(dat$Session)[1])\n    time &lt;- as.POSIXct(Sys.time())\n    time_after_beg &lt;- round(as.numeric(difftime(time, rev(dat$Time)[1], units = \"mins\")), digits = 0)\n    \n    catch &lt;- data.frame(n_ses, \n                        time = as.character(time),\n                        min_fishing = as.character(time_after_beg),\n                        species = input$species,\n                        length = input$length,\n                        lure = input$lure,\n                        colour = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(b), \"catch.csv\")\n    # Upload it to dropbox account \n    drop_upload(\"catch.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Create the map with the results of previous session depending on the choice of the user :\n  \n  observeEvent(input$radio,{\n    \n    output$map &lt;- renderLeaflet({\n      map_data &lt;- map_choice(input$radio)\n      leaflet(map_data) %&gt;% addTiles() %&gt;%\n        addPopups(lng = ~Long,\n                  lat = ~Lat, \n                  with(map_data,\n                       sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  poissons &lt;br/&gt; hauteur d'eau: %.0f m, %s, %.0f min après l'étal\",\n                               n_ses,\n                               duration,\n                               Time,\n                               nb,\n                               Water_level,\n                               Tide_status,\n                               Tide_time)),\n                  options = popupOptions(maxWidth = 100, minWidth = 50))\n    })\n    \n  })\n  \n}"
  },
  {
    "objectID": "posts/2020/ds-fishing-part1.en.html#conclusion-and-future-improvments",
    "href": "posts/2020/ds-fishing-part1.en.html#conclusion-and-future-improvments",
    "title": "Can R and Shiny make me a better fisherman? Part 1",
    "section": "Conclusion and future improvments",
    "text": "Conclusion and future improvments\nYou can find a dummy example of this application (not linked to the dropbox account) here. I have been using this application for 1 year without any problems! The data I collected will be presented in the next post.\nIn the coming months, I must find a new free API to replace the actual one. Indeed, the weather API I am using has been bought by Apple and the free requests will be stopped in the following year."
  },
  {
    "objectID": "posts/2021/the-end-of-a-journey.html",
    "href": "posts/2021/the-end-of-a-journey.html",
    "title": "The end of my PhD journey",
    "section": "",
    "text": "Two weeks ago, I defended my thesis entitled ‘Statistical learning for coastal risks assessment’ in front of an academic jury. The defense went extremely well and the jury and the auditory were genuinely interested by the work I had done during my thesis. This thesis was an amazing and enriching journey in terms of knowledge and research, but also in terms of collaboration and exchange. I now am excited to start a new journey and discover new subjects to work on !\nYou can find my thesis manuscript here:  http://www.theses.fr/2021PAUU3016. If you don’t have the courage to read the entire manuscript (I understand :wink:) you can find below a short abstract of my thesis and the slides I presented during my PhD defense."
  },
  {
    "objectID": "posts/2021/the-end-of-a-journey.html#abstract-of-my-thesis",
    "href": "posts/2021/the-end-of-a-journey.html#abstract-of-my-thesis",
    "title": "The end of my PhD journey",
    "section": "Abstract of my thesis",
    "text": "Abstract of my thesis\nOver the last decades, the quantity of data related to coastal risk has greatly increased with the installation of numerous monitoring networks. In this era of big data, the use of statistical learning methods (SLM) in the development of local predictive models becomes more legitimate and justified. The objective of this thesis is to demonstrate how SLM can contribute to the improvement of coastal risk assessment tools and to the development of an early warning system which aims to reduce coastal flooding risk.\nThree methodologies have been developed and tested on real study sites. The first methodology aims to improve the local wave forecast made by spectral wave model with machine learning methods and data from monitoring networks. We showed that data assimilation with machine learning methods improve significantly the forecast of wave parameters especially the wave height and period. The second methodology concerns the creation of storm impact databases. Even though these databases are essential for the disaster risk reduction process they are rare and sparse. We therefore proposed a methodology based on a deep learning method (convolutional neural networks) to generate automatically qualitative storm impact data from images provided by video monitoring stations installed on the coast. The last methodology is about the development of a storm impact model with a statistical method (bayesian network) based exclusively on data acquired with diverse monitoring networks. With this methodology we were able to predict qualitatively the storm impact on our study site, the Grande Plage of Biarritz.\n\n\n\nOrganization of my PhD manuscript"
  },
  {
    "objectID": "posts/2021/the-end-of-a-journey.html#my-presentation-for-the-phd-defense",
    "href": "posts/2021/the-end-of-a-journey.html#my-presentation-for-the-phd-defense",
    "title": "The end of my PhD journey",
    "section": "My presentation for the PhD defense",
    "text": "My presentation for the PhD defense"
  },
  {
    "objectID": "posts/2021/the-end-of-a-journey.html#appendix-how-to-make-this-slide-show-from-a-pdf-file",
    "href": "posts/2021/the-end-of-a-journey.html#appendix-how-to-make-this-slide-show-from-a-pdf-file",
    "title": "The end of my PhD journey",
    "section": "Appendix: How to make this slide show from a pdf file",
    "text": "Appendix: How to make this slide show from a pdf file\nThe first step is to transform the presentation file (pdf format) into a collection of images (jpeg format) inside a specific folder with the R package pdftools:\nlibrary(pdftools)\npdf_convert(\"filename.pdf\", format = \"jpeg\", pages = NULL, filenames = NULL, dpi = 300, opw = \"\", upw = \"\", verbose = TRUE)\nThen we add the short code “gallery-slider.html” presented in this github repository in this folder: Website_Folder/themes/beautifulhugo/layouts/shortcodes/\nFinally we include the gallery slider our post with the following line of code:\n{{&lt; gallery-slider dir=\"Slides/\" width=\"100%\" height=\"500px\" &gt;}}\nThe directory containing the images must be placed in the Website_Folder/static/ folder."
  },
  {
    "objectID": "posts/2021/coastal-risks-google-trends.en.html",
    "href": "posts/2021/coastal-risks-google-trends.en.html",
    "title": "Coastal risks and statistical learning: Analyzing Google trends with gtrendsR package",
    "section": "",
    "text": "I am currently in the third year of my PhD and it is time for me to synthesize all the work I have done by writing my thesis. One important step is the introduction which presents the general context and the contributions of my thesis in the research world. For my introduction, I decided to include the Google trend analysis of specific terms related to my PhD subject. For information, the aim of my PhD is to demonstrate the potential contributions of statistical learning methods in the study of coastal risks.\nIn this post, we are going to use the R package gtrendsR to analyze the trends of specific words related to statistical learning such as “machine learning” or “data science” and also related to coastal risks: “coastal flood”, “storm surge”."
  },
  {
    "objectID": "posts/2021/coastal-risks-google-trends.en.html#analysis-of-specific-terms-for-the-whole-world",
    "href": "posts/2021/coastal-risks-google-trends.en.html#analysis-of-specific-terms-for-the-whole-world",
    "title": "Coastal risks and statistical learning: Analyzing Google trends with gtrendsR package",
    "section": "Analysis of specific terms for the whole world",
    "text": "Analysis of specific terms for the whole world\nIn this first section, we are going to analyze the interest over time for several terms in the whole world. The gtrends function extracts the “hits” variable for a given term. This variable is a normalized measure of the interest made by Google Trends. The number of search is normalized by geography and time range, then the resulting numbers are scaled on a range of 0 to 100 based on a topic’s proportion to all searches on all topics. All the details are given in the support of Google Trends (here).\nWe can look at the interest over time for “data science”, “deep learning” and “machine learning”:\n\nlibrary(tidyverse)\nlibrary(gtrendsR)\n\nwords_stats &lt;- c(\"machine learning\",\n           \"deep learning\",\n           \"data science\")\n\nres_stats &lt;- lapply(words_stats, function(x){\n  gtrends(x,\n          geo = \"\",\n          time = \"all\")})\n\n\ninterest_stats &lt;- do.call(\"rbind\", \n                    lapply(res_stats, function(x){x$interest_over_time}))\n\ninterest_stats %&gt;% \n  mutate(hits = as.numeric(ifelse(hits == \"&lt;1\", 0, hits))) %&gt;% \n  ggplot(aes(x = date, y = hits)) + \n  geom_line() + \n  facet_grid(~ keyword) +\n  theme_bw()\n\n\n\n\nFrom this figure, we see the increasing interest for these terms over the last decade.\nWe can do the same for terms related to coastal risks :\n\nwords_coastal &lt;- c(\"coastal risk\",\n                   \"coastal flood\",\n                   \"storm surge\")\n\n\nres_coast &lt;- lapply(words_coastal, function(x){\n  gtrends(x,\n          geo = \"\",\n          time = \"all\")})\n\n\ninterest_coast &lt;- do.call(\"rbind\", lapply(res_coast, function(x){x$interest_over_time}))\n\ninterest_coast %&gt;% \n  mutate(hits = as.numeric(ifelse(hits == \"&lt;1\", 0, hits))) %&gt;% \n  ggplot(aes(x = date, y = hits)) + \n  geom_line() + \n  facet_grid(~ keyword) +\n  theme_bw()\n\n\n\n\nContrary to the interest for the statistical terms, the interest in terms related to coastal risk is more punctual. One of my hypothesis is that the interest in these topic increases after a large storm event has occurred in the world. We will investigate on this hypothesis at the end of the post."
  },
  {
    "objectID": "posts/2021/coastal-risks-google-trends.en.html#mapping-the-interest-of-storm-surge-and-data-science-in-the-world",
    "href": "posts/2021/coastal-risks-google-trends.en.html#mapping-the-interest-of-storm-surge-and-data-science-in-the-world",
    "title": "Coastal risks and statistical learning: Analyzing Google trends with gtrendsR package",
    "section": "Mapping the interest of “storm surge” and “data science” in the world",
    "text": "Mapping the interest of “storm surge” and “data science” in the world\nIn addition to analyze the trend over time with gtrendsR, we can also visualize the interest depending on the countries.\nLet’s start with “storm surge”:\n\nres_world &lt;- gtrends(\"Storm surge\",\n          geo = \"\",\n          time = \"all\")\n\n\nInt_country &lt;- as_tibble(res_world$interest_by_country)\n\nInt_country &lt;- Int_country %&gt;% \n  dplyr::rename(region = location)\n\nWolrdMap = ggplot2::map_data(map = \"world\")\n\n\nInt_Merged &lt;- Int_country %&gt;% \n  dplyr::full_join(x = ., \n                   y = WolrdMap , \n                   by = \"region\") %&gt;% \n  mutate(hits = as.numeric(hits))\n\n\nInt_Merged %&gt;% \n  ggplot(aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group, \n                   fill = hits), \n               colour = \"grey40\",\n               size = 0.2) +\n  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(7, \"OrRd\"),\n                       na.value = \"white\") +\n  coord_cartesian(ylim = c(-55, 85)) + \n  theme(legend.position=\"bottom\") +\n  theme_bw() + \n  labs(x = \"\", y = \"\") + \n  theme(panel.grid.major = element_line(size = 0.5, linetype = 2),\n        panel.grid.minor = element_blank()) +\n  labs(x = \"\", y = \"\")\n\n\n\n\nWe see from this map that the number of Google searches for this term is quite low for many countries in comparison with other terms (white color means not enough search comparing to the total number of search). In general the interest is higher in countries that are more likely to be impacted by marine storms or hurricanes. The Republic of Philippines seems the most interested country in storm surge.\nWe can then continue with the term “data science”:\n\nres_world &lt;- gtrends(\"Data Science\",\n          geo = \"\",\n          time = \"all\")\n\n\nInt_country &lt;- as_tibble(res_world$interest_by_country)\n\nInt_country &lt;- Int_country %&gt;% \n  dplyr::rename(region = location)\n\nWolrdMap = ggplot2::map_data(map = \"world\")\n\n\nInt_Merged &lt;- Int_country %&gt;% \n  dplyr::full_join(x = ., \n                   y = WolrdMap , \n                   by = \"region\") %&gt;% \n  mutate(hits = as.numeric(hits))\n\n\nInt_Merged %&gt;% \n  ggplot(aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group, \n                   fill = hits), \n               colour = \"grey40\",\n               size = 0.2) +\n  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(7, \"OrRd\"),\n                       na.value = \"white\") +\n  coord_cartesian(ylim = c(-55, 85)) + \n  theme(legend.position=\"bottom\") +\n  theme_bw() + \n  labs(x = \"\", y = \"\") + \n  theme(panel.grid.major = element_line(size = 0.5, linetype = 2),\n        panel.grid.minor = element_blank()) +\n  labs(x = \"\", y = \"\")\n\n\n\n\nMuch more countries are interested by this topic with India being the country with the highest number of Google searchs about data science."
  },
  {
    "objectID": "posts/2021/coastal-risks-google-trends.en.html#focusing-on-the-trend-of-storm-surge-in-france",
    "href": "posts/2021/coastal-risks-google-trends.en.html#focusing-on-the-trend-of-storm-surge-in-france",
    "title": "Coastal risks and statistical learning: Analyzing Google trends with gtrendsR package",
    "section": "Focusing on the trend of “storm surge” in France",
    "text": "Focusing on the trend of “storm surge” in France\nEarlier in this post, I made the hypothesis that the number of Google search for terms like “storm surge” was related to the recent occurrence of a storm event. Let’s investigate this hypothesis for the term “Submersion marine” (marine submersion/ flood in English) in France.\nFirst, we extract the number of hits on this term for France by indicating geo = \"FR\" in the gtrends function. Then we create a data frame where we gather some severe storms that impacted France (Website listing the severe storms in France). Finally, we join all the information on the same graph (red dashed lines represent storm events):\n\nres_fr &lt;- gtrends(\"submersion marine\",\n          geo = \"FR\",\n          time = \"all\")\n\ndf_storm &lt;- as.data.frame(matrix(c(\n  \"2009-01-24 GMT\", \"Klaus\",\n  \"2010-02-27 GMT\", \"Xynthia\",\n  \"2011-12-16 GMT\", \"Joachim\",\n  \"2014-02-14 GMT\", \"Ulla\",\n  \"2014-11-04 GMT\", \"Qendresa\",\n  \"2016-02-10 GMT\", \"No name\",\n  \"2017-03-06 GMT\", \"Zeus\",\n  \"2018-01-03 GMT\", \"Eleanor\",\n  \"2019-12-13 GMT\", \"No name\"\n  ), ncol = 2, byrow = T), stringsAsFactors = F)\n\n\nnames(df_storm) &lt;- c(\"date\", \"Name of storm\")\ndf_storm$date &lt;- as.POSIXct(df_storm$date, tz = \"GMT\")\n\n\nres_fr$interest_over_time %&gt;% \n  mutate(hits = as.numeric(ifelse(hits == \"&lt;1\", 0, hits))) %&gt;% \n  filter(date &gt; as.POSIXct(\"2006-01-24 GMT\", tz = \"GMT\")) %&gt;% \n  ggplot(aes(x = date, y = hits)) + \n  geom_line() +\n  geom_vline(xintercept = df_storm$date, col = \"red\", lty = 2, alpha = 0.5, lwd = 1) + \n  geom_label(aes(x=date, y = seq(38, 25, length.out=nrow(df_storm)), label =`Name of storm`), data = df_storm) +\n  #geom_vline(xintercept = as.POSIXct(\"2011-07-11 GMT\", tz = \"GMT\"), col = \"blue\", lty = 2, alpha = 0.5) + \n  facet_grid(~ keyword) +\n  theme_bw()\n\n\n\n\nFrom this figure, we can see that after a storm event there is often a surge in interest in the term “marine submersion” in France. This confirms my hypothesis !"
  },
  {
    "objectID": "posts/2021/coastal-risks-google-trends.en.html#conclusion",
    "href": "posts/2021/coastal-risks-google-trends.en.html#conclusion",
    "title": "Coastal risks and statistical learning: Analyzing Google trends with gtrendsR package",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis was really interesting for me as it confirmed what I knew with numbers and graphs. Without a doubt, this analysis will be a great asset for my introduction, especially in the part where I will discuss about the growing interest in data science. I hope this post gave you the envy to reproduce this analysis for the terms of your choice !"
  },
  {
    "objectID": "myprojects.html",
    "href": "myprojects.html",
    "title": "My projects",
    "section": "",
    "text": "Callens, A., Morichon, D., Liria, P., Epelde, I., & Liquet, B. (2021). Automatic Creation of Storm Impact Database Based on Video Monitoring and Convolutional Neural Networks. Remote Sensing, 13(10), 1933, (10.3390/rs13101933).\nCallens, A., Morichon, D., Abadie, S., Delpey, M., Liquet, B. (2020). Applied Ocean Research, 104, (10.1016/j.apor.2020.102339).\nCallens, A., Wang, Y., Fu, L. et al. (2020). Robust Estimation Procedure for Autoregressive Models with Heterogeneity. Environmental Modeling & Assessment, ( 10.1007/s10666-020-09730-w).\nMorichon, D., de Santiago, I., Delpey, M., Somdecoste, T., Callens, A., Liquet, B., … & Arnould, P. (2018). Assessment of flooding hazards at an engineered beach during extreme events: Biarritz, SW France. Journal of Coastal Research, 85(sp1), 801-805, ( 10.2112/SI85-161.1)."
  },
  {
    "objectID": "myprojects.html#academic-papers",
    "href": "myprojects.html#academic-papers",
    "title": "My projects",
    "section": "",
    "text": "Callens, A., Morichon, D., Liria, P., Epelde, I., & Liquet, B. (2021). Automatic Creation of Storm Impact Database Based on Video Monitoring and Convolutional Neural Networks. Remote Sensing, 13(10), 1933, (10.3390/rs13101933).\nCallens, A., Morichon, D., Abadie, S., Delpey, M., Liquet, B. (2020). Applied Ocean Research, 104, (10.1016/j.apor.2020.102339).\nCallens, A., Wang, Y., Fu, L. et al. (2020). Robust Estimation Procedure for Autoregressive Models with Heterogeneity. Environmental Modeling & Assessment, ( 10.1007/s10666-020-09730-w).\nMorichon, D., de Santiago, I., Delpey, M., Somdecoste, T., Callens, A., Liquet, B., … & Arnould, P. (2018). Assessment of flooding hazards at an engineered beach during extreme events: Biarritz, SW France. Journal of Coastal Research, 85(sp1), 801-805, ( 10.2112/SI85-161.1)."
  },
  {
    "objectID": "myprojects.html#my-thesis-report",
    "href": "myprojects.html#my-thesis-report",
    "title": "My projects",
    "section": "My thesis report",
    "text": "My thesis report\n\nPhD thesis:  Statistical learning for coastal risks assessment\nMaster 2 Thesis:  Robust regression for time series exhibiting heterogeneity\nMaster 1 Thesis:  Analysis of physical and climatic forcings impacting the transport of micropollutants in the Adour estuary (French version)."
  },
  {
    "objectID": "myprojects.html#shiny-applications",
    "href": "myprojects.html#shiny-applications",
    "title": "My projects",
    "section": "Shiny applications",
    "text": "Shiny applications\n\nMy first amazing (but laggy) Shiny application summarizing a part of my first internship (master 1).\nDummy example of the fishing application I use \nShiny application presenting the results of my fishing season 2021"
  },
  {
    "objectID": "myprojects.html#r-package",
    "href": "myprojects.html#r-package",
    "title": "My projects",
    "section": "R package",
    "text": "R package\n\nInvolved in the development of rlmDataDriven package (available on CRAN), especially:\n\nrlmDD_het.R : this function performs a robust regression which accounts for temporal correlations and heterogeneity.\nwhm.R : this function is the R implementation of the weighted M-estimation."
  },
  {
    "objectID": "posts/2021/ds-fishing-part3.en.html",
    "href": "posts/2021/ds-fishing-part3.en.html",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "",
    "text": "In this previous post, I presented the shiny application I developed to record data about my fishing session. In today’s post, I will present briefly the changes and updates I made to improve the application. Here are the main changes:"
  },
  {
    "objectID": "posts/2021/ds-fishing-part3.en.html#weather-api",
    "href": "posts/2021/ds-fishing-part3.en.html#weather-api",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "Weather API",
    "text": "Weather API\nSmall changes were made to adapt the former weather function to the new weather API. As this new API do not furnish moon phase data, I decided to compute the moon phase with the oce package:\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(oce)\n\nweather &lt;- function(lat, lon, API_key){\n  url &lt;- paste0(\"api.openweathermap.org/data/2.5/weather?lat=\", lat, \"&lon=\", lon, \"&appid=\", API_key, \"&units=metric\")\n  \n  rep &lt;- GET(url)\n  \n  table &lt;- fromJSON(content(rep, \"text\"))\n  \n  # The weather API don't provide moon phase so I compute it with Oce package\n  moon_phase &lt;- round(moonAngle(t = Sys.Date(),\n                                longitude = as.numeric(lon),\n                                latitude = as.numeric(lat))$illuminatedFraction,\n                      3)\n  \n  \n  current.weather.info &lt;- data.frame(Air_temp = table$main$temp,\n                                     Weather = table$weather$main,\n                                     Atm_pres = table$main$pressure,\n                                     Wind_str = table$wind$speed,\n                                     Wind_dir = table$wind$deg,\n                                     Cloud_cover = table$clouds$all,\n                                     PrecipInt = ifelse(is.null(table$rains$`1h`), 0, table$rains$`1h`),  \n                                     Moon = moon_phase)\n  return(current.weather.info)\n}"
  },
  {
    "objectID": "posts/2021/ds-fishing-part3.en.html#river-flow",
    "href": "posts/2021/ds-fishing-part3.en.html#river-flow",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "River flow",
    "text": "River flow\nI wrote functions to scrap information about the flows of the rivers in which I fish the most on a french website:\n\n# Get and prepare the flow data\nget_Qdata &lt;- function(link){\n  table &lt;- fromJSON(content(GET(link), \"text\"))\n  table &lt;- table$Serie$ObssHydro\n  table &lt;- as.data.frame(table)\n  table$DtObsHydro &lt;- sub(\"T\", \" \", table$DtObsHydro)\n  table$DtObsHydro &lt;- substr(table$DtObsHydro, start = 1, stop = 19)\n  ts &lt;- data.frame(Date = seq.POSIXt(as.POSIXct(range(table$DtObsHydro)[1],'%m/%d/%y %H:%M:%S'), \n                                     as.POSIXct(range(table$DtObsHydro)[2],'%m/%d/%y %H:%M:%S'), by=\"hour\"))\n  \n  table$DtObsHydro &lt;- as.POSIXct(table$DtObsHydro, format = \"%Y-%m-%d %H:%M:%S\")\n  \n  table &lt;- full_join(table, ts, by = c(\"DtObsHydro\" = \"Date\")) %&gt;% arrange(DtObsHydro)\n  return(table)\n}\n\n# Main function to collect river flow \n\nriver_flow &lt;- function(){\n  # Url of website to scrap:\n  url_index &lt;- \"https://www.vigicrues.gouv.fr/services/station.json/index.php\"\n  \n  rep &lt;- GET(url_index)\n  \n  table_index &lt;- fromJSON(content(rep, \"text\"))$Stations%&gt;% \n    na.omit()\n  \n  # I need to add the flow of several rivers to get the flow of the rivers I am interested in:\n  stations &lt;- table_index %&gt;% \n    filter(LbStationHydro %in% c(\"Pontonx-sur-l'Adour\", \"St-Pandelon\", \"Artiguelouve\", \"Escos\",\n                                 \"Aïcirits [St-Palais]\", \"Cambo-les-Bains\"))\n  \n  base_url &lt;- \"http://www.vigicrues.gouv.fr/services/observations.json?CdStationHydro=\"\n  height_url &lt;- \"&FormatDate=iso\"\n  Q_url &lt;- \"&GrdSerie=Q\"\n  \n  stations &lt;- stations %&gt;% \n    mutate(WL_link = paste0(base_url, CdStationHydro, height_url),\n           Q_link = paste0(WL_link, Q_url))\n  \n  data_Q &lt;- lapply(stations$Q_link, \n                   function(x){get_Qdata(x)})\n  \n  data_Q &lt;- suppressWarnings(Reduce(function(...) merge(..., all = TRUE, by = \"DtObsHydro\"),\n                   data_Q))\n  \n  names(data_Q) &lt;- c(\"Date\", stations$LbStationHydro) \n  \n  data_Q &lt;- data_Q  %&gt;% \n    mutate(hour_of_day = format(Date, \"%Y-%m-%d %H\"))\n  \n  \n  data_Q &lt;- aggregate(.~hour_of_day, data = data_Q, mean, na.rm = TRUE, na.action = na.pass)\n  \n  data_Q &lt;- imputeTS::na_interpolation(data_Q, option = \"linear\")\n  \n  final_data &lt;- data_Q %&gt;% \n    mutate(Adour = `Pontonx-sur-l'Adour` +  `Aïcirits [St-Palais]` + Artiguelouve + Escos + `St-Pandelon`,\n           Date = as.POSIXct(hour_of_day, tryFormats = \"%Y-%m-%d %H\")) %&gt;% \n    select(Date, `Cambo-les-Bains`, Adour) %&gt;% \n    rename(Nive = `Cambo-les-Bains`)\n  \n  Cur_flow &lt;- data.frame(\"Nive_c\" = final_data[nrow(final_data), 2],\n                         \"Adour_c\" = final_data[nrow(final_data), 3])\n  \n  \n  final_data &lt;- cbind(Cur_flow, final_data) %&gt;% \n    nest(Ts_flow = c(Date, Nive, Adour)) %&gt;% \n    mutate(Ts_flow = paste(Ts_flow))\n\n  return(final_data)\n}"
  },
  {
    "objectID": "posts/2021/ds-fishing-part3.en.html#shiny-application",
    "href": "posts/2021/ds-fishing-part3.en.html#shiny-application",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "Shiny application",
    "text": "Shiny application\nA simplified graph of the new application is showed below:\n\n\n\nSimplified workflow of the new version of application\n\n\n\nUI side\nThe UI side did not change that much, I only removed the tab that displayed fishing data on a map because I wasn’t using this feature too much:\n\n# Load libraries \nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(googlesheets)\nlibrary(miniUI)\nlibrary(leaflet)\nlibrary(rdrop2)\nSys.setenv(TZ=\"Europe/Paris\")\n\n#Import the functions for weather API and webscrapping \nsuppressMessages(source(\"api_functions.R\"))\n\n\n# Load the dropbox token : \ntoken &lt;&lt;- readRDS(\"token.rds\")\n\n# Minipage for small screens\nui &lt;- miniPage(tags$script('$(document).ready(function () {\n                           navigator.geolocation.getCurrentPosition(onSuccess, onError);\n\n                           function onError (err) {\n                           Shiny.onInputChange(\"geolocation\", false);\n                           }\n\n                           function onSuccess (position) {\n                           setTimeout(function () {\n                           var coords = position.coords;\n                           console.log(coords.latitude + \", \" + coords.longitude);\n                           Shiny.onInputChange(\"geolocation\", true);\n                           Shiny.onInputChange(\"lat\", coords.latitude);\n                           Shiny.onInputChange(\"long\", coords.longitude);\n                           }, 1100)\n                           }\n                           });'),\n               \n               gadgetTitleBar(\"Catch them all\", left = NULL, right = NULL),\n               \n               miniTabstripPanel(\n                 \n                 miniTabPanel(\"Session\", icon = icon(\"sliders\"),\n                              \n                              miniContentPanel(uiOutput(\"UI_sess\", align = \"center\"),\n                                               uiOutput(\"UI\", align = \"center\"))\n                              \n                 )\n               )\n               \n)\n\n\n\nServer side\nSeveral changes were made in the server side to collect data about the lures I used. Now, each time I change my fishing lure, I fill a small form to collect the lure characteristics and it adds a line in a third csv file:\n\nserver &lt;- function(input, output, session){\n  \n  observeEvent(input$go ,{\n    \n  # Read the csv file containing information about fishing session. If a session is running,\n  # display the UI that allows the user to input data about the fish caught. If a session is not started,\n  # display a button to start the session and small survey on lure characteristics.\n    \n    dat &lt;&lt;- drop_read_csv(\"/app_peche/session1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    # Reactive UI\n    \n    output$UI &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        # We now indicate what type of lure we use at the beginning of the session:\n        tagList(\n          selectInput(\"lure1\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure1\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure1\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"go\",\"Commencer session !\"))\n      }else{\n        \n        tagList(actionButton(\"go\",\"End session\"))\n      }\n      \n    })\n    \n    output$UI_sess &lt;- renderUI({\n      \n      if(!is.na(rev(dat$End)[1])){\n        \n        tagList(textInput(\"comments\", label = \"Commentaire avant le début?\", value = \"NA\"))\n        \n      }else{\n        input$catch\n        input$lure\n        tagList(\n          \n          selectInput(\"lure_type\", \n                      label = \"Type de leurre\",\n                      choices = list(\"Shad\" = \"shad\",\n                                     \"Slug\" = \"slug\",\n                                     \"Jerkbait\" = \"jerkbait\",\n                                     \"Casting jig\" = \"jig\",\n                                     \"Topwater\" = \"topwater\"),\n                      selected = \"shad\",\n                      selectize = FALSE),\n          \n          selectInput(\"color_lure\", \n                      label = \"Couleur du leurre\",\n                      choices = list(\"Naturel\" = \"naturel\",\n                                     \"Sombre\" = \"sombre\",\n                                     \"Clair\" = \"clair\",\n                                     \"Flashy\" = \"flashy\" ),\n                      selected = \"naturel\",\n                      selectize = FALSE),\n          \n          selectInput(\"length_lure\",\n                      label = \"Taille du leurre\",\n                      choices = list(\"Petit\" = \"petit\",\n                                     \"Moyen\" = \"moyen\",\n                                     \"Grand\" = \"grand\"),\n                      selected = \"petit\",\n                      selectize = FALSE),\n          \n          actionButton(\"lure\",\n                       label = \"Changer de leurre!\"),\n          \n          br(), \n          br(), \n          \n          h4(\"Ajouter une capture\"),\n          \n          selectInput(\"species\", \n                      label = \"Espèces\",\n                      choices = list(\"Bar\" = \"bar\",\n                                     \"Bar moucheté\" = \"bar_m\",\n                                     \"Alose\" = \"alose\",\n                                     \"Maquereau\" = \"maquereau\",\n                                     \"Chinchard\" = \"chinchard\"),\n                      selected = \"bar\"),\n          \n          sliderInput(\"length\",\n                      label = \"Taille du poisson\",\n                      value = 25, \n                      min = 0, \n                      max = 80, \n                      step = 1),\n          \n          actionButton(\"catch\",\"Rajoutez cette capture aux stats!\"),\n          \n          br(), \n          br(), \n          \n          textInput(\"comments1\", label = h4(\"Commentaire avant la fin ?\"), value = \"NA\")\n        )\n      }\n    })\n  }, ignoreNULL = F)\n  \n  \n  #If the button is pushed, create the line to be added in the csv file. \n  \n  observeEvent(input$go,{\n    \n    # Two outcomes depending if the session starts or ends. This gives the possibility \n    # to the user to add a comment before starting the session or after ending the session\n    \n    if(!is.na(rev(dat$End)[1])){\n      \n      #Tide + geoloc + Weather\n      c_tide &lt;- tide()\n      geoloc &lt;- c(input$lat,input$long)\n      current.weather.info &lt;- weather(lat = geoloc[1], lon = geoloc[2])\n      river.flow &lt;- river_flow()\n      \n      n_ses &lt;- c(rev(dat$Session)[1] + 1)\n      time_beg &lt;- as.character(as.POSIXct(Sys.time()))\n      comment &lt;- input$comments\n      dat.f &lt;&lt;- cbind(data.frame(n_ses,\n                                 time_beg,\n                                 NA,\n                                 geoloc[2],\n                                 geoloc[1]),\n                      current.weather.info,\n                      c_tide,\n                      river.flow,\n                      comment)\n      names(dat.f) &lt;- names(dat)\n      print(dat.f)\n      final_dat &lt;- rbind(dat, dat.f)\n      \n      lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                            header = T,\n                            stringsAsFactors = F,\n                            dtoken = token)\n      \n      new_lure &lt;- data.frame(n_ses = n_ses,\n                             time = as.character(as.POSIXct(Sys.time())),\n                             type_lure = input$lure1,\n                             color_lure = input$color_lure1,\n                             length_lure = input$length_lure1)\n      \n      new_df &lt;- rbind(lure, \n                      new_lure)\n      \n      write_csv(as.data.frame(new_df), \"lure.csv\")\n      drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n      \n\n    }else{\n      \n      dat$End[nrow(dat)] &lt;- as.character(as.POSIXct(Sys.time()))\n      dat$Comments[nrow(dat)] &lt;- paste(dat$Comments[nrow(dat)], \"/\", input$comments1)\n      final_dat &lt;- dat \n    }\n    \n    # Write csv in temporary files of shiny server \n    write_csv(as.data.frame(final_dat), \"session1.csv\")\n    \n    # Upload it to dropbox account \n    drop_upload(\"session1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  # Add a line to the catch csv file whenever a fish is caught\n  observeEvent(input$catch,{\n    caugth &lt;- drop_read_csv(\"/app_peche/catch1.csv\", header = T, stringsAsFactors = F, dtoken = token)\n    \n    catch &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        species = input$species,\n                        length = input$length)\n    \n    b &lt;- rbind(caugth,catch)\n    \n    write_csv(as.data.frame(b), \"catch1.csv\")\n    drop_upload(\"catch1.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n  \n  \n  observeEvent(input$lure,{\n    lure &lt;- drop_read_csv(\"/app_peche/lure.csv\",\n                          header = T,\n                          stringsAsFactors = F,\n                          dtoken = token)\n    \n    new_lure &lt;- data.frame(n_ses = dat$Session[nrow(dat)],\n                        time = as.character(as.POSIXct(Sys.time())),\n                        type_lure = input$lure_type,\n                        color_lure = input$color_lure,\n                        length_lure = input$length_lure)\n    \n    new_df &lt;- rbind(lure, \n               new_lure)\n    \n    write_csv(as.data.frame(new_df), \"lure.csv\")\n    drop_upload(\"lure.csv\", path = \"App_peche\", mode = \"overwrite\", dtoken = token)\n  })\n}"
  },
  {
    "objectID": "posts/2021/ds-fishing-part3.en.html#conclusion",
    "href": "posts/2021/ds-fishing-part3.en.html#conclusion",
    "title": "Can R and Shiny make me a better fisherman? Part 3",
    "section": "Conclusion",
    "text": "Conclusion\nI have tested this new application during two fishing sessions and it has been working like a charm. I can’t wait to present you my findings at the end of this fishing season !"
  },
  {
    "objectID": "posts/2020/ds-fishing-part2.en.html",
    "href": "posts/2020/ds-fishing-part2.en.html",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "",
    "text": "In the previous blog article, I described in details how I built a shiny application that stores data about my fishing sessions. In this post, I will explore the data I have collected during the last year.\nTo sum up, my application store the data in two csv files. The first one contains variables related to the fishing conditions at the beginning and at the end of the session such as :\nThe second one contains information about my catches :"
  },
  {
    "objectID": "posts/2020/ds-fishing-part2.en.html#importing-and-cleaning-my-fishing-data",
    "href": "posts/2020/ds-fishing-part2.en.html#importing-and-cleaning-my-fishing-data",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "Importing and cleaning my fishing data",
    "text": "Importing and cleaning my fishing data\nThe first step of this analysis is to import both csv files and make some transformations.\n\n\nCode\n# Change character variables to factor\nsession_data %&lt;&gt;% \nmutate_at(vars(Weather, Tide_status), as.factor)\n\n# Change character variables to factor\ncatch_data %&lt;&gt;% \nmutate_at(vars(species, lure, colour, length_lure), as.factor)\n\n\n \nAfter cleaning and rearranging the data (code hidden below), we are ready to explore graphically our data!\n\n\nCode\n# Compute mean conditions (between beg and end session) \n\nmean_weather_cond &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(-c(Long, Lat, Water_level, Tide_time)) %&gt;% \nsummarise_if(is.numeric, mean) \n\n\n# Extract fixed conditions and comments + join with mean cond \n\nfixed_cond_com &lt;- session_data %&gt;% \ngroup_by(Session) %&gt;% \nselect(Session, Comments, Long, Lat, Weather) %&gt;% \nmutate(Comments_parsed = paste(na.omit(Comments), collapse = \"\")) %&gt;% \nselect(-Comments) %&gt;% \nslice(1) %&gt;% \ninner_join(mean_weather_cond, by = \"Session\")\n\n# Create end and beg variables for WL, Time , Tide_time, Tide_status\n\nbeg_end_vars &lt;- session_data %&gt;% \nselect(Session, Status, Water_level, Time, Tide_time, Tide_status) %&gt;% \npivot_wider(names_from = Status,\nvalues_from = c(Time, Water_level,  Tide_time, Tide_status))\n\n\n# Assemble both file and calculate duration\n\ndat_ses &lt;-  inner_join(beg_end_vars,\nfixed_cond_com,\nby = \"Session\")\n\n# Calculate duration of the sessions\n\ndat_ses %&lt;&gt;% \nmutate(duration = round(difftime(Time_end,  Time_beg,  units = \"hours\"),\ndigits = 1))\n\ncatch_cond &lt;- full_join(dat_ses,\ncatch_data, by = c( \"Session\" = \"n_ses\" )) %&gt;% \nmutate(Session = factor(Session, levels = 1:length(dat_ses$Session)))\n\ncatch_cond %&lt;&gt;%\nmutate(Tide_status_ses = paste0(Tide_status_beg, \"_\", Tide_status_end))\n\n# Simplify the Tide status variable\n\ncatch_cond$Tide_status_ses &lt;- sapply(catch_cond$Tide_status_ses , function(x){switch(x, \n\"Up_Dead\" = \"Up\",\n\"Up_Up\" = \"Up\",\n\"Up_Down\" = \"Dead\",\n\"Down_Dead\" = \"Down\",\n\"Down_Up\" = \"Dead\",\n\"Down_Down\"  = \"Down\",\n\"Dead_Dead\" = \"Dead\",\n\"Dead_Up\" = \"Up\",\n\"Dead_Down\" = \"Down\"\n)}, USE.NAMES = F)"
  },
  {
    "objectID": "posts/2020/ds-fishing-part2.en.html#graphical-exploration",
    "href": "posts/2020/ds-fishing-part2.en.html#graphical-exploration",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "Graphical exploration",
    "text": "Graphical exploration\n\nWhere did I fish ?\nWe can visualize the locations I fished the most by using the leaflet package:\n\n\nCode\n# Calculate the number of fish caught by session \nfish_number &lt;-  catch_cond  %&gt;% na.omit() %&gt;% group_by(Session) %&gt;%  summarise(nb = length(Session))\n\n# Dataframe with variables we want to show on the map\nmap_data &lt;- catch_cond %&gt;% \ngroup_by(Session) %&gt;%\nselect(Session, Time_beg, Time_end, Long,\nLat, Water_level_beg, Tide_status_beg, Tide_time_beg, duration) \n\nmap_data &lt;- full_join(map_data, fish_number)\n\nmap_data$nb[is.na(map_data$nb)] &lt;- 0\n\n# Interactive map with Popup for each session\nlibrary(leaflet)\n\nleaflet(map_data, width = \"100%\") %&gt;% addTiles() %&gt;%\naddPopups(lng = ~Long, lat = ~Lat, \nwith(map_data, sprintf(\"&lt;b&gt;Session %.0f : %.1f h&lt;/b&gt; &lt;br/&gt; %s &lt;br/&gt; %.0f  fish &lt;br/&gt; Water level: %.0f m, %s, %.0f min since last peak\",                                         Session, duration,  Time_beg, nb, Water_level_beg, Tide_status_beg, Tide_time_beg)), \noptions = popupOptions(maxWidth = 100, minWidth = 50))\n\n\n\n\n\n\nAs you see I fish mostly in the Nive river that is flowing through Bayonne city.\n\n\nWhen it is best to fish ?\n\nTime of the year\nThe following graph shows the number of fish caught depending on the time of the year :\n\n\nCode\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\nggplot(aes(y = n_catch, x =Time_beg)) +\ngeom_point( size = 2) + \n  theme_minimal() + labs(x = \"Date\", y = \"Number of catch\") + scale_x_datetime(date_labels = \"%d/%m/%y\", date_breaks = \"3 months\") \n\n\n\n\n\nFrom this graph we see that I didn’t go fishing during the autumn and winter of 2019, I don’t have any data. Unfortunately for me, autumn is known to be a great period for sea bass fishing, I must go fishing this year to compensate the lack of data in this season. During winter, fishing is really complicated because the large majority of sea bass are returned to the ocean.\n\n\nTime of the day\nThis graph shows the number of fish I catch depending on the hour of the day :\n\n\nCode\ncatch_cond %&gt;% \ngroup_by(Session, Time_beg, .drop = F) %&gt;% \nna.omit() %&gt;% \nsummarise(n_catch = n()) %&gt;% \nright_join(unique(catch_cond[, c(\"Session\", \"Time_beg\")])) %&gt;% \nmutate(n_catch = ifelse(is.na(n_catch), 0, n_catch ), \nhour = format(Time_beg, \"%H\")) %&gt;%\nggplot(aes(y = n_catch, x =hour)) +\ngeom_point( size = 2)  + labs(x = \"Hour\", y = \"Number of catch\")+\ntheme_minimal()\n\n\n\n\n\nI mostly fish after work or during evenings. To draw relevant conclusions about the influence of the fishing hour, I have to go fishing at different hours of the day (in the morning for example).\n\n\nThe tide\nThe tide is an important parameter for fishing in estuaries. Let’s see the effect of the tide current on my catches:\n\n\nCode\nlibrary(ggpubr)\n\ngg1 &lt;- catch_cond %&gt;% \n  group_by(Session, Tide_status_ses, .drop = F)  %&gt;%  \n  drop_na() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Tide_status_ses\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;%\n  ggplot(aes(y = n_catch, x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot() +\n  labs(x = \"Status of tide current\", y = \"Number of catch\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg2 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length,x = Tide_status_ses, fill = Tide_status_ses)) +\n  geom_boxplot()+\n  labs(x = \"Status of tide current\", y = \"Length of the fish\") +\n  theme_minimal()+ theme(legend.position=\"None\")\n\nggarrange(gg1, gg2)\n\n\n\n\n\nIt seems that the status of the tide current does not influence the number of my catch but influences the length of the fish. I tend to catch bigger fish when the current is going down.\n\n\n\nDoes the moon affect my fishing results?\nA widespread belief among fishermen is that the moon influences greatly the behavior of the fish. Data about the moon phase were available thanks to the weather API, I decided to record this variable to investigate if the belief was true. The two graphs show the number and length of fish depending on the phase of moon (0 corresponding to new moon and 1 to full moon):\n\n\nCode\ngg3 &lt;- catch_cond %&gt;% \n  group_by(Session, Moon, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Moon\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Moon)) +\n  geom_point( size = 2) +\n  labs(x = \"Moon phase\", y = \"Number of catch\")+\n  theme_minimal()\n\ngg4 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Moon)) +\n  geom_point( size = 2) +\n  geom_smooth(method=\"lm\", se=T) + \n  labs(x = \"Moon phase\", y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg3, gg4)\n\n\n\n\n\nThe phase of the moon does not seem to influence the number of fish I catch during a session. However, I tend to catch bigger fish the closer we are to the full moon. To confirm this observation, I need to keep going fishing to get more data !\n\n\nDoes the weather affect my fishing results?\nWe can look at the number of fish caught during different weather conditions:\n\n\nCode\n# precipitation probability \n\ngg5 &lt;- catch_cond %&gt;% \n  group_by(Session, Preci_prob, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Preci_prob\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Preci_prob)) +\n  geom_point()+\n  labs(x = \"Precipitation prob.\", y = \"Number of catch\")+\n  theme_minimal()\n\n# Atm pressure \n\ngg6 &lt;- catch_cond %&gt;% \n  group_by(Session, Atm_pres, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Atm_pres\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Atm_pres)) +\n  geom_point() +\n  labs(x = \"Atm. pressure\", y = \"Number of catch\")+\n  theme_minimal()\n\n#Air temp\n\ngg7 &lt;- catch_cond %&gt;% \n  group_by(Session, Air_temp, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Air_temp\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Air_temp)) +\n  geom_point() +\n  labs(x = \"Air temp.\", y = \"Number of catch\")+\n  theme_minimal()\n\n\n#Cloud cover\n\ngg8 &lt;- catch_cond %&gt;% \n  group_by(Session, Cloud_cover, .drop = F) %&gt;%  \n  na.omit() %&gt;% \n  summarise(n_catch = n()) %&gt;% \n  right_join(unique(catch_cond[, c(\"Session\", \"Cloud_cover\")])) %&gt;% \n  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&gt;% \n  ggplot(aes(y = n_catch, x = Cloud_cover)) +\n  geom_point() +\n  labs(x = \"Cloud cover\", y = \"Number of catchh\")+\n  theme_minimal()\n\nggarrange(gg5, gg6, gg7, gg8)\n\n\n\n\n\nAnd to their length :\n\n\nCode\ngg15 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Preci_prob)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg16 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Atm_pres)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\ngg17 &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Air_temp)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\n\ngg18  &lt;- catch_cond %&gt;% \n  ggplot(aes(y = length, x = Cloud_cover)) +\n  geom_point( size = 2) +\n  labs( y = \"Length of the fish\")+\n  theme_minimal()\n\nggarrange(gg15, gg16, gg17, gg18)\n\n\n\n\n\nBecause we have limited data and not all weather conditions are covered, it is difficult to draw any conclusions.\n\n\nWhat are the best lures to catch fish ?\nEach time I catch a fish, I fill a form with my shiny application in order to record the characteristics of the lure used. There are different types of lures that have specific swimming patterns, different colors and size. We can represent the number of fish caught depending on the lure characteristics:\n\n\nCode\nlevels(catch_cond$colour) &lt;- c(\"clear\", \"natural\", \"dark\")\nlevels(catch_cond$length_lure) &lt;- c(\"large\", \"medium\", \"small\")\n\ngg9 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=lure, fill = lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ \n  theme(legend.position=\"None\")\n\ngg10 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot( aes(x=colour, fill = colour)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ \n  theme(legend.position=\"None\")\n\ngg11 &lt;- catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot( aes(x=length_lure, fill = length_lure)) +\n  geom_bar(stat=\"count\", width=0.7)+\n  labs(x = \"Size of the lure\", y = \"\")+\n    scale_fill_brewer(palette=\"Dark2\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg9, gg10, gg11, ncol = 3),\n                left = text_grob(\"Number of catch\", rot = 90)\n)\n\n\n\n\n\nWe can do the same for the length of fish caught:\n\n\nCode\ngg12 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;% \n  ggplot(aes(y = length, x = lure, fill=lure)) +\n  geom_boxplot()+\n  labs(x = \"Type of lure\", y = \"\")+\n  theme_minimal()+ theme(legend.position=\"None\")\n\ngg13 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = colour, fill= colour)) +\n  geom_boxplot()+\n  labs(x = \"Color of the lures\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"BuPu\")+ theme(legend.position=\"None\")\n\ngg14 &lt;-catch_cond %&gt;% \n  na.omit() %&gt;%\n  ggplot(aes(y = length, x = length_lure, fill=length_lure)) +\n  geom_boxplot()+\n  labs(x = \"Size of the lure\", y = \"\")+\n  theme_minimal()+\n    scale_fill_brewer(palette=\"Dark2\")+ theme(legend.position=\"None\")\n\nannotate_figure(ggarrange(gg12, gg13, gg14, ncol = 3),\n                left = text_grob(\"Length of fish\", rot = 90)\n)\n\n\n\n\n\nWith these 6 graphs, we can see that the most successful types of lures for me are the shad and slug types. An honorable mention is the jerkbait type: it only accounts for 2 fish, but 2 big fish (median around 47cm). The colors that worked best for me were clear and natural. For the size of the lure, bigger lures tend to catch bigger fish in average. These conclusions must be taken with a grain of salt because we do not know the time spent with each lure before catching a fish. In addition, I tend to use the same types and colors of lures (habits), I should vary more."
  },
  {
    "objectID": "posts/2020/ds-fishing-part2.en.html#conclusion",
    "href": "posts/2020/ds-fishing-part2.en.html#conclusion",
    "title": "Can R and Shiny make me a better fisherman? Part 2",
    "section": "Conclusion",
    "text": "Conclusion\nAnalyzing my fishing data was very interesting and it brought me some insights on my fishing style! I understood that I was fishing almost the same way with the same habits. Although it seems to be working for me, I have a biased view on how to catch European sea bass. I must use bigger lures to catch bigger fish and I must vary the types of lures used. Indeed, I fish most of the times with slug or shad lures, hence the higher number of fish caught with these types of lures.\nI will keep using the application to gather more data and have a better understanding on my fishing session. I will keep you updated on the results ! :wink:"
  },
  {
    "objectID": "posts/2020/aliexpress-rselenium.html",
    "href": "posts/2020/aliexpress-rselenium.html",
    "title": "Webscraping Aliexpress with Rselenium",
    "section": "",
    "text": "Today, I am going to show you how to scrape product prices from Aliexpress website."
  },
  {
    "objectID": "posts/2020/aliexpress-rselenium.html#a-few-words-on-web-scraping",
    "href": "posts/2020/aliexpress-rselenium.html#a-few-words-on-web-scraping",
    "title": "Webscraping Aliexpress with Rselenium",
    "section": "A few words on web scraping",
    "text": "A few words on web scraping\nBefore diving into the subject, you should be aware that web scraping is not allowed on certain websites. To know if it is the case for the website you want to scrape, I invit you to check the robots.txt page which should be located at the root of the website address. For Aliexpress this page is located here : www.aliexpress.com/robots.txt .\nThis page indicates that webscrapping and crawling are not allowed on several page categories such as /bin/*, /search/*, /wholesale* for example. Fortunately for us, the /item/* category, where the product pages are stored, can be scraped."
  },
  {
    "objectID": "posts/2020/aliexpress-rselenium.html#rselenium",
    "href": "posts/2020/aliexpress-rselenium.html#rselenium",
    "title": "Webscraping Aliexpress with Rselenium",
    "section": "RSelenium",
    "text": "RSelenium\n\nInstallation for Ubuntu 18.04 LTS\nThe installation for RSelenium was not as easy as expected and I encountered two errors.\nThe first error I got after I installed the package and tried the function Rsdriver was :\nError in curl::curl_fetch_disk(url, x$path, handle = handle) :\nUnrecognized content encoding type. libcurl understands deflate, gzip content encodings.\nThanks to this post, I installed the missing package : stringi.\nOnce this error was addressed, I had a different one :\nError: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar\nThis time the problem came from a corrupted file. Thanks to this post, I knew that I just had to download this file selenium-server-standalone-4.0.0-alpha-2.jar from the official selenium website and replace the corrupted file with it.\nI hope this will help some of you to install RSelenium with Ubuntu 18.04 LTS !\n\n\nOpening a web browser\nAfter addressing the errors above, I can now open a firefox browser :\n\nlibrary(RSelenium)\n\n#Open a firefox driver\nrD &lt;- rsDriver(browser = \"firefox\") \nremDr &lt;- rD[[\"client\"]]\n\n\n\nLogging in Aliexpress\nThe first step to scrape product prices on Aliexpress is to log in into your account:\n\nlog_id &lt;- \"Your_mail_adress\"\npassword &lt;- \"Your_password\"\n\n# Navigate to aliexpress login page \nremDr$navigate(\"https://login.aliexpress.com/\")\n\n# Fill the form with mail address\nremDr$findElement(using = \"id\", \"fm-login-id\")$sendKeysToElement(list(log_id))\n\n# Fill the form with password\nremDr$findElement(using = 'id', \"fm-login-password\")$sendKeysToElement(list(password))\n\n#Submit the login form by clicking Submit button\nremDr$findElement(\"class\", \"fm-button\")$clickElement()\n\n\n\nNavigating through the URLs and scraping the prices\nNow we have to navigate through a vector containing the URL of the aliexpress products we are interested in. Then we extract the price of the product by using the xpath of the product price of the webpage. The xpath of the element you want to scrape can be found by using the developer tool of chrome or firefox ( tutorial here ! ). Once the price is extracted we have to ensure this price is in numerical format by removing any special character (euro or dollar sign) and replace the comma by a point for the decimal separator. Here is the R code:\n\n  url_list &lt;- list(\"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m\") \n\n# Allocate a vector to store the price of the products \ncurrentp &lt;- c()\nfor(i in 1:length(url_list)){\n  \n  # Navigate to link [i]\n  remDr$navigate(url_list[i])\n  \n  # Find the price with an xpath selector and findElement.  \n  # Sometimes products can be removed and this could throw an error this is why we are using 'try' to handle the potential errors\n  \n  current &lt;- try(remDr$findElement(using = \"xpath\",'//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"product-price-value\", \" \" ))]'), silent = T)\n  \n  #If error : current price is NA \n  if(class(current) =='try-error'){\n    currentp[i] &lt;- NA\n  }else{\n    # Get the price \n    text &lt;- unlist(current$getElementText())\n    \n    #Remove euro sign\n    text &lt;- gsub(\"[^A-Za-z0-9,;._-]\",\"\",text)\n    \n    #Case when there is a range of price instead of one price + replace comma by point\n    if(grepl(\"-\", text)) {  \n      pe &lt;- sub(\"-.*\",\"\",text) %&gt;% sub(\",\", \".\", ., fixed = TRUE)\n      currentp[i] &lt;-  as.numeric(pe)\n    }else{\n      currentp[i] &lt;- as.numeric(sub(\",\", \".\", text, fixed = TRUE))\n  }\n  }\n  \nSys.sleep(4)\n}\n\nBetween each link it is advised to wait a few seconds with Sys.sleep(4) to avoid being black-listed by the website.\n\n\nPhantomjs version\nIf you execute the code above, you should see a firefox browser open and navigate through the list you provided. In case you don’t want an active window, you can replace firefox by phantomjs browser which is a headless browser (without a window).\nI don’t know why but using rsDriver(browser = \"phantomjs\") does not work for me. I found this post which propose to start the phantomjs browser with the wdman package:\n\nlibrary(wdman)\nlibrary(RSelenium)\n# start phantomjs instance\nrPJS &lt;- wdman::phantomjs(port = 4680L)\n\n# is it alive?\nrPJS$process$is_alive()\n\n#connect selenium to it?\nremDr &lt;-  RSelenium::remoteDriver(browserName=\"phantomjs\", port=4680L)\n\n# open a browser\nremDr$open()\n\nremDr$navigate(\"http://www.google.com/\")\n\n# Screenshot of the headless browser to check if everything is working\nremDr$screenshot(display = TRUE)\n\n# Don't forget to close the browser when you are finished ! \nremDr$close()\n\n\n\nConclusion\nOnce you have understand the basics of RSelenium and how to select elements inside HTML pages, it is really easy to write a script to scrape data on the web. This post was a short example to scrape the product price on Aliexpress pages but the script can be extended to scrape more data on each page such as the name of the item, its rating etc… It is even possible to automate this script to run daily in order to see price changes over time. As you see possibilities are endless!"
  },
  {
    "objectID": "posts/2022/analysis-of-the-top-r-packages.en.html",
    "href": "posts/2022/analysis-of-the-top-r-packages.en.html",
    "title": "Analysis of the top R packages",
    "section": "",
    "text": "After a little while coding in Python every day for my work, I needed to make a break and perform some R analysis! Since the beginning of my postdoc, I haven’t followed the last trends concerning R packages. In this post, I am going to analyze some data about R packages to see what are the most downloaded packages during the past weeks. I will also visualize all the relationships between the R packages by looking at their required dependencies.\nLet’s import the packages required for this analysis:\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(cranlogs)\nlibrary(igraph)\nlibrary(visNetwork)\n\n\nHow to find the most popular R packages ?\nThe first thing is to gather the data about the number of downloads for each package. Luckily for us, there is a package called cranlogs that does just what we need ! With a simple line of command we can collect data about the 50 packages with the most downloads in the last month, we can then plot the result:\n\npopular_pckg &lt;- cran_top_downloads(\"last-month\", 50)\n\npopular_pckg %&gt;%\n  mutate(package = fct_reorder(package, desc(count))) %&gt;% \n  ggplot(aes(x = package, y = count)) +\n  geom_bar(stat=\"identity\") + \n  scale_x_discrete(expand = expansion(mult = c(0, 0.02))) +\n  theme_bw() +\n  xlab(\"\") +\n  ylab(\"Downloads\")+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n        axis.title = element_text(size = 12),\n        axis.text = element_text(size = 9, face = \"bold\"),\n        plot.title = element_text(size = 14),\n        legend.position = \"none\") +labs(x = NULL)\n\n\n\n\nI thought this graph will be harder to make because of the availability of the data but with the right package everything can be done !\n\n\nVisualisation of the dependencies between packages\nOnce I saw the above graph, I was wondering about all the dependencies between these packages and I wanted to know which one was the most “connected”. To answer this question, I need more data, especially about the required dependencies of each package. After some research, I found out that data about package (including description and dependencies) can be extracted with a function in the tools package:\n\ndf_pkg &lt;- tools::CRAN_package_db()[, c('Package', 'Imports')]\ndf_pkg &lt;- df_pkg %&gt;% filter(if_any(\"Package\", ~.x %in% popular_pckg[[\"package\"]]))\n\nHowever, this function extract the data for all the packages and I want to perform the analysis only on the top 50 popular packages. So I decided to couple the function of the cranlog package with the database I collected with the CRAN_package_db() function:\n\n# Can be quite long hence the parallel map\n#plan(multisession, workers = 12)\nmonthly_dl &lt;- map(list(df_pkg$Package), function(x){sum(cran_downloads(x, 'last-month')$count)})\ndf_pkg$monthly_dl &lt;- unlist(monthly_dl)\n# write_csv(df_pkg, 'R_pkg_dl.csv')\n\nWe can then filter by number of downloads:\n\ndf_pkg &lt;- df_pkg %&gt;% \n  distinct(Package, .keep_all= TRUE) %&gt;% \n  arrange(\"monthly_dl\") \n\nNow, it is time to prepare the data for a graph visualization. To make a graph, we need two tables. The first one must contain all the relationships between nodes (in our case nodes are packages), it has two columns : ‘from’ and ‘to’. The second table contains only one column with the names of the nodes.\n\nimport_cleaning &lt;- function(text){\n  text &lt;- gsub('\\\\s*\\\\([^\\\\)]+\\\\)', '', text)\n  text &lt;- gsub('\\\\n', ' ', text)\n  text &lt;- gsub(' ', '', text, fixed = TRUE)\n  text &lt;- str_split(text, ',')\n  return(text)\n}\n\nimport_cleaning(df_pkg$Imports[2])\n\ntest &lt;- df_pkg %&gt;% \n  mutate(cleaned_imports = import_cleaning(Imports))\n\ndf_target &lt;- function(x,y){\n  df &lt;- expand.grid(from=x, to=unlist(y))\n  return(df)}\n\nfor(i in 1:nrow(test)){\n  if(i == 1){\n    df_res = df_target(test$Package[i], test$cleaned_imports[i])\n  }else{\n      df_res = rbind(df_res, df_target(test$Package[i], test$cleaned_imports[i]))\n  }\n}\n\nlinks &lt;- df_res %&gt;% \n  filter(!is.na(to) | (to == \"\"))\n\nnodes &lt;- tibble(id=as.character(unique(unlist(df_res))))\n\nOnce the two matrices are made, we can interactively visualize the graph network with visNetwork package:\n\nvisNetwork(nodes, links) %&gt;%\n    visIgraphLayout(type = \"full\") %&gt;%\n    visNodes(\n        shape = \"dot\",\n        color = list(\n            background = \"#0085AF\",\n            border = \"#013848\",\n            highlight = \"#FF8000\"\n        ),\n        scaling = list(min=2,\n                       max = 10),\n        shadow = list(enabled = TRUE, size = 10)\n    ) %&gt;%\n    visEdges(\n      arrows='to',\n        shadow = FALSE,\n        color = list(color = \"#0085AF\", highlight = \"#C62F4B\")\n    ) %&gt;%\n    visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T)) %&gt;% \n    visLayout(randomSeed = 11)\n\n\n\n\n\n\nDo not hesitate to move, zoom in or select packages to see their dependencies !\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My blog posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nOptimizing my search for Data scientist jobs by scraping Indeed with R\n\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\nEDA\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of the top R packages\n\n\n\n\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nAurelien Callens\n\n\n\n\n\n\n  \n\n\n\n\nCan R and Shiny make me a better fisherman? Part 4\n\n\nExploratory analysis on my fishing data (shiny and plotly)\n\n\n\n\nR\n\n\nShiny\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\nAurélien Callens\n\n\n\n\n\n\n  \n\n\n\n\nThe end of my PhD journey\n\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2021\n\n\nAurelien Callens\n\n\n\n\n\n\n  \n\n\n\n\nCan R and Shiny make me a better fisherman? Part 3\n\n\n\n\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\nAurélien Callens\n\n\n\n\n\n\n  \n\n\n\n\nCoastal risks and statistical learning: Analyzing Google trends with gtrendsR package\n\n\n\n\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\nAurélien Callens\n\n\n\n\n\n\n  \n\n\n\n\nWebscraping Aliexpress with Rselenium\n\n\n\n\n\n\n\nR\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2020\n\n\nAurelien Callens\n\n\n\n\n\n\n  \n\n\n\n\nCan R and Shiny make me a better fisherman? Part 2\n\n\nExploratory analysis on my fishing data\n\n\n\n\nR\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\nAurélien Callens\n\n\n\n\n\n\n  \n\n\n\n\nCan R and Shiny make me a better fisherman? Part 1\n\n\nBuilding a shiny application to store my fishing data\n\n\n\n\nR\n\n\nShiny\n\n\nWeb scraping\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2020\n\n\nAurélien Callens\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]