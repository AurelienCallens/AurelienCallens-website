<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSelenium on Aurélien Callens</title>
    <link>https://aureliencallens.github.io/tags/rselenium/</link>
    <description>Recent content in RSelenium on Aurélien Callens</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>aurelien.callens@gmail.com (Aurelien Callens)</managingEditor>
    <webMaster>aurelien.callens@gmail.com (Aurelien Callens)</webMaster>
    <lastBuildDate>Wed, 18 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://aureliencallens.github.io/tags/rselenium/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Webscraping Aliexpress with Rselenium</title>
      <link>https://aureliencallens.github.io/2020/11/18/2020-11-18-aliexpress_rselenium/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2020/11/18/2020-11-18-aliexpress_rselenium/</guid>
      <description>


&lt;p&gt;Today, I am going to show you how to scrape product prices from Aliexpress website.&lt;/p&gt;
&lt;div id=&#34;a-few-words-on-web-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A few words on web scraping&lt;/h2&gt;
&lt;p&gt;Before diving into the subject, you should be aware that web scraping is not allowed on certain websites. To know if it is the case for the website you want to scrape, I invit you to check the &lt;em&gt;robots.txt&lt;/em&gt; page which should be located at the root of the website adress. For Aliexpress this page is located here : &lt;a href=&#34;https://www.aliexpress.com/robots.txt&#34; target=&#34;_blank&#34;&gt;www.aliexpress.com/robots.txt .&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This page indicates that webscrapping and crawling are not allowed on several page categories such as &lt;code&gt;/bin/*&lt;/code&gt;, &lt;code&gt;/search/*&lt;/code&gt;, &lt;code&gt;/wholesale*&lt;/code&gt; for example. Fortunately for us, the &lt;code&gt;/item/*&lt;/code&gt; category, where the product pages are stored, can be scraped.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rselenium&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RSelenium&lt;/h2&gt;
&lt;div id=&#34;installation-for-ubuntu-18.04-lts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installation for Ubuntu 18.04 LTS&lt;/h3&gt;
&lt;p&gt;The installation for RSelenium was not as easy as expected and I encountered two errors.&lt;/p&gt;
&lt;p&gt;The first error I got after I installed the package and tried the function &lt;em&gt;Rsdriver&lt;/em&gt; was :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error in curl::curl_fetch_disk(url, x$path, handle = handle) :
Unrecognized content encoding type. libcurl understands deflate, gzip content encodings.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/ropensci/RSelenium/issues/186&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;, I installed the missing package : &lt;em&gt;stringi&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Once this error was addressed, I had a different one :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time the problem came from a corrupted file. Thanks to &lt;a href=&#34;https://stackoverflow.com/questions/20680229/invalid-or-corrupt-jarfile-usr-local-bin-selenium-server-standalone-2-38-0-jar&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;, I knew that I just had to download this file &lt;em&gt;selenium-server-standalone-4.0.0-alpha-2.jar&lt;/em&gt; from the official &lt;a href=&#34;https://selenium-release.storage.googleapis.com/index.html?path=4.0/&#34; target=&#34;_blank&#34;&gt;selenium website&lt;/a&gt; and replace the corrupted file with it.&lt;/p&gt;
&lt;p&gt;I hope this will help some of you to install RSelenium with Ubuntu 18.04 LTS !&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;opening-a-web-browser&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Opening a web browser&lt;/h3&gt;
&lt;p&gt;After addressing the errors above, I can now open a firefox browser :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RSelenium)

#Open a firefox driver
rD &amp;lt;- rsDriver(browser = &amp;quot;firefox&amp;quot;) 
remDr &amp;lt;- rD[[&amp;quot;client&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-in-aliexpress&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logging in Aliexpress&lt;/h3&gt;
&lt;p&gt;The first step to scrape product prices on aliexpress is to log in into your account:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_id &amp;lt;- &amp;quot;Your_mail_adress&amp;quot;
password &amp;lt;- &amp;quot;Your_password&amp;quot;

# Navigate to aliexpress login page 
remDr$navigate(&amp;quot;https://login.aliexpress.com/&amp;quot;)

# Fill the form with mail address
remDr$findElement(using = &amp;quot;id&amp;quot;, &amp;quot;fm-login-id&amp;quot;)$sendKeysToElement(list(log_id))

# Fill the form with password
remDr$findElement(using = &amp;#39;id&amp;#39;, &amp;quot;fm-login-password&amp;quot;)$sendKeysToElement(list(password))

#Submit the login form by clicking Submit button
remDr$findElement(&amp;quot;class&amp;quot;, &amp;quot;fm-button&amp;quot;)$clickElement()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;navigating-through-the-urls-and-scraping-the-prices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Navigating through the URLs and scraping the prices&lt;/h3&gt;
&lt;p&gt;Now we have to navigate through a vector containing the URL of the aliexpress products we are interested in. Then we extract the price of the product by using the xpath of the product price of the webpage. The xpath of the element you want to scrape can be found by using the developper tool of chrome or firefox ( &lt;a href=&#34;https://www.scrapingbee.com/blog/practical-xpath-for-web-scraping/&#34;&gt;tutorial here !&lt;/a&gt; ). Once the price is extracted we have to ensure this price is in numerical format by removing any special character (euro or dollar sign) and replace the comma by a point for the decimal separator. Here is the R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  url_list &amp;lt;- list(&amp;quot;https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ&amp;quot;,
            &amp;quot;https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ&amp;quot;,
            &amp;quot;https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m&amp;quot;) 

# Allocate a vector to store the price of the products 
currentp &amp;lt;- c()
for(i in 1:length(url_list)){
  
  # Navigate to link [i]
  remDr$navigate(url_list[i])
  
  # Find the price with an xpath selector and findElement.  
  # Sometimes products can be removed and this could throw an error this is why we are using &amp;#39;try&amp;#39; to handle the potential errors
  
  current &amp;lt;- try(remDr$findElement(using = &amp;quot;xpath&amp;quot;,&amp;#39;//*[contains(concat( &amp;quot; &amp;quot;, @class, &amp;quot; &amp;quot; ), concat( &amp;quot; &amp;quot;, &amp;quot;product-price-value&amp;quot;, &amp;quot; &amp;quot; ))]&amp;#39;), silent = T)
  
  #If error : current price is NA 
  if(class(current) ==&amp;#39;try-error&amp;#39;){
    currentp[i] &amp;lt;- NA
  }else{
    # Get the price 
    text &amp;lt;- unlist(current$getElementText())
    
    #Remove euro sign
    text &amp;lt;- gsub(&amp;quot;[^A-Za-z0-9,;._-]&amp;quot;,&amp;quot;&amp;quot;,text)
    
    #Case when there is a range of price instead of one price + replace comma by point
    if(grepl(&amp;quot;-&amp;quot;, text)) {  
      pe &amp;lt;- sub(&amp;quot;-.*&amp;quot;,&amp;quot;&amp;quot;,text) %&amp;gt;% sub(&amp;quot;,&amp;quot;, &amp;quot;.&amp;quot;, ., fixed = TRUE)
      currentp[i] &amp;lt;-  as.numeric(pe)
    }else{
      currentp[i] &amp;lt;- as.numeric(sub(&amp;quot;,&amp;quot;, &amp;quot;.&amp;quot;, text, fixed = TRUE))
  }
  }
  
Sys.sleep(4)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between each link it is advised to wait a few seconds with &lt;em&gt;Sys.sleep(4)&lt;/em&gt; to avoid being black-listed by the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;phantomjs-version&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Phantomjs version&lt;/h3&gt;
&lt;p&gt;If you execute the code above, you should see a firefox browser open and navigate through the list you provided. In case you don’t want an active window, you can replace firefox by phantomjs browser which is a headless browser (without a window).&lt;/p&gt;
&lt;p&gt;I don’t know why but using &lt;code&gt;rsDriver(browser = &#34;phantomjs&#34;)&lt;/code&gt; does not work for me. I found &lt;a href=&#34;https://cbelanger.netlify.app/post/web-scraping-in-r-selenium-firefox-and-phantomjs/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; which propose to start the phantomjs browser with the wdman package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wdman)
library(RSelenium)
# start phantomjs instance
rPJS &amp;lt;- wdman::phantomjs(port = 4680L)

# is it alive?
rPJS$process$is_alive()

#connect selenium to it?
remDr &amp;lt;-  RSelenium::remoteDriver(browserName=&amp;quot;phantomjs&amp;quot;, port=4680L)

# open a browser
remDr$open()

remDr$navigate(&amp;quot;http://www.google.com/&amp;quot;)

# Screenshot of the headless browser to check if everything is working
remDr$screenshot(display = TRUE)

# Don&amp;#39;t forget to close the browser when you are finished ! 
remDr$close()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Once you have understand the basics of RSelenium and how to select elements inside HTML pages, it is really easy to write a script to scrape data on the web. This post was a short example to scrape the product price on Aliexpress pages but the script can be extended to scrape more data on each page such as the name of the item, its rating etc… It is even possible to automate this script to run daily in order to see price changes over time. As you see possibilities are endless!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
