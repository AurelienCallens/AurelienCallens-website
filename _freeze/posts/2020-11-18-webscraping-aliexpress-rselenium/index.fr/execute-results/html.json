{
  "hash": "7042562a39e2869864a0ec4ed6388221",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Webscraping sur Aliexpress avec Rselenium\"\nauthor: \"Aurelien Callens\"\ndate: \"2020-11-18\"\nlang: fr\nexecute:\n  eval: false\ncategories:\n  - R\n  - Web scraping\n---\n\n\n\n\n  \nAujourd'hui, je vais vous montrer comment récupérer les prix des produits sur le site Aliexpress.\n\n## Quelques mots sur le web scraping\n\nAvant de plonger dans le sujet, vous devez savoir que le web scraping n'est pas autorisé sur certains sites web. Pour savoir si cela s'applique au site que vous souhaitez scraper, je vous invite à vérifier la page *robots.txt* qui devrait se trouver à la racine de l'adresse du site. Pour Aliexpress, cette page se trouve ici : [www.aliexpress.com/robots.txt](https://www.aliexpress.com/robots.txt).\n\nCette page indique que le web scraping et le crawling ne sont pas autorisés sur plusieurs catégories de pages telles que `/bin/*`, `/search/*`, `/wholesale*` par exemple. Heureusement pour nous, la catégorie `/item/*`, où les pages des produits sont stockées, peut être scrappée.\n\n## RSelenium\n\n### Installation pour Ubuntu 18.04 LTS\n\nL'installation de RSelenium n'a pas été aussi simple que prévu et j'ai rencontré deux erreurs.\n\nLa première erreur que j'ai obtenue après avoir installé le package et essayé la fonction *Rsdriver* était :\n\n\n```\nError in curl::curl_fetch_disk(url, x$path, handle = handle) :\nUnrecognized content encoding type. libcurl understands deflate, gzip content encodings.\n```\n\nGrâce à <a href=\"https://github.com/ropensci/RSelenium/issues/186\" target=\"_blank\">ce post</a>, j'ai installé le package manquant : *stringi*.\n\nUne fois cette erreur corrigée, j'en ai rencontré une autre :\n\n\n```\nError: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar\n```\n\nCette fois-ci, le problème venait d'un fichier corrompu. Grâce à <a href=\"https://stackoverflow.com/questions/20680229/invalid-or-corrupt-jarfile-usr-local-bin-selenium-server-standalone-2-38-0-jar\" target=\"_blank\">ce post</a>, j'ai su que je devais simplement télécharger ce fichier *selenium-server-standalone-4.0.0-alpha-2.jar* depuis le site officiel <a href=\"https://selenium-release.storage.googleapis.com/index.html?path=4.0/\" target=\"_blank\">de Selenium</a> et remplacer le fichier corrompu par celui-ci.\n\nJ'espère que cela aidera certains d'entre vous à installer RSelenium sur Ubuntu 18.04 LTS !\n\n### Ouverture d'un navigateur web\n\nAprès avoir corrigé les erreurs ci-dessus, je peux maintenant ouvrir un navigateur Firefox :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RSelenium)\n\n#Open a firefox driver\nrD <- rsDriver(browser = \"firefox\") \nremDr <- rD[[\"client\"]]\n```\n:::\n\n\n\n### Connexion à Aliexpress\n\nLa première étape pour récupérer les prix des produits sur Aliexpress est de se connecter à son compte :\n \n\n::: {.cell}\n\n```{.r .cell-code}\nlog_id <- \"Your_mail_adress\"\npassword <- \"Your_password\"\n\n# Navigate to aliexpress login page \nremDr$navigate(\"https://login.aliexpress.com/\")\n\n# Fill the form with mail address\nremDr$findElement(using = \"id\", \"fm-login-id\")$sendKeysToElement(list(log_id))\n\n# Fill the form with password\nremDr$findElement(using = 'id', \"fm-login-password\")$sendKeysToElement(list(password))\n\n#Submit the login form by clicking Submit button\nremDr$findElement(\"class\", \"fm-button\")$clickElement()\n```\n:::\n\n\n\n### Navigating through the URLs and scraping the prices\n\nMaintenant, on doit naviguer à travers un vecteur contenant les URL des produits Aliexpress qui nous intéressent. Ensuite, on extrait le prix du produit en utilisant le xpath du prix du produit sur la page web. Le xpath de l'élément que vous voulez scraper peut être trouvé en utilisant les outils de développement de Chrome ou Firefox ([tutoriel ici !](https://www.scrapingbee.com/blog/practical-xpath-for-web-scraping/)). Une fois le prix extrait, il faut s'assurer que ce prix soit sous un format numérique en supprimant tout caractère spécial (symbole euro ou dollar) et en remplaçant la virgule par un point pour le séparateur décimal. Voici le code R :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  url_list <- list(\"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ\",\n            \"https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m\") \n\n# Allocate a vector to store the price of the products \ncurrentp <- c()\nfor(i in 1:length(url_list)){\n  \n  # Navigate to link [i]\n  remDr$navigate(url_list[i])\n  \n  # Find the price with an xpath selector and findElement.  \n  # Sometimes products can be removed and this could throw an error this is why we are using 'try' to handle the potential errors\n  \n  current <- try(remDr$findElement(using = \"xpath\",'//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"product-price-value\", \" \" ))]'), silent = T)\n  \n  #If error : current price is NA \n  if(class(current) =='try-error'){\n    currentp[i] <- NA\n  }else{\n    # Get the price \n    text <- unlist(current$getElementText())\n    \n    #Remove euro sign\n    text <- gsub(\"[^A-Za-z0-9,;._-]\",\"\",text)\n    \n    #Case when there is a range of price instead of one price + replace comma by point\n    if(grepl(\"-\", text)) {  \n      pe <- sub(\"-.*\",\"\",text) %>% sub(\",\", \".\", ., fixed = TRUE)\n      currentp[i] <-  as.numeric(pe)\n    }else{\n      currentp[i] <- as.numeric(sub(\",\", \".\", text, fixed = TRUE))\n  }\n  }\n  \nSys.sleep(4)\n}\n```\n:::\n\n\nIl est conseillé d'attendre quelques secondes entre chaque lien avec *Sys.sleep(4)* afin d'éviter d'être mis sur liste noire par le site web.\n\n### Version Phantomjs\n\nSi vous exécutez le code ci-dessus, vous devriez voir un navigateur Firefox s'ouvrir et naviguer à travers la liste que vous avez fournie. Dans le cas où vous ne souhaitez pas une fenêtre active, vous pouvez remplacer Firefox par le navigateur phantomjs, qui est un navigateur sans interface graphique (headless).\n\nJe ne sais pas pourquoi, mais l'utilisation de `rsDriver(browser = \"phantomjs\")` ne fonctionne pas pour moi. J'ai trouvé <a href=\"https://cbelanger.netlify.app/post/web-scraping-in-r-selenium-firefox-and-phantomjs/\" target=\"_blank\">cet article</a> qui propose de démarrer le navigateur phantomjs avec le package wdman :\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wdman)\nlibrary(RSelenium)\n# start phantomjs instance\nrPJS <- wdman::phantomjs(port = 4680L)\n\n# is it alive?\nrPJS$process$is_alive()\n\n#connect selenium to it?\nremDr <-  RSelenium::remoteDriver(browserName=\"phantomjs\", port=4680L)\n\n# open a browser\nremDr$open()\n\nremDr$navigate(\"http://www.google.com/\")\n\n# Screenshot of the headless browser to check if everything is working\nremDr$screenshot(display = TRUE)\n\n# Don't forget to close the browser when you are finished ! \nremDr$close()\n```\n:::\n\n\n\n### Conclusion\n\nUne fois que l'on comprend les bases de RSelenium et comment sélectionner des éléments dans des pages HTML, c'est assez facile d'écrire un script pour extraire des données sur le web. Cet article est un exemple simple d'extraction du prix des produits sur les pages Aliexpress, mais le script peut être étendu pour extraire plus de données sur chaque page, telles que le nom de l'article, sa note, etc. Il est même possible d'automatiser ce script pour qu'il s'exécute quotidiennement afin de suivre l'évolution des prix au fil du temps. Les possibilités sont infinies !\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}