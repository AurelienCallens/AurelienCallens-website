<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Aurélien Callens</title>
    <link>https://aureliencallens.github.io/post/</link>
    <description>Recent content in Posts on Aurélien Callens</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>aurelien.callens@gmail.com (Aurelien Callens)</managingEditor>
    <webMaster>aurelien.callens@gmail.com (Aurelien Callens)</webMaster>
    <lastBuildDate>Wed, 21 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://aureliencallens.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimizing my search for Data scientist jobs by scraping Indeed with R</title>
      <link>https://aureliencallens.github.io/2022/09/21/web-scraping-indeed-with-r/</link>
      <pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2022/09/21/web-scraping-indeed-with-r/</guid>
      <description>
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/proj4/proj4.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-providers/leaflet-providers_1.9.0.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-providers-plugin/leaflet-providers-plugin.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-markercluster/MarkerCluster.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-markercluster/MarkerCluster.Default.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-markercluster/leaflet.markercluster.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-markercluster/leaflet.markercluster.freezable.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-markercluster/leaflet.markercluster.layersupport.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A few weeks ago, I started looking for a data scientist position in industry. My first moves were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To look at the job posts on websites such as Indeed&lt;/li&gt;
&lt;li&gt;To update my resume&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After reading numerous job posts and work several hours on my resume, I wondered if I could optimize these steps with R and Data Science. I therefore decided to scrape Indeed and analyze the data about data science jobs to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Get a visual overview of essential information such as location, type of contract, salary range for the large number of job posts&lt;/li&gt;
&lt;li&gt;Optimize my resume for ATS scan with accurate key words&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;table-of-contents&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#loading-libraries&#34;&gt;Loading libraries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#collect-the-data-with-web-scraping&#34;&gt;Collect the data with web scraping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-of-the-proposed-salaries&#34;&gt;Visualization of the proposed salaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mapping-job-locations&#34;&gt;Mapping job locations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analyzing-job-descriptions&#34;&gt;Analyzing Job descriptions&lt;/a&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;#downloading-and-cleaning-each-job-description&#34;&gt;Downloading and cleaning each job description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#annotation-procedure-with-udpipe-package&#34;&gt;Annotation procedure with udpipe Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#most-common-nouns&#34;&gt;Most common nouns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#extracting-key-words-for-resume-writing&#34;&gt;Extracting key words for resume writing&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#custom-functions-to-clean-data-extracted-from-the-webpage&#34;&gt;Custom functions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Loading libraries&lt;/h2&gt;
&lt;p&gt;The first step is to import several packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# General
library(tidyverse)
# Webscraping 
library(rvest)
library(RSelenium)
# Geo data
library(tidygeocoder)
library(leaflet)
library(rnaturalearth)
library(sf)
# NLP
library(udpipe)
library(textrank)
library(wordcloud)
# Cleaning
library(stringr)
# Additional functions presented at the end of the post 
source(&amp;#39;scraping_functions.R&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;collect-the-data-with-web-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Collect the data with web scraping&lt;/h2&gt;
&lt;p&gt;In the beginning of this project, I was using &lt;code&gt;read_html()&lt;/code&gt; from &lt;strong&gt;rvest&lt;/strong&gt; to access and download the webpage from Indeed. However, Indeed pages are protected by an anti-scrapping software that blocked any of my requests even though scraping is not forbidden on the pages I am interested in (I checked the &lt;em&gt;robots.txt&lt;/em&gt; page).&lt;/p&gt;
&lt;p&gt;This is why I decided to access the pages with &lt;strong&gt;Rselenium&lt;/strong&gt; which allows to run an headless browser. We first navigate to the page corresponding to the search results of data scientist jobs in France:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url = &amp;quot;https://fr.indeed.com/jobs?q=data%20scientist&amp;amp;l=France&amp;amp;from=searchOnHP&amp;quot;

# Headless Firefox browser
exCap &amp;lt;- list(&amp;quot;moz:firefoxOptions&amp;quot; = list(args = list(&amp;#39;--headless&amp;#39;)))
rD &amp;lt;- rsDriver(browser = &amp;quot;firefox&amp;quot;, extraCapabilities = exCap, port=1111L,
                verbose = F)
remDr &amp;lt;- rD$client

# Navigate to the url
remDr$navigate(url)

# Store page source 
web_page &amp;lt;- remDr$getPageSource(header = TRUE)[[1]] %&amp;gt;% read_html()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To scrape a specific information on a webpage you need to follow these steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Find on the web page the element/text/data you want to scrape&lt;/li&gt;
&lt;li&gt;Find the associated xpath or css selector with the developer tool of chrome or firefox ( &lt;a href=&#34;https://www.scrapingbee.com/blog/practical-xpath-for-web-scraping/&#34;&gt;tutorial here !&lt;/a&gt; )&lt;/li&gt;
&lt;li&gt;Extract the element with &lt;code&gt;hmtl_element()&lt;/code&gt; by indicating the xpath or css selector&lt;/li&gt;
&lt;li&gt;Transform the data to text with &lt;code&gt;html_text2()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Clean the data if necessary&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is the example with the number of listed data science jobs in France:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;web_page %&amp;gt;%
  html_element(css = &amp;quot;div.jobsearch-JobCountAndSortPane-jobCount&amp;quot;) %&amp;gt;% # selecting with css 
  html_text2() %&amp;gt;% # Transform to text
  str_remove_all(&amp;quot;[^0-9.-]&amp;quot;) %&amp;gt;% # Clean the data to only get numbers
  substr(start = 2, stop = 8) %&amp;gt;% 
  as.numeric()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1761&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For now, we can only scrape the data from the first page. However, I am interested in all the job posts and I need to access the other pages ! After navigating through the first 3 pages of listed jobs, I remarked a pattern in the URL address (valid at the time of writing), this means that with a line of code, I can produce a list containing the URLs for the first 40 pages.&lt;/p&gt;
&lt;p&gt;Once I have the list, the only thing left is to loop over all the URLs with some delay (good practice for web-scraping), collect the data and clean it with custom functions (at the end of the post):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating URL link corresponding to the first 40 pages
base_url = &amp;quot;https://fr.indeed.com/jobs?q=data%20scientist&amp;amp;l=France&amp;amp;start=&amp;quot;
url_list &amp;lt;- c(url, paste0(base_url, as.character(seq(from=10, to=400, by=10))))

# Looping through the URL list
res &amp;lt;- list()
for(i in 1:length(url_list)){
  # Navigate to the URL
  remDr$navigate(url_list[i])
  
  # Store page source 
  web_page &amp;lt;- remDr$getPageSource(header = TRUE)[[1]] %&amp;gt;% read_html()

  # Job title 
  job_title &amp;lt;- web_page %&amp;gt;%
    html_elements(css = &amp;quot;.mosaic-provider-jobcards .result&amp;quot;) %&amp;gt;%
    html_elements(css = &amp;quot;.resultContent&amp;quot;) %&amp;gt;%
    html_element(&amp;quot;h2&amp;quot;) %&amp;gt;%
    html_text2() %&amp;gt;%
    str_replace(&amp;quot;.css.*;\\}&amp;quot;, &amp;quot;&amp;quot;)

  # URL for job post 
  job_url &amp;lt;- web_page %&amp;gt;%
    html_elements(css = &amp;quot;.mosaic-provider-jobcards .result&amp;quot;)%&amp;gt;%
    html_elements(css = &amp;quot;.resultContent&amp;quot;) %&amp;gt;%
    html_element(&amp;quot;h2&amp;quot;) %&amp;gt;%
    html_element(&amp;quot;a&amp;quot;) %&amp;gt;%
    html_attr(&amp;#39;href&amp;#39;) %&amp;gt;%
    lapply(function(x){paste0(&amp;quot;https://fr.indeed.com&amp;quot;, x)}) %&amp;gt;%
    unlist()
  
  # Data about company
  company_info &amp;lt;- web_page %&amp;gt;%
    html_elements(css = &amp;quot;.mosaic-provider-jobcards .result&amp;quot;)%&amp;gt;%
    html_elements(css = &amp;quot;.resultContent&amp;quot;)%&amp;gt;%
    html_element(css = &amp;quot;.company_location&amp;quot;)%&amp;gt;%
    html_text2() %&amp;gt;%
    lapply(FUN = tidy_comploc) %&amp;gt;% # Function to clean the textual data
    do.call(rbind, .)

  # Data about job description
  job_desc &amp;lt;- web_page %&amp;gt;%
    html_elements(css = &amp;quot;.mosaic-provider-jobcards .result&amp;quot;)%&amp;gt;%
    html_element(css =&amp;quot;.slider_container .jobCardShelfContainer&amp;quot;)%&amp;gt;%
    html_text2() %&amp;gt;%
    tidy_job_desc() # Function to clean the textual data related to job desc.

  # Data about salary (when indicated)
  salary_hour &amp;lt;- web_page %&amp;gt;%
    html_elements(css = &amp;quot;.mosaic-provider-jobcards .result .resultContent&amp;quot;)%&amp;gt;%
    html_element(css = &amp;quot;.salaryOnly&amp;quot;) %&amp;gt;%
    html_text2() %&amp;gt;%
    lapply(FUN = tidy_salary) %&amp;gt;% # Function to clean the data related to salary
    do.call(rbind, .)
  
  # Job posts in the same format
  final_df &amp;lt;- cbind(job_title, company_info, salary_hour, job_desc, job_url)
  colnames(final_df) &amp;lt;- c(&amp;quot;Job_title&amp;quot;, &amp;quot;Company&amp;quot;, &amp;quot;Location&amp;quot;, &amp;quot;Rating&amp;quot;, &amp;quot;Low_salary&amp;quot;, &amp;quot;High_salary&amp;quot;, &amp;quot;Contract_info&amp;quot;, &amp;quot;Job_desc&amp;quot;, &amp;quot;url&amp;quot;)
  res[[i]] &amp;lt;- final_df
  
  # Sleep 5 seconds, good practice for web scraping
  Sys.sleep(5)
}

# Gather all the job post in a tibble
final_df &amp;lt;- as_tibble(do.call(&amp;quot;rbind&amp;quot;, res))

# Final data cleaning
final_df &amp;lt;- final_df %&amp;gt;%
  mutate_at(c(&amp;quot;Rating&amp;quot;, &amp;quot;Low_salary&amp;quot;, &amp;quot;High_salary&amp;quot;), as.numeric)

# Clean job title
final_df$Job_title_c &amp;lt;- clean_job_title(final_df$Job_title)  
final_df$Job_title_c &amp;lt;- as.factor(final_df$Job_title_c)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have now a tidy data set! Here is a truncated example of the 5 first rows:&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; &#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Job_title
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Company
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Location
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Rating
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Low_salary
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
High_salary
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Contract_info
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Job_desc
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Job_type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Job_title_c
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Data Scientist junior (H/F)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kea &amp;amp; Partners
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
92240 Malakoff
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3750
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4583
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CDI +2 | Travail en journée +1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Plusieurs postes à pourvoirMaitrise de Python et des packages de data science. 1er cabinet européen de conseil en stratégie à devenir Société à Mission, certifiés B-Corp depuis 2021*,…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Présentiel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
data scientist junior
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Data Scientist (F ou H)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SNCF
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Saint-Denis (93)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CDI
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Le développement informatique (C, C++, Python, Azure, …). Valider et recetter les phases des projets. Travailler avec des méthodes agiles avec les équipes et…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Présentiel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
data scientist
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Data Scientist (H/F) (IT)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yzee Services
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Paris (75)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2916
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3750
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Temps plein
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Recueillir, structurer et analyser les données pertinentes pour l’entreprise (activité liée à la relation client, conseil en externe).
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Présentiel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
data scientist
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Data Scientist H/F
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Natan (SSII)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Paris (75)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4583
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5833
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CDI +1 | Travail en journée
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Plusieurs postes à pourvoirVous retrouverez une &lt;em&gt;ESN ambitieuse portée par le goût de l’excellence.&lt;/em&gt;. Au sein du département en charge d’automatisation transverse des besoins de la…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Présentiel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
data scientist
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Data Scientist Junior H/F / Freelance
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
karma partners
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Roissy-en-Brie (77)
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
400
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
550
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Temps plein +1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Le profil recherché est un profil junior (0-2 ans d’expérience) en data science, avec une appétence technique et des notions d’architecture logicielle et de…
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Présentiel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
data scientist junior
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-of-the-proposed-salaries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization of the proposed salaries&lt;/h3&gt;
&lt;p&gt;Let’s see if we can get some insights about data science jobs by making some graphical representations. The first thing I wanted to know is how much the companies are willing to pay in order to recruit a data science candidate. I therefore decided to make some figures about the salary range depending on the company and the job title.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beware!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The following graphs must be taken with a grain of salt as they display a small sample of the data. Indeed, the salary was listed for only 14% of the job post. The insights or trends in these graphs may not be representative of companies that have not listed their proposed salary.&lt;/p&gt;
&lt;div id=&#34;salary-by-company&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Salary by company&lt;/h4&gt;
&lt;p&gt;The following graphic shows the monthly income listed by some companies (not all the companies list their proposed salary):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function to make euro X scale 
euro &amp;lt;- scales::label_dollar(
  prefix = &amp;quot;&amp;quot;,
  suffix = &amp;quot;\u20ac&amp;quot;,
  big.mark = &amp;quot;.&amp;quot;,
  decimal.mark = &amp;quot;,&amp;quot;
)

final_df %&amp;gt;%
  filter(Low_salary &amp;gt; 1600) %&amp;gt;% # To remove internships and freelance works
  select(Company, Low_salary, High_salary) %&amp;gt;%
  group_by(Company) %&amp;gt;%
  summarize_if(is.numeric, mean) %&amp;gt;%
  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),
           Company = fct_reorder(Company, desc(-Mean_salary))) %&amp;gt;%
  ggplot(aes(x = Company)) +
  geom_point(aes(y = Mean_salary), colour = &amp;quot;#267266&amp;quot;) +
  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +
  geom_hline(aes(yintercept = median(Mean_salary)), lty=2, col=&amp;#39;red&amp;#39;, alpha = 0.7) +
  scale_y_continuous(labels = euro) +
  ylab(&amp;quot;Monthly income&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) +
  coord_flip() +
  theme_bw(base_size = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-09-21-web-scraping-indeed-with-r_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The median monthly salary is around 3700 euros. As you can see the salaries can vary a lot depending on the company. This is partly due because I didn’t make distinction between the different data science jobs (data scientist, data analyst, data engineer, senior or lead).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;salary-by-job-title&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Salary by job title&lt;/h4&gt;
&lt;p&gt;We can plot the same graph but instead of grouping by company we can group by job title:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_df %&amp;gt;%
  filter(Low_salary &amp;gt; 1600) %&amp;gt;%  # To remove internships and freelance works
  select(Job_title_c, Low_salary, High_salary, Job_type) %&amp;gt;%
  group_by(Job_title_c) %&amp;gt;%
  summarize_if(is.numeric, ~ mean(.x, na.rm = TRUE)) %&amp;gt;%
  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),
         Job_title_c = fct_reorder(Job_title_c, desc(-Mean_salary))) %&amp;gt;%
  ggplot(aes(x = Job_title_c, y = Mean_salary)) +
  geom_point(aes(y = Mean_salary), colour = &amp;quot;#267266&amp;quot;) +
  geom_linerange(aes(ymin = Low_salary, ymax = High_salary)) +
  #geom_label(aes(label = n, Job_title_c, y = 1500), data = count_df) + 
  scale_y_continuous(labels = euro) +
  theme_bw(base_size = 12) +
  xlab(&amp;quot;&amp;quot;) +
  ylab(&amp;quot;Monthly Income&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-09-21-web-scraping-indeed-with-r_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We clearly see the differences in proposed salaries depending on the job title: data scientists seem to earn slightly more in average than data analysts. The companies also seem to propose higher salaries for jobs with more responsibilities or requiring more experiences (senior, lead).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;salary-depending-on-location-full-remote-hybrid-on-site&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Salary depending on location: full remote, hybrid, on site ?&lt;/h4&gt;
&lt;p&gt;Finally we can plot the salaries depending on the location (full remote, hybrid, on site) to see if it has an impact:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Tidy the types and locations of listed jobs
final_df &amp;lt;- tidy_location(final_df)
count_df &amp;lt;- count(final_df %&amp;gt;% filter(Low_salary &amp;gt; 1600), Job_type)
final_df %&amp;gt;%
  filter(Low_salary &amp;gt; 1600) %&amp;gt;% 
  drop_na(Location) %&amp;gt;%
  mutate(Mean_salary = rowMeans(cbind(Low_salary, High_salary), na.rm = T),
         Job_type = as.factor(Job_type)) %&amp;gt;%
    ggplot(aes(x = Job_type, y = Mean_salary)) +
  geom_boxplot(na.rm = TRUE) +
  geom_label(aes(label = n, Job_type, y = 5500), data = count_df) + 
  scale_y_continuous(labels = euro) + 
  theme_bw(base_size = 12) +
  xlab(&amp;quot;Job Type&amp;quot;) +
  ylab(&amp;quot;Income&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-09-21-web-scraping-indeed-with-r_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is worth noting that most of the jobs proposed in France are on site jobs. The median salary for this type of jobs is slightly lower than hybrid jobs. The salary distribution of full remote and hybrid jobs must be taken with care as it is only represented by 12 job posts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mapping-job-locations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mapping job locations&lt;/h3&gt;
&lt;p&gt;During my job search, I was frustrated not to see a geographical map regrouping the locations of all the proposed jobs. Such map could help me greatly in my search. Let’s do it !&lt;/p&gt;
&lt;p&gt;First, we must tidy and homogenize the locations for all the job posts. To this end, I made a custom function (&lt;code&gt;tidy_location()&lt;/code&gt;) which includes some &lt;strong&gt;stringr&lt;/strong&gt; functions, you can find more details about this function at the end of this post. It outputs the location in this format &lt;code&gt;[Town]([Zip code])&lt;/code&gt;. Even though all the locations have been homogenized, it can not be plotted on a map (we need the longitude and latitude). To get the latitude and longitude with the town name and zip code I used the &lt;code&gt;geocode()&lt;/code&gt; function from &lt;strong&gt;tidygeocoder&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract coordinates from town name
final_df &amp;lt;- final_df %&amp;gt;%
  mutate(Loc_tidy_fr = paste(Loc_tidy, &amp;#39;France&amp;#39;)) %&amp;gt;%
  geocode(Loc_tidy_fr, method = &amp;#39;arcgis&amp;#39;, lat = latitude , long = longitude) %&amp;gt;%
  select(- Loc_tidy_fr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;distribution-of-data-science-jobs-in-france&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Distribution of Data Science jobs in France&lt;/h4&gt;
&lt;p&gt;We can now represent the number of Data Science jobs by departments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Map of France from rnaturalearth package
france &amp;lt;- ne_states(country = &amp;quot;France&amp;quot;, returnclass = &amp;quot;sf&amp;quot;) %&amp;gt;% 
  filter(!name %in% c(&amp;quot;Guyane française&amp;quot;, &amp;quot;Martinique&amp;quot;, &amp;quot;Guadeloupe&amp;quot;, &amp;quot;La Réunion&amp;quot;, &amp;quot;Mayotte&amp;quot;))

# Transform location to st point 
test &amp;lt;- st_sf(final_df, geom= lapply(1:nrow(final_df), function(x){st_point(c(final_df$longitude[x],final_df$latitude[x]))}))
st_crs(test) &amp;lt;- 4326

# St_join by departments 
joined &amp;lt;- france %&amp;gt;%
  st_join(test, left = T)

# Custom breaks for visual representation
my_breaks = c(0, 2, 5, 10, 30, 50, 100, 260)

joined %&amp;gt;% 
  mutate(region=as.factor(name)) %&amp;gt;% 
  group_by(region) %&amp;gt;% 
  summarize(Job_number=n()) %&amp;gt;% 
  mutate(Job_number = cut(Job_number, my_breaks)) %&amp;gt;% 
  ggplot() +
  geom_sf(aes(fill=Job_number), col=&amp;#39;grey&amp;#39;, lwd=0.2) + 
  scale_fill_brewer(&amp;quot;Job number&amp;quot;,palette = &amp;quot;GnBu&amp;quot;) + 
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-09-21-web-scraping-indeed-with-r_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is really interesting to see that the distribution of jobs is quite heterogeneous in France. The majority of the jobs are concentrated in a few departments that include a large city. It is expected as most of the jobs are proposed by large company that are often installed in the proximity of important cities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactive-map&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Interactive map&lt;/h4&gt;
&lt;p&gt;We can go further and plot an interactive map with leaflet which allows us to search dynamically for a job post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot leaflet map
final_df %&amp;gt;%
  mutate(pop_up_text = sprintf(&amp;quot;&amp;lt;b&amp;gt;%s&amp;lt;/b&amp;gt; &amp;lt;br/&amp;gt; %s&amp;quot;,
                                     Job_title, Company)) %&amp;gt;% # Make popup text
  leaflet() %&amp;gt;%
  setView(lng = 2.36, lat = 46.31, zoom = 5.2) %&amp;gt;% # Center of France
  addProviderTiles(providers$CartoDB.Positron) %&amp;gt;%
  addMarkers(
    popup = ~as.character(pop_up_text),
    clusterOptions = markerClusterOptions()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;setView&#34;:[[46.31,2.36],5.2,[]],&#34;calls&#34;:[{&#34;method&#34;:&#34;addProviderTiles&#34;,&#34;args&#34;:[&#34;CartoDB.Positron&#34;,null,null,{&#34;errorTileUrl&#34;:&#34;&#34;,&#34;noWrap&#34;:false,&#34;detectRetina&#34;:false}]},{&#34;method&#34;:&#34;addMarkers&#34;,&#34;args&#34;:[[48.8196774463495,48.9399300000001,48.8276940066496,48.8276940066496,48.7999060070039,48.7999060070039,44.8367000000001,48.88264,50.674,48.8276940066496,48.8121100000001,47.37208,43.6211015185853,48.8260444622458,48.8276940066496,48.8276940066496,48.8276940066496,48.8266600000001,48.8481900230275,43.63458,43.6597741755991,43.7042970134431,44.2021300000001,48.8276940066496,43.62235,43.6211015185853,48.8931433281449,43.3003027583506,48.8276940066496,46.559417044,43.6211015185853,48.8305285361195,48.89617,48.88822,48.8931433281449,45.194121000913,44.8367000000001,43.5703500000001,48.77108,48.8571700000001,43.5168546243986,48.8276940066496,43.6597741755991,48.8276940066496,43.5168546243986,48.89617,43.5168546243986,48.5395800000001,48.8276940066496,48.7687300000001,48.8276940066496,48.8571700000001,48.8935500000001,48.8276940066496,48.8931433281449,48.7327323895336,48.8276940066496,48.88822,48.89617,48.8262011247733,48.88822,48.89617,48.8262011247733,48.6999700000001,48.8151800000001,48.8305285361195,46.559417044,48.88822,48.83525,46.32124,48.8276940066496,43.6392095751544,48.8263481193168,48.8571700000001,47.5049560132719,48.8276940066496,48.9399300000001,48.82163,44.8367000000001,48.8121100000001,48.79983,48.84365,46.559417044,48.7929100000001,48.8276940066496,48.8276940066496,48.8574097079022,45.9982900000001,43.6211015185853,47.32698,48.8276940066496,48.8258208870661,48.8276940066496,45.7365411623063,48.8276940066496,48.8276940066496,48.9399300000001,50.6371823834797,48.8276940066496,48.8276940066496,50.6371823834797,46.15943,48.83525,48.8276940066496,50.6371823834797,48.8276940066496,48.8276940066496,44.8367000000001,49.1272448557142,48.79988,43.3003027583506,48.8931433281449,43.31856,45.75917,48.8259799593331,48.88822,48.8677300000001,50.6371823834797,48.8276940066496,48.8276940066496,48.775958474443,47.3661200000001,48.8931433281449,48.8276940066496,48.8305285361195,48.8262011247733,48.8276940066496,44.8367000000001,48.83525,48.7999060070039,48.8276940066496,48.83525,48.8276940066496,48.8481900230275,48.8276940066496,50.6120100433994,49.1147,48.8276940066496,45.790968583422,48.8276940066496,50.6371823834797,48.8276940066496,48.8276940066496,48.8276940066496,48.88264,48.9399300000001,43.31856,50.6371823834797,48.83525,45.75917,46.559417044,43.3299611373217,48.7929100000001,48.83525,48.5977500000001,48.8574097079022,45.8400600000001,48.8276940066496,47.3884600000001,48.7322534636745,48.8276940066496,45.8059200000001,50.674,45.7358762720328,48.8305285361195,48.88822,47.8117669528344,48.8276940066496,45.194121000913,48.8276940066496,48.76532,48.82163,49.00397,48.7322534636745,45.7766800000001,48.8116800000001,48.8276940066496,45.8159200000001,43.31856,45.7754997089554,45.7754997089554,46.69746,48.8276940066496,43.31856,48.830678830864,48.8258947847109,43.6211015185853,48.8276940066496,48.8258208870661,48.8276940066496,48.5218300000001,48.88822,48.08018,48.8276940066496,48.8276940066496,45.194121000913,48.8258208870661,48.76532,50.60585,50.6120100433994,47.3884600000001,50.6120100433994,45.7358762720328,43.6211015185853,48.8276940066496,48.8116800000001,48.8276940066496,48.86452,45.8059200000001,45.75917,48.8276940066496,48.89617,48.8276940066496,48.8276940066496,48.8276940066496,49.4962600000001,48.8305285361195,48.88822,48.8276940066496,48.7946600000001,48.9001400000001,43.6597741755991,48.8276940066496,47.2067075956284,48.8935500000001,48.8935500000001,48.8276940066496,50.6194914652083,48.8276940066496,50.674,45.8159200000001,48.8276940066496,48.8376400000001,48.58504,50.60585,48.8276940066496,48.8574097079022,48.7820785390683,48.8276940066496,48.9250200000001,48.8276940066496,48.8276940066496,50.6371823834797,50.60585,48.8276940066496,48.8261226082913,50.6120100433994,50.6120100433994,48.89617,47.2086104781915,48.8276940066496,48.8935500000001,48.8276940066496,48.9001400000001,48.8276940066496,48.8276940066496,48.6159000000001,45.75917,45.194121000913,48.8276940066496,48.8276940066496,48.8276940066496,44.24965,48.71299,48.8276940066496,50.6371823834797,48.8262746628261,48.8276940066496,48.8935500000001,48.8276940066496,48.8276940066496,48.9399300000001,43.7042970134431,44.8224958398699,48.25316,47.2086104781915,48.8258947847109,48.39043,48.8276940066496,46.3310324878392,48.8276940066496,48.830678830864,45.194121000913,45.75917,50.6371823834797,45.75917,50.6371823834797,48.88822,48.8258208870661,45.75917,46.559417044,50.6371823834797,43.62235,43.5168546243986,43.6597741755991,47.2086104781915,48.8258947847109,44.8367000000001,48.8276940066496,48.8262746628261,48.89617,45.790968583422,49.2091530679694,48.8276940066496,48.9250200000001,48.8276940066496,48.88264,48.8276940066496,47.2086104781915,48.8276940066496,48.8212300000001,48.8276940066496,48.9071000000001,48.8276940066496,50.6914300000001,47.2086104781915,50.6582419810157,45.1564112169298,48.7929100000001,43.6211015185853,48.843226371132,43.62235,48.0981927928816,48.8116800000001,43.6211015185853,48.88437,48.8276940066496,48.8212300000001,48.89617,50.6371823834797,43.6392095751544,50.6371823834797,48.8371784449714,48.69883,48.83525,48.8276940066496,48.8276940066496,50.60585,48.8574097079022,43.5168546243986,48.9071000000001,48.88822,50.6371823834797,49.00397,48.8212300000001,48.8212300000001,48.8212300000001,48.830678830864,48.0981927928816,48.9001400000001,43.6211015185853,48.8276940066496,48.8276940066496,48.8481900230275,48.89617,48.8276940066496,48.8276940066496,48.76989,48.89617,48.8276940066496,48.76989,48.89617,48.8276940066496,46.3000000000001,48.88437,50.6914300000001,44.8367000000001,43.5168546243986,50.6371823834797,48.88822,43.6211015185853,48.8574097079022,45.194121000913,45.75917,48.88437,48.88437,50.6371823834797,48.8276940066496,48.8276940066496,48.88822,44.7881781565531,48.89617,48.8276940066496,48.37269,43.69381,48.43602,48.88264,50.60585,48.8305285361195,48.8371283233509,48.8935500000001,48.8258947847109,46.559417044,48.8276940066496,48.8276940066496,47.3019094848752,50.6582419810157,48.8276940066496,48.8276940066496,48.8276940066496,43.31856,48.80668,48.8276940066496,50.6798000000001,47.8029700000001,47.4741687345227,47.2086104781915,48.8262746628261,48.8276940066496,48.8212300000001,48.8276940066496,48.8276940066496,45.8991,48.8931433281449,43.6597741755991,48.8276940066496,48.8276940066496,48.80668,48.8276940066496,50.39689,50.6371823834797,48.83525,48.775958474443,48.88822,48.8276940066496,48.871045,44.8418100000001,48.8276940066496,48.8276940066496,46.32124,48.9280000000001,48.8371283233509,48.8276940066496,48.83525,48.8276940066496,48.8935500000001,43.31856,50.6371823834797,48.8276940066496,48.8263481193168,48.8276940066496,48.8276940066496,48.871045,43.6211015185853,45.8991,44.9113400000001,50.6371823834797,46.32124,45.1637400000001,50.6371823834797,50.6371823834797,47.47457,48.80082,50.6582419810157,48.88822,45.6188700000001,48.88264,48.8276940066496,48.83525,48.82163,46.559417044,50.687580340085,48.8276940066496,47.2086104781915,46.559417044,48.8276940066496,43.31856,48.8276940066496,48.8276940066496,47.2975000000001,48.8276940066496,45.6467700000001,48.8276940066496,48.8276940066496,48.79983,48.9280000000001,48.8276940066496,48.8276940066496,48.9136,48.88264,43.5168546243986,48.8276940066496,48.8276940066496,44.8367000000001,48.65058,50.6371823834797,50.6371823834797,48.8276940066496,45.194121000913,48.8305285361195,48.8276940066496,45.75917,48.80668,43.5934298639117,48.88437,43.6211015185853,48.8276940066496,50.6371823834797,48.8276940066496,48.8574097079022,45.1564112169298,45.75917,48.88437,48.8276940066496,47.2086104781915,43.62235,48.8276940066496,43.5296500000001,48.83525,46.3265000000001,46.32124,47.6583400000001,43.5168546243986,44.8367000000001,50.7228769167483,48.8276940066496,48.8276940066496,44.8367000000001,48.8677300000001,43.31856,48.8276940066496,48.8276940066496,45.7740463965525,48.8276940066496,44.8367000000001,44.8367000000001,48.8276940066496,48.8375632433342,45.75917,48.8276940066496,48.8574097079022,46.559417044,47.7517,48.8931433281449,45.75917,48.88437,48.7130300000001,48.88822,43.62235,48.8931433281449,48.8677300000001,48.8276940066496,43.6211015185853,49.00397,48.8276940066496,48.89617,48.95599,48.8370782017304,48.6126300000001,50.6371823834797,48.8305285361195,45.764734043248,49.0755100000001,48.81215,43.3003027583506,43.62235,48.89617,41.3877400000001,48.8276940066496,48.8276940066496,50.6371823834797,48.8375632433342,48.8276940066496,48.88437,48.8276940066496,48.7999060070039,50.6371823834797,48.89617,48.8677300000001,48.8276940066496,48.8574097079022,48.83525,48.8276940066496,50.6371823834797,45.764734043248,43.69381,48.8931433281449,48.77108,50.6582419810157,48.8276940066496,50.5764300000001,50.6371823834797,48.58504,50.6371823834797,48.5802100000001,48.8571700000001,48.8276940066496,45.75917,50.6371823834797,49.0752600000001,48.8574097079022,48.8276940066496,50.6371823834797,48.8276940066496,43.41808,48.64261,45.75917,48.81215,43.6392095751544,50.6120100433994,48.8258208870661,48.84707,48.8276940066496,48.89617,48.8305285361195,45.194121000913,48.8276940066496,48.8276940066496,45.75917,48.8276940066496,48.8481900230275,48.8375632433342,43.63458,48.8574097079022,48.8370782017304],[2.30434645326201,2.35547000000003,2.37910003512124,2.37910003512124,2.63375547244205,2.63375547244205,-0.581069999999954,2.24024000000003,3.09420000000006,2.37910003512124,2.23791000000006,-1.17890999999997,1.41814319795288,2.38156341157237,2.37910003512124,2.37910003512124,2.37910003512124,2.12583000000006,2.24489767198392,1.39684000000005,1.42183453177351,7.25148501212311,0.620550000000037,2.37910003512124,7.04721000000006,1.41814319795288,2.22674174092761,-0.347240731774215,2.37910003512124,2.55053995300005,1.41814319795288,2.37650024406499,2.25648000000007,2.19428000000005,2.22674174092761,5.72091403683689,-0.581069999999954,3.90526000000006,2.06970000000007,2.34140000000002,5.44543867691176,2.37910003512124,1.42183453177351,2.37910003512124,5.44543867691176,2.25648000000007,5.44543867691176,2.66413000000006,2.37910003512124,1.94898000000006,2.37910003512124,2.34140000000002,2.28959000000003,2.37910003512124,2.22674174092761,2.28826745484995,2.37910003512124,2.19428000000005,2.25648000000007,2.38130747959828,2.19428000000005,2.25648000000007,2.38130747959828,2.22724000000005,2.34860000000003,2.37650024406499,2.55053995300005,2.19428000000005,2.24073000000004,-0.463379999999972,2.37910003512124,3.87336959872347,2.38104254662466,2.34140000000002,-0.604462092824452,2.37910003512124,2.35547000000003,2.41350000000006,-0.581069999999954,2.23791000000006,2.28980000000007,2.41788000000003,2.55053995300005,2.36933000000005,2.37910003512124,2.37910003512124,2.34159982722465,4.90230000000003,1.41814319795288,5.04299000000003,2.37910003512124,2.38151779666531,2.37910003512124,4.92261171724876,2.37910003512124,2.37910003512124,2.35547000000003,3.01920473644255,2.37910003512124,2.37910003512124,3.01920473644255,-1.15163999999993,2.24073000000004,2.37910003512124,3.01920473644255,2.37910003512124,2.37910003512124,-0.581069999999954,6.16731649296845,2.26310000000007,-0.347240731774215,2.22674174092761,5.40836000000007,4.82965000000007,2.38168989223694,2.19428000000005,2.22612000000004,3.01920473644255,2.37910003512124,2.37910003512124,2.45522013504922,-1.20213999999993,2.22674174092761,2.37910003512124,2.37650024406499,2.38130747959828,2.37910003512124,-0.581069999999954,2.24073000000004,2.63375547244205,2.37910003512124,2.24073000000004,2.37910003512124,2.24489767198392,2.37910003512124,3.055030118234,6.17145000000005,2.37910003512124,4.81297429597299,2.37910003512124,3.01920473644255,2.37910003512124,2.37910003512124,2.37910003512124,2.24024000000003,2.35547000000003,5.40836000000007,3.01920473644255,2.24073000000004,4.82965000000007,2.55053995300005,5.39068374837129,2.36933000000005,2.24073000000004,2.42461000000003,2.34159982722465,5.00204000000002,2.37910003512124,0.68957000000006,2.26610016700937,2.37910003512124,4.75352000000004,3.09420000000006,4.8196192395397,2.37650024406499,2.19428000000005,1.0811230001304,2.37910003512124,5.72091403683689,2.37910003512124,2.13844000000006,2.41350000000006,2.51638000000003,2.26610016700937,3.07722000000007,2.38489000000004,2.37910003512124,4.81732000000005,5.40836000000007,4.8635710317682,4.8635710317682,-1.76098999999994,2.37910003512124,5.40836000000007,2.37679893592174,2.38185942419139,1.41814319795288,2.37910003512124,2.38151779666531,2.37910003512124,2.26638000000003,2.19428000000005,7.36469000000005,2.37910003512124,2.37910003512124,5.72091403683689,2.38151779666531,2.13844000000006,3.07743000000005,3.055030118234,0.68957000000006,3.055030118234,4.8196192395397,1.41814319795288,2.37910003512124,2.38489000000004,2.37910003512124,2.44265000000007,4.75352000000004,4.82965000000007,2.37910003512124,2.25648000000007,2.37910003512124,2.37910003512124,2.37910003512124,0.359270000000038,2.37650024406499,2.19428000000005,2.37910003512124,2.33495000000005,2.30646000000007,1.42183453177351,2.37910003512124,-1.57767546611809,2.28959000000003,2.28959000000003,2.37910003512124,3.13113286260462,2.37910003512124,3.09420000000006,4.81732000000005,2.37910003512124,2.63240000000008,7.73642000000007,3.07743000000005,2.37910003512124,2.34159982722465,2.20206992499755,2.37910003512124,2.29449000000005,2.37910003512124,2.37910003512124,3.01920473644255,3.07743000000005,2.37910003512124,2.38143838663328,3.055030118234,3.055030118234,2.25648000000007,-1.61106447839568,2.37910003512124,2.28959000000003,2.37910003512124,2.30646000000007,2.37910003512124,2.37910003512124,2.37778000000003,4.82965000000007,5.72091403683689,2.37910003512124,2.37910003512124,2.37910003512124,2.14805000000007,2.36571000000004,2.37910003512124,3.01920473644255,2.38117616321901,2.37910003512124,2.28959000000003,2.37910003512124,2.37910003512124,2.35547000000003,7.25148501212311,-0.636072428533597,-0.777579999999944,-1.61106447839568,2.38185942419139,-4.48657999999995,2.37910003512124,-0.488464577230201,2.37910003512124,2.37679893592174,5.72091403683689,4.82965000000007,3.01920473644255,4.82965000000007,3.01920473644255,2.19428000000005,2.38151779666531,4.82965000000007,2.55053995300005,3.01920473644255,7.04721000000006,5.44543867691176,1.42183453177351,-1.61106447839568,2.38185942419139,-0.581069999999954,2.37910003512124,2.38117616321901,2.25648000000007,4.81297429597299,-0.326178402238162,2.37910003512124,2.29449000000005,2.37910003512124,2.24024000000003,2.37910003512124,-1.61106447839568,2.37910003512124,2.25161000000003,2.37910003512124,2.03846000000004,2.37910003512124,3.17318000000006,-1.61106447839568,3.09283602195458,5.73250854172348,2.36933000000005,1.41814319795288,2.60427375623012,7.04721000000006,-1.70524202042241,2.38489000000004,1.41814319795288,2.26935000000003,2.37910003512124,2.25161000000003,2.25648000000007,3.01920473644255,3.87336959872347,3.01920473644255,2.37055521314355,2.18778000000003,2.24073000000004,2.37910003512124,2.37910003512124,3.07743000000005,2.34159982722465,5.44543867691176,2.03846000000004,2.19428000000005,3.01920473644255,2.51638000000003,2.25161000000003,2.25161000000003,2.25161000000003,2.37679893592174,-1.70524202042241,2.30646000000007,1.41814319795288,2.37910003512124,2.37910003512124,2.24489767198392,2.25648000000007,2.37910003512124,2.37910003512124,7.41319000000004,2.25648000000007,2.37910003512124,7.41319000000004,2.25648000000007,2.37910003512124,4.83333000000005,2.26935000000003,3.17318000000006,-0.581069999999954,5.44543867691176,3.01920473644255,2.19428000000005,1.41814319795288,2.34159982722465,5.72091403683689,4.82965000000007,2.26935000000003,2.26935000000003,3.01920473644255,2.37910003512124,2.37910003512124,2.19428000000005,-0.713274833749649,2.25648000000007,2.37910003512124,7.59395000000006,5.50134000000003,-4.40055999999993,2.24024000000003,3.07743000000005,2.37650024406499,2.3705783511708,2.28959000000003,2.38185942419139,2.55053995300005,2.37910003512124,2.37910003512124,-1.49208678984398,3.09283602195458,2.37910003512124,2.37910003512124,2.37910003512124,5.40836000000007,2.33684000000005,2.37910003512124,3.15685000000002,6.38246000000004,-0.545803247183234,-1.61106447839568,2.38117616321901,2.37910003512124,2.25161000000003,2.37910003512124,2.37910003512124,6.12870000000004,2.22674174092761,1.42183453177351,2.37910003512124,2.37910003512124,2.33684000000005,2.37910003512124,3.06215000000003,3.01920473644255,2.24073000000004,2.45522013504922,2.19428000000005,2.37910003512124,2.22145149000005,-0.64758999999998,2.37910003512124,2.37910003512124,-0.463379999999972,2.04257000000007,2.3705783511708,2.37910003512124,2.24073000000004,2.37910003512124,2.28959000000003,5.40836000000007,3.01920473644255,2.37910003512124,2.38104254662466,2.37910003512124,2.37910003512124,2.22145149000005,1.41814319795288,6.12870000000004,-0.24398999999994,3.01920473644255,-0.463379999999972,6.08892000000003,3.01920473644255,3.01920473644255,-0.630699999999933,2.03087000000005,3.09283602195458,2.19428000000005,5.22923000000003,2.24024000000003,2.37910003512124,2.24073000000004,2.41350000000006,2.55053995300005,3.1160102433327,2.37910003512124,-1.61106447839568,2.55053995300005,2.37910003512124,5.40836000000007,2.37910003512124,2.37910003512124,-1.49180999999993,2.37910003512124,5.02340000000004,2.37910003512124,2.37910003512124,2.28980000000007,2.04257000000007,2.37910003512124,2.37910003512124,2.38237000000004,2.24024000000003,5.44543867691176,2.37910003512124,2.37910003512124,-0.581069999999954,-2.02314999999993,3.01920473644255,3.01920473644255,2.37910003512124,5.72091403683689,2.37650024406499,2.37910003512124,4.82965000000007,2.33684000000005,2.23266090223159,2.26935000000003,1.41814319795288,2.37910003512124,3.01920473644255,2.37910003512124,2.34159982722465,5.73250854172348,4.82965000000007,2.26935000000003,2.37910003512124,-1.61106447839568,7.04721000000006,2.37910003512124,1.52709000000004,2.24073000000004,-0.46037578499994,-0.463379999999972,-2.75984999999997,5.44543867691176,-0.581069999999954,3.16421650211755,2.37910003512124,2.37910003512124,-0.581069999999954,2.22612000000004,5.40836000000007,2.37910003512124,2.37910003512124,3.11905656280128,2.37910003512124,-0.581069999999954,-0.581069999999954,2.37910003512124,2.37058478131182,4.82965000000007,2.37910003512124,2.34159982722465,2.55053995300005,7.34367000000003,2.22674174092761,4.82965000000007,2.26935000000003,2.24628000000007,2.19428000000005,7.04721000000006,2.22674174092761,2.22612000000004,2.37910003512124,1.41814319795288,2.51638000000003,2.37910003512124,2.25648000000007,2.54041000000007,2.37060148919805,2.48231000000004,3.01920473644255,2.37650024406499,4.88650254912447,2.67528000000004,2.35699000000005,-0.347240731774215,7.04721000000006,2.25648000000007,9.16087000000005,2.37910003512124,2.37910003512124,3.01920473644255,2.37058478131182,2.37910003512124,2.26935000000003,2.37910003512124,2.63375547244205,3.01920473644255,2.25648000000007,2.22612000000004,2.37910003512124,2.34159982722465,2.24073000000004,2.37910003512124,3.01920473644255,4.88650254912447,5.50134000000003,2.22674174092761,2.06970000000007,3.09283602195458,2.37910003512124,3.05293000000006,3.01920473644255,7.73642000000007,3.01920473644255,7.68693000000007,2.34140000000002,2.37910003512124,4.82965000000007,3.01920473644255,2.10472000000004,2.34159982722465,2.37910003512124,3.01920473644255,2.37910003512124,5.21420000000006,2.29250000000008,4.82965000000007,2.35699000000005,3.87336959872347,3.055030118234,2.38151779666531,2.37578000000002,2.37910003512124,2.25648000000007,2.37650024406499,5.72091403683689,2.37910003512124,2.37910003512124,4.82965000000007,2.37910003512124,2.24489767198392,2.37058478131182,1.39684000000005,2.34159982722465,2.37060148919805],null,null,null,{&#34;interactive&#34;:true,&#34;draggable&#34;:false,&#34;keyboard&#34;:true,&#34;title&#34;:&#34;&#34;,&#34;alt&#34;:&#34;&#34;,&#34;zIndexOffset&#34;:0,&#34;opacity&#34;:1,&#34;riseOnHover&#34;:false,&#34;riseOffset&#34;:250},[&#34;&lt;b&gt;Data Scientist junior (H/F)&lt;\/b&gt; &lt;br/&gt; Kea &amp; Partners&#34;,&#34;&lt;b&gt;Data Scientist (F ou H)&lt;\/b&gt; &lt;br/&gt; SNCF&#34;,&#34;&lt;b&gt;Data Scientist (H/F) (IT)&lt;\/b&gt; &lt;br/&gt; Yzee Services&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Natan (SSII)&#34;,&#34;&lt;b&gt;Data Scientist Junior H/F / Freelance&lt;\/b&gt; &lt;br/&gt; karma partners&#34;,&#34;&lt;b&gt;Data Scientist junior / Freelance&lt;\/b&gt; &lt;br/&gt; STA&#34;,&#34;&lt;b&gt;Data scientist H/F&lt;\/b&gt; &lt;br/&gt; Unicancer&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; datakeen&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; KISS THE BRIDE&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Groupe Demeter&#34;,&#34;&lt;b&gt;Data Scientist - H/F&lt;\/b&gt; &lt;br/&gt; Bouygues Telecom&#34;,&#34;&lt;b&gt;Data scientist&lt;\/b&gt; &lt;br/&gt; Manitou Group&#34;,&#34;&lt;b&gt;Bioprocess modeler/Data scientist H/F&lt;\/b&gt; &lt;br/&gt; ALTRAN&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; K-ciopé&#34;,&#34;&lt;b&gt;Data Scientist - Projet Énergie Verte&lt;\/b&gt; &lt;br/&gt; MP Data&#34;,&#34;&lt;b&gt;Data Scientist - Projet Énergie Verte&lt;\/b&gt; &lt;br/&gt; MP Data&#34;,&#34;&lt;b&gt;Data Scientist (m/f/d) - Paris&lt;\/b&gt; &lt;br/&gt; Simon-Kucher &amp; Partners&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; GCS SAS&#34;,&#34;&lt;b&gt;Data Scientist NLP&lt;\/b&gt; &lt;br/&gt; MP DATA&#34;,&#34;&lt;b&gt;DATA ENGINEER/SCIENTIST SKYWISE (H/F)&lt;\/b&gt; &lt;br/&gt; Airbus&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Solutec&#34;,&#34;&lt;b&gt;Data Scientist (Data team)&lt;\/b&gt; &lt;br/&gt; Veepee&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Réseau Primever France&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; NEO2&#34;,&#34;&lt;b&gt;Data Scientist - H/F&lt;\/b&gt; &lt;br/&gt; Micromania Zing&#34;,&#34;&lt;b&gt;Data Scientist bio-informatique H/F&lt;\/b&gt; &lt;br/&gt; ALTRAN&#34;,&#34;&lt;b&gt;Consultant débutant en data sciences F/H - Paris&lt;\/b&gt; &lt;br/&gt; EY&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; MP DATA&#34;,&#34;&lt;b&gt;Consultant.e Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; June Partners&#34;,&#34;&lt;b&gt;Computer Vision ML, Data scientist&lt;\/b&gt; &lt;br/&gt; Sightengine&#34;,&#34;&lt;b&gt;Data Scientist - Computer Vision - Full remote / Freelance&lt;\/b&gt; &lt;br/&gt; Trait d&#39;Union Consulting&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Nestle&#34;,&#34;&lt;b&gt;Data Scientist - Énergies Renouvelables&lt;\/b&gt; &lt;br/&gt; MP DATA&#34;,&#34;&lt;b&gt;Junior Data Scientist&lt;\/b&gt; &lt;br/&gt; Systemathics&#34;,&#34;&lt;b&gt;Consultant débutant data analytics F/H - Paris&lt;\/b&gt; &lt;br/&gt; EY&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Inetum&#34;,&#34;&lt;b&gt;Data Scientist F/M&lt;\/b&gt; &lt;br/&gt; Betclic Group&#34;,&#34;&lt;b&gt;Data analyst junior H/F&lt;\/b&gt; &lt;br/&gt; Septeo&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Renault Group&#34;,&#34;&lt;b&gt;Data scientist H/F&lt;\/b&gt; &lt;br/&gt; Pôle Emploi&#34;,&#34;&lt;b&gt;Data scientist confirmé AIX EN PROVENCE H/F&lt;\/b&gt; &lt;br/&gt; Capgemini&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Groupe Pierre &amp; Vacances - Center Parcs&#34;,&#34;&lt;b&gt;DATA SCIENTIST / DATA ENGINEER, Secteur aéronautique&lt;\/b&gt; &lt;br/&gt; ALTEN&#34;,&#34;&lt;b&gt;DATA SCIENTIST F/H&lt;\/b&gt; &lt;br/&gt; HARNHAM&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Voyage Privé&#34;,&#34;&lt;b&gt;DATA SCIENTIST&lt;\/b&gt; &lt;br/&gt; Aquila Consulting&#34;,&#34;&lt;b&gt;ALT - DATA Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; GRDF&#34;,&#34;&lt;b&gt;Data scientist / Ingénieur en Intelligence Artificielle F/H...&lt;\/b&gt; &lt;br/&gt; Pôle Emploi&#34;,&#34;&lt;b&gt;DATA SCIENTIST F/H&lt;\/b&gt; &lt;br/&gt; InVivo&#34;,&#34;&lt;b&gt;Data Scientist - F/H&lt;\/b&gt; &lt;br/&gt; Thales&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; AZAP&#34;,&#34;&lt;b&gt;Data scientist user behaviour H/F&lt;\/b&gt; &lt;br/&gt; Se Loger&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Ysance&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist Junior H/F&lt;\/b&gt; &lt;br/&gt; AKKA TECHNOLOGIES&#34;,&#34;&lt;b&gt;Consultant Débutant - DATA Analyst - 2022 - Paris - H/F - oc...&lt;\/b&gt; &lt;br/&gt; EY&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist Junior - Intelligence Artificielle...&lt;\/b&gt; &lt;br/&gt; Ivalua&#34;,&#34;&lt;b&gt;DATA SCIENTIST COMPUTER VISION&lt;\/b&gt; &lt;br/&gt; Aquila Consulting&#34;,&#34;&lt;b&gt;Data Scientist Paiements -(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale&#34;,&#34;&lt;b&gt;Data Scientist - H/F&lt;\/b&gt; &lt;br/&gt; Dalkia&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Collective Thinking&#34;,&#34;&lt;b&gt;Data Scientist Paiements -(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale&#34;,&#34;&lt;b&gt;Data Scientist - H/F&lt;\/b&gt; &lt;br/&gt; Dalkia&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Collective Thinking&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; haxio&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Silex&#34;,&#34;&lt;b&gt;Data scientist H/F&lt;\/b&gt; &lt;br/&gt; Crédit Agricole Assurances&#34;,&#34;&lt;b&gt;Audio &amp; Speech Recognition Data scientist&lt;\/b&gt; &lt;br/&gt; Sightengine&#34;,&#34;&lt;b&gt;Data Scientist - Analytics Clients - H/F&lt;\/b&gt; &lt;br/&gt; BNP Paribas&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Bouygues Telecom&#34;,&#34;&lt;b&gt;Data Scientist Scoring - CDD 12 mois - NIORT F/H&lt;\/b&gt; &lt;br/&gt; MAIF&#34;,&#34;&lt;b&gt;Data Analyst Junior (F/H) - CDI&lt;\/b&gt; &lt;br/&gt; Nexity&#34;,&#34;&lt;b&gt;Data analyste&lt;\/b&gt; &lt;br/&gt; PwC&#34;,&#34;&lt;b&gt;Consultant Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; METRICS&#34;,&#34;&lt;b&gt;Data Scientist (full time, all seniority levels) - QuantumBl...&lt;\/b&gt; &lt;br/&gt; McKinsey &amp; Company&#34;,&#34;&lt;b&gt;Data scientist&lt;\/b&gt; &lt;br/&gt; Meggitt&#34;,&#34;&lt;b&gt;Consultant Junior &amp; Data Scientist (H/F/N)&lt;\/b&gt; &lt;br/&gt; Ekimetrics&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Generali France&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Natixis&#34;,&#34;&lt;b&gt;Data Scientist F/M&lt;\/b&gt; &lt;br/&gt; Automotive Cells Company - ACC&#34;,&#34;&lt;b&gt;Data Scientist Performance Mobile H/F&lt;\/b&gt; &lt;br/&gt; Bouygues Telecom&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; LES MOUSQUETAIRES&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; datafab.io&#34;,&#34;&lt;b&gt;NLP Data scientist&lt;\/b&gt; &lt;br/&gt; Sightengine&#34;,&#34;&lt;b&gt;Data Scientist IG H/F&lt;\/b&gt; &lt;br/&gt; LCL&#34;,&#34;&lt;b&gt;Data Scientist | Python | Editeur de logiciel en...&lt;\/b&gt; &lt;br/&gt; Octopus IT&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Positive Thinking Company&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Numberly&#34;,&#34;&lt;b&gt;Data analyst F/H&lt;\/b&gt; &lt;br/&gt; Eiffage&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; PredExIA&#34;,&#34;&lt;b&gt;Engineer Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; BioSerenity&#34;,&#34;&lt;b&gt;Data Scientist Energétique&lt;\/b&gt; &lt;br/&gt; WeSmart&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; DAMAE Medical&#34;,&#34;&lt;b&gt;Consultant(e) Data scientist – métiers bancaire&lt;\/b&gt; &lt;br/&gt; Groupe Consortia&#34;,&#34;&lt;b&gt;INGENIEUR·E D’ETUDE DATA SCIENTIST&lt;\/b&gt; &lt;br/&gt; Université Gustave Eiffel&#34;,&#34;&lt;b&gt;Data Scientist - Data Science&lt;\/b&gt; &lt;br/&gt; Datadog&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; TotalEnergies&#34;,&#34;&lt;b&gt;Data Scientist (modélisation études risque) H/F en CDI&lt;\/b&gt; &lt;br/&gt; La Banque Postale Consumer Finance&#34;,&#34;&lt;b&gt;Data Analyst Junior (H/F)&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;DATA SCIENTIST - F/H&lt;\/b&gt; &lt;br/&gt; Banque de France&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; Kent FR&#34;,&#34;&lt;b&gt;Data Analyst / Freelance&lt;\/b&gt; &lt;br/&gt; GROUPE HN&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; pasteque.io&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; agap2 IT&#34;,&#34;&lt;b&gt;Consultant Explorateur Data Scientist – Santé H/F&lt;\/b&gt; &lt;br/&gt; Alcimed&#34;,&#34;&lt;b&gt;Data Scientist – Consultant – Financial Services (H/F)&lt;\/b&gt; &lt;br/&gt; Deloitte&#34;,&#34;&lt;b&gt;Data Scientist / Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; FactSet Research Systems&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Michael Page&#34;,&#34;&lt;b&gt;Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; Dataworks&#34;,&#34;&lt;b&gt;Data Scientist Python - Azure H/F&lt;\/b&gt; &lt;br/&gt; DSI Group&#34;,&#34;&lt;b&gt;Data Scientist - FMCG / données consommateurs H/F/X&lt;\/b&gt; &lt;br/&gt; Mondelez&#34;,&#34;&lt;b&gt;Data Analyst/Data Quality H/F&lt;\/b&gt; &lt;br/&gt; CGI Inc&#34;,&#34;&lt;b&gt;Consultant Data Analyst Débutant F/H - Septembre 2022&lt;\/b&gt; &lt;br/&gt; EY&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Volta Medical&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Activus Group&#34;,&#34;&lt;b&gt;Data analyst Datalake (F/H)&lt;\/b&gt; &lt;br/&gt; Digital Partners&#34;,&#34;&lt;b&gt;DATA SCIENTIST SPÉCIALISÉ R&lt;\/b&gt; &lt;br/&gt; Aquila Consulting&#34;,&#34;&lt;b&gt;Data Scientist (F/H) CDI&lt;\/b&gt; &lt;br/&gt; Médiaperformances&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;Chargé d&#39;Etudes Statistiques/Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; PRINTEMPS&#34;,&#34;&lt;b&gt;DATA SCIENTIST&lt;\/b&gt; &lt;br/&gt; Harnham&#34;,&#34;&lt;b&gt;Computer Vision Data Scientist&lt;\/b&gt; &lt;br/&gt; Essilor&#34;,&#34;&lt;b&gt;Data scientist F/H&lt;\/b&gt; &lt;br/&gt; Manitou&#34;,&#34;&lt;b&gt;Senior Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Allianz France&#34;,&#34;&lt;b&gt;Consultant.e Data Scientist&lt;\/b&gt; &lt;br/&gt; Softeam&#34;,&#34;&lt;b&gt;Data scientist&lt;\/b&gt; &lt;br/&gt; FABDEV&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Groupe IGS&#34;,&#34;&lt;b&gt;Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; Harwell Management&#34;,&#34;&lt;b&gt;DATA SCIENTIST - DÉVELOPPEUR CHATBOT CONFIRMÉ(E) - H/F&lt;\/b&gt; &lt;br/&gt; Talan&#34;,&#34;&lt;b&gt;Data Scientist-Analyst GERS (H/F) - Boulogne-Billancourt (Fr...&lt;\/b&gt; &lt;br/&gt; Cegedim&#34;,&#34;&lt;b&gt;Data Scientist / Machine Learning Engineer&lt;\/b&gt; &lt;br/&gt; Air France-KLM&#34;,&#34;&lt;b&gt;Data Scientist h/f&lt;\/b&gt; &lt;br/&gt; HeadMind Partners&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; L ETUDIANT&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Artefact&#34;,&#34;&lt;b&gt;Consultant data Scientist et Python F/H&lt;\/b&gt; &lt;br/&gt; MANAGEMENT INFORMATIQUE &amp; NOUVELLES TECHNOLOGIES...&#34;,&#34;&lt;b&gt;Consultant(e) Junior Data Analytics H/F&lt;\/b&gt; &lt;br/&gt; mc2i&#34;,&#34;&lt;b&gt;Data Scientist H/F - Lille&lt;\/b&gt; &lt;br/&gt; AVISIA&#34;,&#34;&lt;b&gt;Data Scientist Junior&lt;\/b&gt; &lt;br/&gt; Xtramile&#34;,&#34;&lt;b&gt;Data Scientist - COPERNEEC&lt;\/b&gt; &lt;br/&gt; Coperneec&#34;,&#34;&lt;b&gt;DATA SCIENTIST F/H&lt;\/b&gt; &lt;br/&gt; AVISIA ALPES&#34;,&#34;&lt;b&gt;Data Analyst Junior&lt;\/b&gt; &lt;br/&gt; RSM France&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; AUSY FRANCE&#34;,&#34;&lt;b&gt;Data Scientist NLP - Paris - H/F&lt;\/b&gt; &lt;br/&gt; CleverConnect&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Nova Consulting&#34;,&#34;&lt;b&gt;Data Analyst Junior F/H&lt;\/b&gt; &lt;br/&gt; PICNIC&#34;,&#34;&lt;b&gt;Data Analyst [risque]&lt;\/b&gt; &lt;br/&gt; Kaino&#34;,&#34;&lt;b&gt;ALT-DATA ANALYST DASHBOARD PROJETS H/F&lt;\/b&gt; &lt;br/&gt; Generali France&#34;,&#34;&lt;b&gt;Data scientist&lt;\/b&gt; &lt;br/&gt; WitMonki&#34;,&#34;&lt;b&gt;DATA ANALYST&lt;\/b&gt; &lt;br/&gt; Harnham&#34;,&#34;&lt;b&gt;Data Analyst IT (H/F)&lt;\/b&gt; &lt;br/&gt; Renault Group&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; GRANDLYON HABITAT&#34;,&#34;&lt;b&gt;Data &amp; Analytics Consultant&lt;\/b&gt; &lt;br/&gt; Dataworks&#34;,&#34;&lt;b&gt;Offre d&#39;emploi en CDI : Data Scientist&lt;\/b&gt; &lt;br/&gt; Jalis&#34;,&#34;&lt;b&gt;DATA Analyst H/F&lt;\/b&gt; &lt;br/&gt; LCL&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Lobster communication&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; BFR Systems&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Deezer&#34;,&#34;&lt;b&gt;Analyste Data h/f&lt;\/b&gt; &lt;br/&gt; ISERBA&#34;,&#34;&lt;b&gt;Data Scientist (F/M)&lt;\/b&gt; &lt;br/&gt; Younited Credit&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Daher&#34;,&#34;&lt;b&gt;Data/Analyst Data Translator (H/F)&lt;\/b&gt; &lt;br/&gt; Carrefour&#34;,&#34;&lt;b&gt;Consultant Data Analytics&lt;\/b&gt; &lt;br/&gt; Dataworks&#34;,&#34;&lt;b&gt;DATA ANALYST H/F&lt;\/b&gt; &lt;br/&gt; HC RESOURCES&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Lesaffre&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; KLANIK&#34;,&#34;&lt;b&gt;Data analyst&lt;\/b&gt; &lt;br/&gt; Pôle Emploi&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Groupama Assurances Mutuelles&#34;,&#34;&lt;b&gt;Responsable Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; Don&#39;t Call Me Jennyfer&#34;,&#34;&lt;b&gt;Data Scientist – F/H&lt;\/b&gt; &lt;br/&gt; Adevinta Group&#34;,&#34;&lt;b&gt;DATA SCIENTIST - CDI - F/H&lt;\/b&gt; &lt;br/&gt; spartoo.com&#34;,&#34;&lt;b&gt;DATA SCIENTIST SANTÉ F/H&lt;\/b&gt; &lt;br/&gt; GIP SESAN&#34;,&#34;&lt;b&gt;Data Scientist - F/H&lt;\/b&gt; &lt;br/&gt; Air Liquide&#34;,&#34;&lt;b&gt;Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; Groupe BPCE&#34;,&#34;&lt;b&gt;Data Analyst- Economie des Lignes H/F&lt;\/b&gt; &lt;br/&gt; Air France-KLM&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Carrefour&#34;,&#34;&lt;b&gt;Data Scientist Expérimenté - F/H&lt;\/b&gt; &lt;br/&gt; Accenture&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Fnac Darty&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Margo Conseil&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; REEL&#34;,&#34;&lt;b&gt;Data Scientist pour l&#39;imagerie en Neuroscience F/H&lt;\/b&gt; &lt;br/&gt; Aix-Marseille Université&#34;,&#34;&lt;b&gt;Data Analyst - H/F&lt;\/b&gt; &lt;br/&gt; Avisto&#34;,&#34;&lt;b&gt;Data Analyst - H/F&lt;\/b&gt; &lt;br/&gt; Avisto&#34;,&#34;&lt;b&gt;Data scientist H/F&lt;\/b&gt; &lt;br/&gt; Segula Technologies&#34;,&#34;&lt;b&gt;Data Scientist Senior&lt;\/b&gt; &lt;br/&gt; Cleyrop&#34;,&#34;&lt;b&gt;Data scientist&lt;\/b&gt; &lt;br/&gt; IA BTP&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; AVIV Group&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Mydral&#34;,&#34;&lt;b&gt;DATA SCIENTIST – 3DTRUST H/F&lt;\/b&gt; &lt;br/&gt; Basseti Group&#34;,&#34;&lt;b&gt;Data Scientist h-f&lt;\/b&gt; &lt;br/&gt; Danem People France&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Kaino&#34;,&#34;&lt;b&gt;Accelerator - Junior Data Scientist (M/F)&lt;\/b&gt; &lt;br/&gt; Sanofi&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist Prestation GMP H/F&lt;\/b&gt; &lt;br/&gt; Renault Group&#34;,&#34;&lt;b&gt;CHARGE(E) D’ETUDES STATISTIQUES / DATA ANALYST-(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale&#34;,&#34;&lt;b&gt;Research Engineer – Data Scientist&lt;\/b&gt; &lt;br/&gt; PPRS Research&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; YZEE SERVICES&#34;,&#34;&lt;b&gt;Data Scientist H/F - CDI&lt;\/b&gt; &lt;br/&gt; Richemont&#34;,&#34;&lt;b&gt;DATA SCIENTIST - CDI - F/H&lt;\/b&gt; &lt;br/&gt; spartoo.com&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Kaino&#34;,&#34;&lt;b&gt;Data Scientist - F/H&lt;\/b&gt; &lt;br/&gt; Air Liquide&#34;,&#34;&lt;b&gt;Data Scientist - DP4P (H/F)&lt;\/b&gt; &lt;br/&gt; ADEO Services&#34;,&#34;&lt;b&gt;Data Analyst (F/H)&lt;\/b&gt; &lt;br/&gt; Meilleurtaux&#34;,&#34;&lt;b&gt;Data Analyste H/F&lt;\/b&gt; &lt;br/&gt; Q1C1&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Extia&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; KLANIK&#34;,&#34;&lt;b&gt;DATA SCIENTIST – 3DTRUST H/F&lt;\/b&gt; &lt;br/&gt; Basseti Group&#34;,&#34;&lt;b&gt;Data Scientist – F/H&lt;\/b&gt; &lt;br/&gt; Adevinta Group&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Fnac Darty&#34;,&#34;&lt;b&gt;Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; Cegid&#34;,&#34;&lt;b&gt;Data Scientist / Data analyst F/H&lt;\/b&gt; &lt;br/&gt; AUTOVISION&#34;,&#34;&lt;b&gt;DATA ANALYST H/F&lt;\/b&gt; &lt;br/&gt; HC RESOURCES&#34;,&#34;&lt;b&gt;Data analyst junior&lt;\/b&gt; &lt;br/&gt; Easylife&#34;,&#34;&lt;b&gt;Data Science Manager (F/H)&lt;\/b&gt; &lt;br/&gt; Accenture&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; autobiz&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Guerlain&#34;,&#34;&lt;b&gt;DATA SCIENCE CONSULTANT PARIS&lt;\/b&gt; &lt;br/&gt; managementsolutions&#34;,&#34;&lt;b&gt;DATA SCIENTIST NLP&lt;\/b&gt; &lt;br/&gt; Aquila Consulting&#34;,&#34;&lt;b&gt;Data Analyst KUSMI TEA H/F&lt;\/b&gt; &lt;br/&gt; KUSMI TEA&#34;,&#34;&lt;b&gt;Data scientist&lt;\/b&gt; &lt;br/&gt; FABDEV&#34;,&#34;&lt;b&gt;CHARGE(E) D’ETUDES STATISTIQUES / DATA ANALYST-(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale&#34;,&#34;&lt;b&gt;Consultant.e Data Scientist&lt;\/b&gt; &lt;br/&gt; Softeam&#34;,&#34;&lt;b&gt;Data Analyst - (F/H)&lt;\/b&gt; &lt;br/&gt; Avisto&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; TAXIS G7&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; GESER BEST&#34;,&#34;&lt;b&gt;CDD DATA ANALYST H/F&lt;\/b&gt; &lt;br/&gt; Caisse des Dépôts&#34;,&#34;&lt;b&gt;Data Scientist Climat&lt;\/b&gt; &lt;br/&gt; Generali France&#34;,&#34;&lt;b&gt;Junior data analyst H/F&lt;\/b&gt; &lt;br/&gt; Lagardere&#34;,&#34;&lt;b&gt;Junior data analyst H/F&lt;\/b&gt; &lt;br/&gt; Lagardere&#34;,&#34;&lt;b&gt;Data Scientist - COPERNEEC&lt;\/b&gt; &lt;br/&gt; Coperneec&#34;,&#34;&lt;b&gt;Data Analyst (F/H)&lt;\/b&gt; &lt;br/&gt; Auchan Retail France&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; AKKA TECHNOLOGIES&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Lesaffre&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; REEL&#34;,&#34;&lt;b&gt;Data Analyst (H/F/NB)&lt;\/b&gt; &lt;br/&gt; Synchrone&#34;,&#34;&lt;b&gt;Analyste de données (Data analyst)&lt;\/b&gt; &lt;br/&gt; ONISEP&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Hess Automobile&#34;,&#34;&lt;b&gt;DATA ANALYST - SCDP (F/H)&lt;\/b&gt; &lt;br/&gt; ADEO Services&#34;,&#34;&lt;b&gt;Data Scientist Confirmé&lt;\/b&gt; &lt;br/&gt; IPANEMA CONSULTING&#34;,&#34;&lt;b&gt;Product Analytic - Data Scientist&lt;\/b&gt; &lt;br/&gt; Criteo&#34;,&#34;&lt;b&gt;DATA SCIENTIST H/F&lt;\/b&gt; &lt;br/&gt; CAPGEMINI ENGINEERING&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; YZEE SERVICES&#34;,&#34;&lt;b&gt;CDD - Data Analyst Junior H/F 92230, Gennevilliers, Hauts-de...&lt;\/b&gt; &lt;br/&gt; Audika&#34;,&#34;&lt;b&gt;Accelerator - Junior Data Scientist (M/F)&lt;\/b&gt; &lt;br/&gt; Sanofi&#34;,&#34;&lt;b&gt;Responsable Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; Kent FR&#34;,&#34;&lt;b&gt;data analyst&lt;\/b&gt; &lt;br/&gt; Ausy&#34;,&#34;&lt;b&gt;Data Analyst Positive Impact - H/F&lt;\/b&gt; &lt;br/&gt; ADEO Services&#34;,&#34;&lt;b&gt;Lead Data Scientist&lt;\/b&gt; &lt;br/&gt; NATAN&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Les Nouveaux Héritiers&#34;,&#34;&lt;b&gt;Data Scientist H/F - Lille&lt;\/b&gt; &lt;br/&gt; AVISIA&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; TALENTS RH&#34;,&#34;&lt;b&gt;Innovation / R&amp;D / Data Sciences_Modèle d&#39;offre d&#39;emploi&lt;\/b&gt; &lt;br/&gt; SUEZ&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; TELITEM CONSULTING&#34;,&#34;&lt;b&gt;Data analyst H/F&lt;\/b&gt; &lt;br/&gt; Econocom&#34;,&#34;&lt;b&gt;Junior data analyst H/F&lt;\/b&gt; &lt;br/&gt; Lagardère Travel Retail&#34;,&#34;&lt;b&gt;Apprenticeship Data Scientist F/M/&lt;\/b&gt; &lt;br/&gt; Subsea 7&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Monoprix.fr&#34;,&#34;&lt;b&gt;Data Analyst Senior F/H&lt;\/b&gt; &lt;br/&gt; 24S.com&#34;,&#34;&lt;b&gt;Data Analyst Senior F/H&lt;\/b&gt; &lt;br/&gt; 24S.com&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; ÉQUIPEMENT DE LA MAISON&#34;,&#34;&lt;b&gt;ST I Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Vision Systems&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Schneider Electric&#34;,&#34;&lt;b&gt;DATA SCIENTIST / BIOSTATISTICIAN / BIOSTATISTICS&lt;\/b&gt; &lt;br/&gt; Ariana Pharma&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Sport Faction&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; TCO&#34;,&#34;&lt;b&gt;Ingénieur Pépinière DOP - Data Analyst Hubgrade (F/H)&lt;\/b&gt; &lt;br/&gt; Veolia&#34;,&#34;&lt;b&gt;Data Scientist Confirmé H/F&lt;\/b&gt; &lt;br/&gt; Air France-KLM&#34;,&#34;&lt;b&gt;Data Scientist H/F - CDI&lt;\/b&gt; &lt;br/&gt; Richemont&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Cenisis&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; NATIXIS&#34;,&#34;&lt;b&gt;Data Scientist Senior&lt;\/b&gt; &lt;br/&gt; Dataworks&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Ekino France&#34;,&#34;&lt;b&gt;Data Scientist h-f&lt;\/b&gt; &lt;br/&gt; Danem People France&#34;,&#34;&lt;b&gt;Data Scientist h-f&lt;\/b&gt; &lt;br/&gt; Danem People France&#34;,&#34;&lt;b&gt;ALT - DATA SCIENTIST CLIMAT&lt;\/b&gt; &lt;br/&gt; Generali France&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Eau d&#39;Azur&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; BNP Paribas&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; บริษัท โตโยต้า ลีสซิ่ง (ประเทศไทย) จำกัด&#34;,&#34;&lt;b&gt;DATA SCIENTIST/DATA ANALYST&lt;\/b&gt; &lt;br/&gt; Go Concept&#34;,&#34;&lt;b&gt;DATA SCIENTIST (H/F)&lt;\/b&gt; &lt;br/&gt; mycommunIT&#34;,&#34;&lt;b&gt;Data Scientist / Data Analyst&lt;\/b&gt; &lt;br/&gt; Siderlog&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Epoka for Apside&#34;,&#34;&lt;b&gt;DATA ANALYST F/H&lt;\/b&gt; &lt;br/&gt; GESER BEST&#34;,&#34;&lt;b&gt;Ph.D Data Scientist - Machine Learning et Prévisions de séri...&lt;\/b&gt; &lt;br/&gt; QUANTMETRY&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; AVIV Group&#34;,&#34;&lt;b&gt;Data Scientist - Machine Learning (F/H)&lt;\/b&gt; &lt;br/&gt; Kelkoo LTD&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; TOPORDER&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; ADHERENCE CONSULTING&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; TOPORDER&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; ADHERENCE CONSULTING&#34;,&#34;&lt;b&gt;Data Analyste - Opérations/Ventes&lt;\/b&gt; &lt;br/&gt; EATON&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Interforum&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; MDA Electroménager&#34;,&#34;&lt;b&gt;Ingénieur(e) Data Science, Machine Learning , Deep Learning&lt;\/b&gt; &lt;br/&gt; Data2innov&#34;,&#34;&lt;b&gt;Data Scientist NLP - Lille H/F&lt;\/b&gt; &lt;br/&gt; CleverConnect&#34;,&#34;&lt;b&gt;Data Scientist Senior&lt;\/b&gt; &lt;br/&gt; MyDataModels&#34;,&#34;&lt;b&gt;Data Analyst - Environnement&lt;\/b&gt; &lt;br/&gt; Simpliciti&#34;,&#34;&lt;b&gt;Thèse CIFRE (PhD Thesis) - Data Science / Artificial Intelli...&lt;\/b&gt; &lt;br/&gt; AIRBUS&#34;,&#34;&lt;b&gt;CHARGE(E) DE STATISTIQUES / DATA ANALYST (H/F)&lt;\/b&gt; &lt;br/&gt; CAF DE LOIRE-ATLANTIQUE&#34;,&#34;&lt;b&gt;Data Scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Mydral&#34;,&#34;&lt;b&gt;Senior Data Scientist_&lt;\/b&gt; &lt;br/&gt; Sense4data&#34;,&#34;&lt;b&gt;Business Analyst Data&lt;\/b&gt; &lt;br/&gt; Softeam&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; 8SEC&#34;,&#34;&lt;b&gt;Data Analyst Junior - Paris - 2022 H/F&lt;\/b&gt; &lt;br/&gt; MAZARS&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; IKIGAÏ&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist / Dataviz h/f&lt;\/b&gt; &lt;br/&gt; Legallais&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Pernod Ricard&#34;,&#34;&lt;b&gt;CDD - Data Analyst Junior H/F&lt;\/b&gt; &lt;br/&gt; Audika Groupe&#34;,&#34;&lt;b&gt;Data Analyst / Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Actirise&#34;,&#34;&lt;b&gt;ESG Data Analyst-(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale&#34;,&#34;&lt;b&gt;DATA ANALYST F/H&lt;\/b&gt; &lt;br/&gt; AQUANTIS&#34;,&#34;&lt;b&gt;DATA ANALYST H/F&lt;\/b&gt; &lt;br/&gt; Segula Technologies&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; NJ PARTNERS&#34;,&#34;&lt;b&gt;CDD - Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; La Banque Postale&#34;,&#34;&lt;b&gt;Data Engineer / Data Scientist&lt;\/b&gt; &lt;br/&gt; Quotatis Groupe&#34;,&#34;&lt;b&gt;DATA ANALYST – CDI (H/F)&lt;\/b&gt; &lt;br/&gt; IRI&#34;,&#34;&lt;b&gt;Data Analyst Fraude&lt;\/b&gt; &lt;br/&gt; Adevinta Group&#34;,&#34;&lt;b&gt;DATA SCIENTIST H/F&lt;\/b&gt; &lt;br/&gt; ÏDKIDS GROUP&#34;,&#34;&lt;b&gt;CHARGE(E) DE STATISTIQUES / DATA ANALYST (H/F)&lt;\/b&gt; &lt;br/&gt; CAF DE LOIRE-ATLANTIQUE&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Market Espace&#34;,&#34;&lt;b&gt;Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; HP&#34;,&#34;&lt;b&gt;Datascientist / Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; LCL&#34;,&#34;&lt;b&gt;Data Analyste - Intelligence Economique (H/F)&lt;\/b&gt; &lt;br/&gt; Eowin&#34;,&#34;&lt;b&gt;DATA ANALYST BATIMENT&lt;\/b&gt; &lt;br/&gt; CSTB&#34;,&#34;&lt;b&gt;Data Scientist Senior&lt;\/b&gt; &lt;br/&gt; MyDataModels&#34;,&#34;&lt;b&gt;Data analyst F/H&lt;\/b&gt; &lt;br/&gt; YUMENS&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; Fnac Darty&#34;,&#34;&lt;b&gt;DATA MINER F/H&lt;\/b&gt; &lt;br/&gt; PRO DIRECT SERVICES&#34;,&#34;&lt;b&gt;Lead Data Scientist&lt;\/b&gt; &lt;br/&gt; ILLUIN TECHNOLOGY&#34;,&#34;&lt;b&gt;Ph.D Data Scientist - Machine Learning et Prévisions de séri...&lt;\/b&gt; &lt;br/&gt; QUANTMETRY&#34;,&#34;&lt;b&gt;VIE - CANAL+ SUISSE - Data Analyst - F/H&lt;\/b&gt; &lt;br/&gt; Canal Plus&#34;,&#34;&lt;b&gt;Data Scientist III&lt;\/b&gt; &lt;br/&gt; American Express Global Business Travel&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Sii&#34;,&#34;&lt;b&gt;Data analyste| PwC Montpellier | CDI/CDD | H/F&lt;\/b&gt; &lt;br/&gt; PwC&#34;,&#34;&lt;b&gt;Data Analyst – Nord – Lille F/H&lt;\/b&gt; &lt;br/&gt; OTTEO&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; Groupe IGS&#34;,&#34;&lt;b&gt;Data Scientist Position&lt;\/b&gt; &lt;br/&gt; PARIS-SACLAY CENTER FOR DATA SCIENCE&#34;,&#34;&lt;b&gt;Data Analyst Marketing Stratégique - Boursorama-(H/F)&lt;\/b&gt; &lt;br/&gt; Boursorama&#34;,&#34;&lt;b&gt;Data Analyst / Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Actirise&#34;,&#34;&lt;b&gt;Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; Pernod Ricard&#34;,&#34;&lt;b&gt;Data Analyst - DP4P (H/F)&lt;\/b&gt; &lt;br/&gt; ADEO Services&#34;,&#34;&lt;b&gt;Data Analyst Programmatic&lt;\/b&gt; &lt;br/&gt; Numberly&#34;,&#34;&lt;b&gt;Data Analyst - Environnement&lt;\/b&gt; &lt;br/&gt; Groupe Berto&#34;,&#34;&lt;b&gt;DATA ANALYST – CDI (H/F)&lt;\/b&gt; &lt;br/&gt; IRI&#34;,&#34;&lt;b&gt;Data Scientist Senior&lt;\/b&gt; &lt;br/&gt; Plastic Omnium&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; WEB TRANSITION&#34;,&#34;&lt;b&gt;Finance Data Analyst Flying Blue H/F&lt;\/b&gt; &lt;br/&gt; Air France-KLM&#34;,&#34;&lt;b&gt;CDD - Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; La Banque Postale&#34;,&#34;&lt;b&gt;CDD - Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; La Banque Postale&#34;,&#34;&lt;b&gt;Data Analyst Transport H/F&lt;\/b&gt; &lt;br/&gt; Transdev&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; AVIV Group&#34;,&#34;&lt;b&gt;Data Analyst Marketing F/H&lt;\/b&gt; &lt;br/&gt; Avanci&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; G7&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; umlaut&#34;,&#34;&lt;b&gt;CDD - 6 MOIS - DATA ANALYSTE&lt;\/b&gt; &lt;br/&gt; Caisse des Dépôts&#34;,&#34;&lt;b&gt;Consultant(e) BI / Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Decivision&#34;,&#34;&lt;b&gt;data analyst - reglementations bancaires+data F/H&lt;\/b&gt; &lt;br/&gt; GROUPE AYDON&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; KANTAR&#34;,&#34;&lt;b&gt;BUSINESS ANALYST DATA H/F&lt;\/b&gt; &lt;br/&gt; Caisse des Dépôts&#34;,&#34;&lt;b&gt;Vertica Data Scientist EMEA&lt;\/b&gt; &lt;br/&gt; Micro Focus&#34;,&#34;&lt;b&gt;Industrial Engineering Data Analyst&lt;\/b&gt; &lt;br/&gt; MARS&#34;,&#34;&lt;b&gt;CDD 12 MOIS - BUSINESS ET DATA ANALYST H/F H/F&lt;\/b&gt; &lt;br/&gt; CARGLASS&#34;,&#34;&lt;b&gt;Data scientist H/F - Innovation numérique&lt;\/b&gt; &lt;br/&gt; Polyconseil&#34;,&#34;&lt;b&gt;Industrial Engineering Data Analyst&lt;\/b&gt; &lt;br/&gt; MARS&#34;,&#34;&lt;b&gt;CDD 12 MOIS - BUSINESS ET DATA ANALYST H/F H/F&lt;\/b&gt; &lt;br/&gt; CARGLASS&#34;,&#34;&lt;b&gt;Data scientist H/F - Innovation numérique&lt;\/b&gt; &lt;br/&gt; Polyconseil&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; MASSILLY HOLDING&#34;,&#34;&lt;b&gt;Data Analyst | H/F&lt;\/b&gt; &lt;br/&gt; PwC&#34;,&#34;&lt;b&gt;DATA SCIENTIST H/F&lt;\/b&gt; &lt;br/&gt; ÏDKIDS GROUP&#34;,&#34;&lt;b&gt;Data Scientist - Développeur Chatbot Confirmé(e) - H/F&lt;\/b&gt; &lt;br/&gt; Talan Opérations&#34;,&#34;&lt;b&gt;DATA ANALYST F/H&lt;\/b&gt; &lt;br/&gt; ALLOPNEUS.COM&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Inetum&#34;,&#34;&lt;b&gt;Analyste Business Data-(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale&#34;,&#34;&lt;b&gt;Data Analyst Océanographe – Observation spatiale (F/H)&lt;\/b&gt; &lt;br/&gt; ALTEN&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Fideliz&#34;,&#34;&lt;b&gt;Data Analyst Senior H/F&lt;\/b&gt; &lt;br/&gt; CGI Inc&#34;,&#34;&lt;b&gt;Data scientist (H/F)&lt;\/b&gt; &lt;br/&gt; Opéra Conseil&#34;,&#34;&lt;b&gt;Manager Data Analytics| CDI | H/F&lt;\/b&gt; &lt;br/&gt; PwC&#34;,&#34;&lt;b&gt;Manager Data Analytics| CDI | H/F&lt;\/b&gt; &lt;br/&gt; PwC&#34;,&#34;&lt;b&gt;Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; AG SOLUTION&#34;,&#34;&lt;b&gt;Data Engineer / Data Scientist&lt;\/b&gt; &lt;br/&gt; Quotatis Groupe&#34;,&#34;&lt;b&gt;Data Analyst | Luxe (H/F) Paris&lt;\/b&gt; &lt;br/&gt; CENOVA&#34;,&#34;&lt;b&gt;Data Analyst Achats H/F&lt;\/b&gt; &lt;br/&gt; Spie Batignolles&#34;,&#34;&lt;b&gt;Analyste (data) SIRH H/F&lt;\/b&gt; &lt;br/&gt; Mercer&#34;,&#34;&lt;b&gt;Innovation / R&amp;D / Data Sciences_Modèle d&#39;offre d&#39;emploi&lt;\/b&gt; &lt;br/&gt; SUEZ&#34;,&#34;&lt;b&gt;Data Analyst H/NB/F&lt;\/b&gt; &lt;br/&gt; Ubisoft&#34;,&#34;&lt;b&gt;Data Business Analyst* H/F&lt;\/b&gt; &lt;br/&gt; Socomec&#34;,&#34;&lt;b&gt;Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; PELLENC ST&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Crédit Mutuel Arkea&#34;,&#34;&lt;b&gt;Data Analyst - Power BI/Tableau F/H&lt;\/b&gt; &lt;br/&gt; MINSART-KREMER&#34;,&#34;&lt;b&gt;LEAD DATA SCIENTIST - F/H&lt;\/b&gt; &lt;br/&gt; ADEO Services&#34;,&#34;&lt;b&gt;Data Fraud Analyst&lt;\/b&gt; &lt;br/&gt; OVHcloud&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; Crédit Agricole Assurances&#34;,&#34;&lt;b&gt;Data Analyst Marketing&lt;\/b&gt; &lt;br/&gt; Axys Consultants&#34;,&#34;&lt;b&gt;Manager Data Analytics (H/F)&lt;\/b&gt; &lt;br/&gt; Mydral&#34;,&#34;&lt;b&gt;Ingénieur(e) Data Science, Machine Learning , Deep Learning&lt;\/b&gt; &lt;br/&gt; Data2innov&#34;,&#34;&lt;b&gt;Responsable Data Science H/F&lt;\/b&gt; &lt;br/&gt; Crédit Agricole d&#39;Ile-de-France&#34;,&#34;&lt;b&gt;Consultant Data Analytics H/F&lt;\/b&gt; &lt;br/&gt; Group onePoint&#34;,&#34;&lt;b&gt;Analyste DATA H/F&lt;\/b&gt; &lt;br/&gt; Leasecom&#34;,&#34;&lt;b&gt;Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; LESAFFRE GROUP&#34;,&#34;&lt;b&gt;JUNIOR DATA SCIENTIST FINANCE/ QUANTITATIVE ANALYST (Paris)...&lt;\/b&gt; &lt;br/&gt; Swiss Life Asset managers&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Legrand Support&#34;,&#34;&lt;b&gt;Data Scientist Senior (H/F)&lt;\/b&gt; &lt;br/&gt; Equancy&#34;,&#34;&lt;b&gt;Head of Data science&lt;\/b&gt; &lt;br/&gt; Data Recrutement&#34;,&#34;&lt;b&gt;Marketing Performance Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Amplifon&#34;,&#34;&lt;b&gt;Consultant(e) Data Analyst&lt;\/b&gt; &lt;br/&gt; SOCIO DATA MANAGEMENT&#34;,&#34;&lt;b&gt;Data Analyst Risk F/H&lt;\/b&gt; &lt;br/&gt; Groupe BPCE&#34;,&#34;&lt;b&gt;P&amp;S EAME Data Scientist – Modelling and Analytics (M/W)&lt;\/b&gt; &lt;br/&gt; Syngenta&#34;,&#34;&lt;b&gt;Data Analyste (F/H) - CDD Angers&lt;\/b&gt; &lt;br/&gt; AXA&#34;,&#34;&lt;b&gt;DATA ANALYST H/F&lt;\/b&gt; &lt;br/&gt; Segula Technologies&#34;,&#34;&lt;b&gt;Chargé d&#39;études statistiques / Data analyst / Data scientist...&lt;\/b&gt; &lt;br/&gt; Unédic&#34;,&#34;&lt;b&gt;Senior Data Scientist NLP&lt;\/b&gt; &lt;br/&gt; Data Recrutement&#34;,&#34;&lt;b&gt;Responsable Data Analytics F/H&lt;\/b&gt; &lt;br/&gt; La Banque Postale&#34;,&#34;&lt;b&gt;Responsable Data &amp; Analytics&lt;\/b&gt; &lt;br/&gt; Harnham&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; NJ PARTNERS&#34;,&#34;&lt;b&gt;Traffic Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Groupe ATOLL&#34;,&#34;&lt;b&gt;Consultant(e)s Débutant(e)s en Data &amp; Analytics et Innovatio...&lt;\/b&gt; &lt;br/&gt; EY&#34;,&#34;&lt;b&gt;Data Analyst Propulsion System&lt;\/b&gt; &lt;br/&gt; Airbus&#34;,&#34;&lt;b&gt;Consultant Data &amp; Digital – Analyste&lt;\/b&gt; &lt;br/&gt; Eight Advisory&#34;,&#34;&lt;b&gt;Data Scientist/Data Engineer&lt;\/b&gt; &lt;br/&gt; Capital Management Fund&#34;,&#34;&lt;b&gt;Data Analyst CA en contrat d&#39;apprentissage&lt;\/b&gt; &lt;br/&gt; Orange France&#34;,&#34;&lt;b&gt;Data Analyst – H/F&lt;\/b&gt; &lt;br/&gt; MALHERBE Paris&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Imprimerie Nationale&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; alteca&#34;,&#34;&lt;b&gt;CDI - Data Analyst Outremer (F/H)&lt;\/b&gt; &lt;br/&gt; Canal Plus&#34;,&#34;&lt;b&gt;Data Scientist Sénior h-f&lt;\/b&gt; &lt;br/&gt; Danem People France&#34;,&#34;&lt;b&gt;Data Analyst -FRANFINANCE-(H/F)&lt;\/b&gt; &lt;br/&gt; Franfinance&#34;,&#34;&lt;b&gt;Performance Data Analyst - H/F&lt;\/b&gt; &lt;br/&gt; BNP Paribas&#34;,&#34;&lt;b&gt;Data/Business Analyst - Paris (H/F)&lt;\/b&gt; &lt;br/&gt; Scient&#34;,&#34;&lt;b&gt;Data Business Analyst&lt;\/b&gt; &lt;br/&gt; Oncrawl&#34;,&#34;&lt;b&gt;Data analyst H/F&lt;\/b&gt; &lt;br/&gt; L&#39;atelier des Chefs&#34;,&#34;&lt;b&gt;WM - Data Analyste Epargne Financière, F/H&lt;\/b&gt; &lt;br/&gt; BNP Paribas&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Acii&#34;,&#34;&lt;b&gt;Data Engineer/ Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; PSAPeugeotCitroen&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; 1G LINK&#34;,&#34;&lt;b&gt;Data &amp; Analytics – Consultant Confirmé&lt;\/b&gt; &lt;br/&gt; Adone Conseil&#34;,&#34;&lt;b&gt;CDI - Data Analyst Afrique (F/H)&lt;\/b&gt; &lt;br/&gt; Canal Plus&#34;,&#34;&lt;b&gt;Junior marketing data analyst&lt;\/b&gt; &lt;br/&gt; Richemont&#34;,&#34;&lt;b&gt;Chef de Projets Data &amp; Analytics (F/H)&lt;\/b&gt; &lt;br/&gt; Micropole&#34;,&#34;&lt;b&gt;Head of Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Volta Medical&#34;,&#34;&lt;b&gt;Data Scientist F/H&lt;\/b&gt; &lt;br/&gt; Web Transition&#34;,&#34;&lt;b&gt;DATA DIGITAL ANALYST (H/F)&lt;\/b&gt; &lt;br/&gt; OUI.sncf&#34;,&#34;&lt;b&gt;Data Analyst - CDI F/H&lt;\/b&gt; &lt;br/&gt; FAB GROUP&#34;,&#34;&lt;b&gt;Data Analyst - Reply France (h/f)&lt;\/b&gt; &lt;br/&gt; Reply&#34;,&#34;&lt;b&gt;Senior Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Capgemini Invent&#34;,&#34;&lt;b&gt;Data/Business Analyst - Paris (H/F)&lt;\/b&gt; &lt;br/&gt; Scient&#34;,&#34;&lt;b&gt;DATA ANALYST H/F&lt;\/b&gt; &lt;br/&gt; Klanik&#34;,&#34;&lt;b&gt;Traffic Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Groupe ATOLL&#34;,&#34;&lt;b&gt;Data Analyst / Marketing&lt;\/b&gt; &lt;br/&gt; Ets Chambon et Fils&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; NHOOD&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Acii&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Club Med&#34;,&#34;&lt;b&gt;Data Analyst PowerBI ( H/F )&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;Data analyst SQL datastudio (H/F)&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;Data Analyst (H/F) - CDI&lt;\/b&gt; &lt;br/&gt; MSD&#34;,&#34;&lt;b&gt;DATA ANALYST FORCE DE VENTE (H/F)&lt;\/b&gt; &lt;br/&gt; Atmosphères&#34;,&#34;&lt;b&gt;Data Scientist ML/DL &amp; DataViz H/F&lt;\/b&gt; &lt;br/&gt; Smily RH&#34;,&#34;&lt;b&gt;Data Analyst -FRANFINANCE-(H/F)&lt;\/b&gt; &lt;br/&gt; Franfinance&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Vicat&#34;,&#34;&lt;b&gt;Data Analyst - Power BI/Tableau F/H&lt;\/b&gt; &lt;br/&gt; MINSART-KREMER&#34;,&#34;&lt;b&gt;WM - Data Analyste Epargne Financière, F/H&lt;\/b&gt; &lt;br/&gt; BNP Paribas&#34;,&#34;&lt;b&gt;CDI - Data Analyst Afrique (F/H)&lt;\/b&gt; &lt;br/&gt; Canal Plus&#34;,&#34;&lt;b&gt;Data Scientist Senior F/H&lt;\/b&gt; &lt;br/&gt; NICKEL&#34;,&#34;&lt;b&gt;Ingénieur(e) Data Science, Machine Learning , Deep Learning&lt;\/b&gt; &lt;br/&gt; Data2innov&#34;,&#34;&lt;b&gt;Data Analyst – F/H&lt;\/b&gt; &lt;br/&gt; Atecna&#34;,&#34;&lt;b&gt;Data &amp; Analytics – Consultant Confirmé&lt;\/b&gt; &lt;br/&gt; Adone Conseil&#34;,&#34;&lt;b&gt;Analyste Développeur orienté DATA-(H/F)&lt;\/b&gt; &lt;br/&gt; Société Générale Securities Services&#34;,&#34;&lt;b&gt;Data Scientist Engineering Manager&lt;\/b&gt; &lt;br/&gt; Kpler&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Crédit Agricole Assurances&#34;,&#34;&lt;b&gt;Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; Ekkiden&#34;,&#34;&lt;b&gt;JUNIOR DATA SCIENTIST FINANCE/ QUANTITATIVE ANALYST (Paris)...&lt;\/b&gt; &lt;br/&gt; Swiss Life Asset managers&#34;,&#34;&lt;b&gt;Data Analyst Senior&lt;\/b&gt; &lt;br/&gt; Adevinta Group&#34;,&#34;&lt;b&gt;Data analyst Talend H/F&lt;\/b&gt; &lt;br/&gt; YA HUNTING&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Inetum Capital Market&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; POTAIN Manitowoc&#34;,&#34;&lt;b&gt;Performance Data Analyst - H/F&lt;\/b&gt; &lt;br/&gt; BNP Paribas&#34;,&#34;&lt;b&gt;Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; alteca&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; LES MOUSQUETAIRES&#34;,&#34;&lt;b&gt;Data Engineer/ Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; PSAPeugeotCitroen&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; NJ PARTNERS&#34;,&#34;&lt;b&gt;DATAMINER SENIOR (H/F)&lt;\/b&gt; &lt;br/&gt; OUI.sncf&#34;,&#34;&lt;b&gt;Data Analyst&lt;\/b&gt; &lt;br/&gt; Quick&#34;,&#34;&lt;b&gt;Data analyste&lt;\/b&gt; &lt;br/&gt; KANTAR&#34;,&#34;&lt;b&gt;Senior data scientist AIX EN PROVENCE H/F&lt;\/b&gt; &lt;br/&gt; Capgemini&#34;,&#34;&lt;b&gt;Data Scientist // Full remote // Fluent English // Ecologie&lt;\/b&gt; &lt;br/&gt; Sept Lieues&#34;,&#34;&lt;b&gt;Data Analyst / Chargé d&#39;études H/F - Siège&lt;\/b&gt; &lt;br/&gt; TINGARI&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Betclic Group&#34;,&#34;&lt;b&gt;BI/DATA ANALYST (H/F)&lt;\/b&gt; &lt;br/&gt; Groupe Beaumanoir&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; NEO2&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; HN SERVICES - HN FORMATION - HN RECRUTEMENT -...&#34;,&#34;&lt;b&gt;Business Analyst Data H/F&lt;\/b&gt; &lt;br/&gt; Group onePoint&#34;,&#34;&lt;b&gt;Data Scientist F/M&lt;\/b&gt; &lt;br/&gt; Schneider Electric&#34;,&#34;&lt;b&gt;Game Data Analyst (F/H/NB)&lt;\/b&gt; &lt;br/&gt; Focus Entertainment&#34;,&#34;&lt;b&gt;Consultant.e Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; June Partners&#34;,&#34;&lt;b&gt;Data Analyst H/F - CDI&lt;\/b&gt; &lt;br/&gt; Viatris&#34;,&#34;&lt;b&gt;Web et Data Analyst&lt;\/b&gt; &lt;br/&gt; Groupe Atlantic&#34;,&#34;&lt;b&gt;Data analyst&lt;\/b&gt; &lt;br/&gt; PIERRE FABRE S.A.&#34;,&#34;&lt;b&gt;Consultant Data Analytics| CDI| H/F&lt;\/b&gt; &lt;br/&gt; PwC&#34;,&#34;&lt;b&gt;Data Scientist_PhD Level (M/F)&lt;\/b&gt; &lt;br/&gt; Evotec&#34;,&#34;&lt;b&gt;Consultant Data expérimenté : Data Scientist / Data Engineer...&lt;\/b&gt; &lt;br/&gt; IBM interactive&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;Data Analyst Senior (H/F)&lt;\/b&gt; &lt;br/&gt; RSM France&#34;,&#34;&lt;b&gt;Data Scientist - PhD Graduates&lt;\/b&gt; &lt;br/&gt; SmartAdServer&#34;,&#34;&lt;b&gt;Data scientist H/F&lt;\/b&gt; &lt;br/&gt; HAYS&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Opéra Conseil&#34;,&#34;&lt;b&gt;Consultant Business Intelligence (BI) / Data Analytics - Neu...&lt;\/b&gt; &lt;br/&gt; Grant Thornton France&#34;,&#34;&lt;b&gt;Lead Data Scientist / Freelance&lt;\/b&gt; &lt;br/&gt; CELAD&#34;,&#34;&lt;b&gt;Analyste DATA H/F&lt;\/b&gt; &lt;br/&gt; CGI Inc&#34;,&#34;&lt;b&gt;Docteur/PhD spécialisé en Data Science F/H&lt;\/b&gt; &lt;br/&gt; Scalian&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; NOVELIS&#34;,&#34;&lt;b&gt;Product Data Analyst&lt;\/b&gt; &lt;br/&gt; Pictarine&#34;,&#34;&lt;b&gt;DATA ANALYST POWER BI ASAP&lt;\/b&gt; &lt;br/&gt; Groupe Trèfle&#34;,&#34;&lt;b&gt;Data Analyst - Niort H(/F)&lt;\/b&gt; &lt;br/&gt; Scient&#34;,&#34;&lt;b&gt;Data Scientist/Chatbot-Niort F/H&lt;\/b&gt; &lt;br/&gt; SIDERLOG CONSEIL&#34;,&#34;&lt;b&gt;Analyste Data - Référent H/F&lt;\/b&gt; &lt;br/&gt; Saur&#34;,&#34;&lt;b&gt;Data Analyst - CLIENT FINAL F/H&lt;\/b&gt; &lt;br/&gt; My Talent Expert&#34;,&#34;&lt;b&gt;Data Scientist F/H/X&lt;\/b&gt; &lt;br/&gt; EXTERNATIC&#34;,&#34;&lt;b&gt;Data Analyst Senior H/F&lt;\/b&gt; &lt;br/&gt; DataKhi&#34;,&#34;&lt;b&gt;Margo Analytics - Data Engineer – H/F&lt;\/b&gt; &lt;br/&gt; Margo Conseil&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Content Square&#34;,&#34;&lt;b&gt;Manager Data Analytics F/M&lt;\/b&gt; &lt;br/&gt; Betclic Group&#34;,&#34;&lt;b&gt;Data Scientist - Logiciels B-to-B / B-to-C en temps réel - 9...&lt;\/b&gt; &lt;br/&gt; Sept Lieues&#34;,&#34;&lt;b&gt;Actuaire IARD DATA ANALYTICS (F/H)&lt;\/b&gt; &lt;br/&gt; AXA&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; FAB Group&#34;,&#34;&lt;b&gt;Data Analyst &amp; Steward (f/m/d)&lt;\/b&gt; &lt;br/&gt; Allianz Global Investors&#34;,&#34;&lt;b&gt;Consultant / Analyste Business Data (H/F)&lt;\/b&gt; &lt;br/&gt; CGI Inc&#34;,&#34;&lt;b&gt;Consultant(e) Data Analyst – Confirmé(e)&lt;\/b&gt; &lt;br/&gt; SOCIO DATA MANAGEMENT&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; HOREA CONSEIL&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; HOREA CONSEIL&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Presans&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; AVISIA&#34;,&#34;&lt;b&gt;Data Analyst Consultant (H/F)&lt;\/b&gt; &lt;br/&gt; ENDRIX&#34;,&#34;&lt;b&gt;Business Data Analyst CDI / Permanent Job Paris / France&lt;\/b&gt; &lt;br/&gt; Teemo&#34;,&#34;&lt;b&gt;Data Analyst - Industry Team&lt;\/b&gt; &lt;br/&gt; Deezer&#34;,&#34;&lt;b&gt;Data Science Manager - Instagram Creator Relevance&lt;\/b&gt; &lt;br/&gt; Instagram&#34;,&#34;&lt;b&gt;ANALYTICS DATA ARCHITECT&lt;\/b&gt; &lt;br/&gt; EasyNeo&#34;,&#34;&lt;b&gt;Data Scientist expert en IA Conversationnelle F/H&lt;\/b&gt; &lt;br/&gt; Orange Business Services&#34;,&#34;&lt;b&gt;Data Scientist – Machine Deep learning (H/F)&lt;\/b&gt; &lt;br/&gt; DAVIDSON&#34;,&#34;&lt;b&gt;CDI - Product Owner Data Analytics (F/H)&lt;\/b&gt; &lt;br/&gt; Sephora&#34;,&#34;&lt;b&gt;Data Science &amp; Artificial Intelligence Research Scientist M/...&lt;\/b&gt; &lt;br/&gt; TotalEnergies&#34;,&#34;&lt;b&gt;Consultant Data Scientist confirmé - (H/F)&lt;\/b&gt; &lt;br/&gt; Groupe HLI&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist Expérimenté (H/F)&lt;\/b&gt; &lt;br/&gt; Thales&#34;,&#34;&lt;b&gt;Data Scientist Expérimenté H/F&lt;\/b&gt; &lt;br/&gt; EY&#34;,&#34;&lt;b&gt;Actuaire/Data Scientist - H/F&lt;\/b&gt; &lt;br/&gt; AXA&#34;,&#34;&lt;b&gt;Data Scientist (Python and SQL) - Freelance&lt;\/b&gt; &lt;br/&gt; Veepee&#34;,&#34;&lt;b&gt;Data Analyst Industrie 4.0 H/F&lt;\/b&gt; &lt;br/&gt; Inetum&#34;,&#34;&lt;b&gt;Web Data Scientist Confirmé H/F&lt;\/b&gt; &lt;br/&gt; Raja France&#34;,&#34;&lt;b&gt;Analyste Data et Business H/F&lt;\/b&gt; &lt;br/&gt; TotalEnergies&#34;,&#34;&lt;b&gt;Analyste métier confirmé - Data Steward H/F&lt;\/b&gt; &lt;br/&gt; Enedis&#34;,&#34;&lt;b&gt;Contrôleur de gestion / Data analyst H/F - Villepinte&lt;\/b&gt; &lt;br/&gt; Petit Forestier&#34;,&#34;&lt;b&gt;Analyste Data BCBS 239 F/H&lt;\/b&gt; &lt;br/&gt; NATIXIS&#34;,&#34;&lt;b&gt;Test &amp; Data Analytics Engineer - Silicon Photonics (ID 2207)&lt;\/b&gt; &lt;br/&gt; Ligentec SA&#34;,&#34;&lt;b&gt;Data Scientist Sénior H/F&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;DATA ANALYST&lt;\/b&gt; &lt;br/&gt; Groupe Europa&#34;,&#34;&lt;b&gt;ANALYSTE DATA BI (H/F)&lt;\/b&gt; &lt;br/&gt; MISTER AUTO&#34;,&#34;&lt;b&gt;Consultant data analytics H/F&lt;\/b&gt; &lt;br/&gt; Leihia&#34;,&#34;&lt;b&gt;DATA ANALYST Domaine Assurances, Mutuelles H/F&lt;\/b&gt; &lt;br/&gt; MGEN&#34;,&#34;&lt;b&gt;Data Scientist confirmé(e) H/F&lt;\/b&gt; &lt;br/&gt; TotalEnergies&#34;,&#34;&lt;b&gt;Ingénieur Data Scientist Expérimenté (H/F)&lt;\/b&gt; &lt;br/&gt; Thales&#34;,&#34;&lt;b&gt;Analyste métier confirmé - Data Steward H/F&lt;\/b&gt; &lt;br/&gt; Enedis&#34;,&#34;&lt;b&gt;Lead - Data Analytics&lt;\/b&gt; &lt;br/&gt; Everise&#34;,&#34;&lt;b&gt;Data Scientist&lt;\/b&gt; &lt;br/&gt; Data Recrutement&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; NOVELIS&#34;,&#34;&lt;b&gt;Data Analyst / Quality Manager (H/F)&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; AVISIA&#34;,&#34;&lt;b&gt;Data Miner&lt;\/b&gt; &lt;br/&gt; Data Recrutement&#34;,&#34;&lt;b&gt;CDI - Product Owner Data Analytics (F/H)&lt;\/b&gt; &lt;br/&gt; Sephora&#34;,&#34;&lt;b&gt;Senior Data Scientist&lt;\/b&gt; &lt;br/&gt; Attraqt&#34;,&#34;&lt;b&gt;Ingénieur Recherche Opérationnelle - Data Scientist H/F&lt;\/b&gt; &lt;br/&gt; Air France-KLM&#34;,&#34;&lt;b&gt;Business Analyst Data H/F&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;DATA ANALYST - H/F&lt;\/b&gt; &lt;br/&gt; Dalkia&#34;,&#34;&lt;b&gt;Actuaire/Data Scientist - H/F&lt;\/b&gt; &lt;br/&gt; AXA&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; FAB Group&#34;,&#34;&lt;b&gt;Consultant data analytics expérimenté(e)&lt;\/b&gt; &lt;br/&gt; The Information Lab&#34;,&#34;&lt;b&gt;DATA ANALYST POWER BI ASAP&lt;\/b&gt; &lt;br/&gt; Groupe Trèfle&#34;,&#34;&lt;b&gt;Analyste Data et Business H/F&lt;\/b&gt; &lt;br/&gt; TotalEnergies&#34;,&#34;&lt;b&gt;Process Data Scientist&lt;\/b&gt; &lt;br/&gt; AG SOLUTION&#34;,&#34;&lt;b&gt;ANALYSTE DATA BI (H/F)&lt;\/b&gt; &lt;br/&gt; MISTER AUTO&#34;,&#34;&lt;b&gt;DATA ANALYST (H/F/D)&lt;\/b&gt; &lt;br/&gt; Pellenc&#34;,&#34;&lt;b&gt;CONSULTANT DÉBUTANT DATA ANALYST EN FINANCEMENT F/H&lt;\/b&gt; &lt;br/&gt; KPMG&#34;,&#34;&lt;b&gt;Data Scientist – Contrôle Statistique des Systèmes de produc...&lt;\/b&gt; &lt;br/&gt; Renault Group&#34;,&#34;&lt;b&gt;Data Scientist ML/DL &amp; DataViz H/F&lt;\/b&gt; &lt;br/&gt; Smily RH&#34;,&#34;&lt;b&gt;Data Analyst marketing client - EMEA zone&lt;\/b&gt; &lt;br/&gt; Louis Vuitton&#34;,&#34;&lt;b&gt;Sourcing Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Kingfisher&#34;,&#34;&lt;b&gt;Business Analyst Data H/F&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;Data scientist F/H&lt;\/b&gt; &lt;br/&gt; Synergie&#34;,&#34;&lt;b&gt;Data Analyst / Quality Manager (H/F)&lt;\/b&gt; &lt;br/&gt; Insitoo&#34;,&#34;&lt;b&gt;BI/Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; PROMINENT FRANCE&#34;,&#34;&lt;b&gt;Data Analyst (Flex/Remote)&lt;\/b&gt; &lt;br/&gt; Scaleway&#34;,&#34;&lt;b&gt;Confirmed Consultant &amp; Data Scientist (H/F/N)&lt;\/b&gt; &lt;br/&gt; Ekimetrics&#34;,&#34;&lt;b&gt;Data Analyst Consultant H/F à Lyon 7 CDI&lt;\/b&gt; &lt;br/&gt; ENDRIX&#34;,&#34;&lt;b&gt;Data Analyst F/H&lt;\/b&gt; &lt;br/&gt; AVISIA NORD&#34;,&#34;&lt;b&gt;Data Analyst H/F&lt;\/b&gt; &lt;br/&gt; Expectra&#34;,&#34;&lt;b&gt;Data Analyst - Industry Team&lt;\/b&gt; &lt;br/&gt; Deezer&#34;,&#34;&lt;b&gt;Paris Office - Data &amp; Analytics Specialist (m/f/d)&lt;\/b&gt; &lt;br/&gt; L.E.K. Consulting&#34;,&#34;&lt;b&gt;Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; @talentEgal&#34;,&#34;&lt;b&gt;Data Miner&lt;\/b&gt; &lt;br/&gt; Data Recrutement&#34;,&#34;&lt;b&gt;Support Engineering: Product Data Analyst (M/F)&lt;\/b&gt; &lt;br/&gt; Airbus&#34;,&#34;&lt;b&gt;Data analyst merchandising H/F - CDD&lt;\/b&gt; &lt;br/&gt; Brico depôt&#34;,&#34;&lt;b&gt;Junior Data &amp; Content Analyst&lt;\/b&gt; &lt;br/&gt; Metrixx&#34;,&#34;&lt;b&gt;DATA ANALYST Domaine Assurances, Mutuelles H/F&lt;\/b&gt; &lt;br/&gt; MGEN&#34;,&#34;&lt;b&gt;Ingénieur Développement Python &amp; Data Analyst (H/F)&lt;\/b&gt; &lt;br/&gt; Viveris&#34;,&#34;&lt;b&gt;Data Analyst - Supply Chain H/F&lt;\/b&gt; &lt;br/&gt; Viveris&#34;,&#34;&lt;b&gt;Customer Success Data Analyst&lt;\/b&gt; &lt;br/&gt; Sociabble&#34;,&#34;&lt;b&gt;Data Scientist R&amp;D (PhD) | Optimisation stochastique |...&lt;\/b&gt; &lt;br/&gt; Octopus IT&#34;,&#34;&lt;b&gt;Data Scientist - France/Germany/UK/Spain&lt;\/b&gt; &lt;br/&gt; Shift Technology&#34;,&#34;&lt;b&gt;Data Transformation &amp; Analytics engineer (F/H)&lt;\/b&gt; &lt;br/&gt; Saint-Gobain&#34;,&#34;&lt;b&gt;Senior Clinical Data Analyst&lt;\/b&gt; &lt;br/&gt; DentalMonitoring&#34;,&#34;&lt;b&gt;PhD en Data Science F/H&lt;\/b&gt; &lt;br/&gt; Jean-Yves Arrouet&#34;,&#34;&lt;b&gt;Data Scientist confirmé(e)&lt;\/b&gt; &lt;br/&gt; Data Recrutement&#34;,&#34;&lt;b&gt;Lead Data Scientist Senior F/H&lt;\/b&gt; &lt;br/&gt; GroupAgora&#34;,&#34;&lt;b&gt;Data Analyst Exploitation Eolien (H/F)&lt;\/b&gt; &lt;br/&gt; Boralex&#34;,&#34;&lt;b&gt;Product Data Analyst&lt;\/b&gt; &lt;br/&gt; Ledger&#34;,&#34;&lt;b&gt;Data Scientist Confirmé F/H&lt;\/b&gt; &lt;br/&gt; LINCOLN&#34;,&#34;&lt;b&gt;Data Scientist (F/H)&lt;\/b&gt; &lt;br/&gt; Descartes Underwriting&#34;,&#34;&lt;b&gt;Data Science Experts for Finance Transformation (m/f)&lt;\/b&gt; &lt;br/&gt; Airbus&#34;,&#34;&lt;b&gt;Data Analyst, Retail Media&lt;\/b&gt; &lt;br/&gt; Criteo&#34;,&#34;&lt;b&gt;Senior Data Analyst&lt;\/b&gt; &lt;br/&gt; Harnham&#34;],null,{&#34;showCoverageOnHover&#34;:true,&#34;zoomToBoundsOnClick&#34;:true,&#34;spiderfyOnMaxZoom&#34;:true,&#34;removeOutsideVisibleBounds&#34;:true,&#34;spiderLegPolylineOptions&#34;:{&#34;weight&#34;:1.5,&#34;color&#34;:&#34;#222&#34;,&#34;opacity&#34;:0.5},&#34;freezeAtZoom&#34;:false},null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]}],&#34;limits&#34;:{&#34;lat&#34;:[41.3877400000001,50.7228769167483],&#34;lng&#34;:[-4.48657999999995,9.16087000000005]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analyzing-job-descriptions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analyzing job descriptions&lt;/h2&gt;
&lt;p&gt;Nowadays most of the resumes are scanned and interpreted by an applicant tracking system (ATS). To make things simple, this system looks for key words in your resume and assess the match with the job you are applying for. It is therefore important to describe your experiences with specific key words to improve the chances of getting to the next step of the hiring process.&lt;/p&gt;
&lt;p&gt;But what key words should I include in my resume ? Let’s answer this question by analyzing the job descriptions of data scientist jobs.&lt;/p&gt;
&lt;div id=&#34;downloading-and-cleaning-each-job-description&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Downloading and cleaning each job description&lt;/h3&gt;
&lt;p&gt;First we download the full description of each job by navigating through all the URL listed in our table. We then clean and homogenize the description with a custom function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Loop through all the URLs
job_descriptions &amp;lt;- list()
pb &amp;lt;- txtProgressBar(min = 1, max = length(final_df$url), style = 3)
for(i in 1:length(final_df$url)){
  remDr$navigate(final_df$url[i])
  web_page &amp;lt;- remDr$getPageSource(header = TRUE)[[1]] %&amp;gt;% read_html()
  job_descriptions[[i]] &amp;lt;- web_page %&amp;gt;%
        html_elements(css = &amp;quot;.jobsearch-JobComponent-description&amp;quot;) %&amp;gt;%
      html_text2()
  Sys.sleep(2)
  setTxtProgressBar(pb, i)
}
# Gathering in dataframe
job_descriptions &amp;lt;- as.data.frame(do.call(&amp;quot;rbind&amp;quot;, job_descriptions))
names(job_descriptions) &amp;lt;- c(&amp;quot;Description&amp;quot;)

# Binding to same table:
final_df &amp;lt;- cbind(final_df, job_descriptions)

# Homogenize with custom function
final_df$Description_c &amp;lt;- lapply(final_df$Description, function(x){clean_job_desc(x)[[2]]})
final_df$Language &amp;lt;- textcat::textcat(final_df$Description)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation-procedure-with-udpipe-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Annotation procedure with udpipe Package&lt;/h3&gt;
&lt;p&gt;This part is inspired from this &lt;a href=&#34;https://www.r-bloggers.com/2018/04/an-overview-of-keyword-extraction-techniques/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that the descriptions of all the listed jobs are imported and pre-cleaned, we can annotate the textual data with &lt;strong&gt;udpipe&lt;/strong&gt; package. This package contains functions and models which can perform tokenisation, lemmatisation and key word extraction.&lt;/p&gt;
&lt;p&gt;We first restrict this analysis to data scientist job post written in french, then we annotate all the descriptions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Restricting the analysis to Data scientist post written in french
desc_data_scientist &amp;lt;- final_df %&amp;gt;%
  filter((Job_title_c == &amp;quot;data scientist&amp;quot;) &amp;amp; (Language == &amp;quot;french&amp;quot;)) %&amp;gt;%
  select(Description_c)

ud_model &amp;lt;- udpipe_download_model(language = &amp;quot;french&amp;quot;) # Download the model if necessary
ud_model &amp;lt;- udpipe_load_model(ud_model$file_model) 

# Annotate the descriptions 
x &amp;lt;- udpipe_annotate(ud_model, x = paste(desc_data_scientist, collapse = &amp;quot; &amp;quot;))
x &amp;lt;- as.data.frame(x)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;most-common-nouns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most common nouns&lt;/h3&gt;
&lt;p&gt;We can visualize the most employed word throughout the data scientist job posts written in french:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats &amp;lt;- subset(x, upos %in% &amp;quot;NOUN&amp;quot;)
stats &amp;lt;- txt_freq(x = stats$lemma)

stats %&amp;gt;%
  top_n(50, freq) %&amp;gt;%
  mutate(key = as.factor(key),
         key = fct_reorder(key, freq)) %&amp;gt;%
  ggplot(aes(x = key, y = freq)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  coord_flip() + 
  ylab(&amp;quot;Most common nouns&amp;quot;) + 
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-09-21-web-scraping-indeed-with-r_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even though, it gives us an idea of words to include it is not very informative as key words are often composed by two or more words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extracting-key-words-for-resume-writing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extracting key words for resume writing&lt;/h3&gt;
&lt;p&gt;There are several methods implemented in &lt;strong&gt;udpipe&lt;/strong&gt; to extract key words from a text. After testing several methods, I selected the Rapid Automatic Keyword Extraction (RAKE) which gives me the best results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats &amp;lt;- keywords_rake(x = x,
                       term = &amp;quot;token&amp;quot;,# Search on token
                       group = c(&amp;quot;doc_id&amp;quot;, &amp;quot;sentence_id&amp;quot;), # On every post 
                       relevant = x$upos %in% c(&amp;quot;NOUN&amp;quot;, &amp;quot;ADJ&amp;quot;),  # Only among noun and adj.
                       ngram_max = 2, n_min = 2, sep = &amp;quot; &amp;quot;)

stats &amp;lt;- subset(stats, stats$freq &amp;gt;= 5 &amp;amp; stats$rake &amp;gt; 3)

stats %&amp;gt;% 
  arrange(desc(rake)) %&amp;gt;% 
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     keyword ngram freq     rake
## 1 intelligence artificielle     2    9 9.368889
## 2             tableaux bord     2    5 8.504274
## 3      formation supérieure     2    5 8.374725
## 4        modèles prédictifs     2   15 7.581294
## 5         force proposition     2    6 7.190238
## 6        production échelle     2    5 7.034038&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wordcloud(words = stats$keyword, freq = stats$freq, min.freq = 3,
          max.words=100, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, &amp;quot;Dark2&amp;quot;), scale = c(2.5, .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-09-21-web-scraping-indeed-with-r_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that this method has selected important french key words related to the data scientist job !
In the first positions, we find the key words: “artificial intelligence”, “dashboards”, “higher education”, “predictive model”. I’d better check if these words appear on my resume !&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I hope I convinced you that it is possible to optimize your job search with Data Science!&lt;/p&gt;
&lt;p&gt;If this post has caught your interest and you are looking for a new Data Scientist, do not hesitate to contact me on my &lt;a href=&#34;mailto:aurelien.callens@gmail.com&#34;&gt;mail&lt;/a&gt; as I am currently looking for a job in France (Hybrid, Remote) or in Europe (Remote).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;custom-functions-to-clean-data-extracted-from-the-webpage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Custom functions to clean data extracted from the webpage&lt;/h2&gt;
&lt;p&gt;These functions use several methods such as regular expressions, stop words and conditional statements to clean the textual data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(stringr)
library(httr)
library(tidystopwords)
library(textcat)

# Function to tidy the data related to the company
tidy_comploc &amp;lt;- function(text){
  lst &amp;lt;- str_split(text, pattern = &amp;quot;\n&amp;quot;, simplify =T)
  ext_str &amp;lt;- substr(lst[1], nchar(lst[1])-2, nchar(lst[1]))
  res &amp;lt;- suppressWarnings(as.numeric(gsub(&amp;#39;,&amp;#39;, &amp;#39;.&amp;#39;, ext_str)))
  lst[1] &amp;lt;- ifelse(is.na(res), lst[1], substr(lst[1], 1, nchar(lst[1])-3))
  lst[3] &amp;lt;- res
  t(as.matrix(lst))
}

# Function to tidy the short job description provided with the job post
tidy_job_desc &amp;lt;- function(text){
  stopwords &amp;lt;- c(&amp;quot;Candidature facile&amp;quot;, &amp;quot;Employeur réactif&amp;quot;)
  text &amp;lt;- str_remove_all(text, paste(stopwords, collapse = &amp;quot;|&amp;quot;))
  stopwords_2 &amp;lt;- &amp;quot;(Posted|Employer).*&amp;quot;
  text &amp;lt;- str_remove_all(text, stopwords_2)
  text
}

# Function to tidy the salary data if provided
tidy_salary &amp;lt;- function(text){
  if(is.na(text)){
    others &amp;lt;- NA
    sal_low &amp;lt;- NA
    sal_high &amp;lt;- NA
  }else{
    text &amp;lt;- str_split(text, &amp;quot;\n&amp;quot;, simplify = T)
    others &amp;lt;- paste(text[str_detect(text, &amp;quot;€&amp;quot;, negate = T)], collapse = &amp;quot; | &amp;quot;)
    sal &amp;lt;- text[str_detect(text, &amp;quot;€&amp;quot;, negate = F)]
    if(rlang::is_empty(sal)){
      sal_low &amp;lt;- NA
      sal_high &amp;lt;- NA
    }else{
      range_sal &amp;lt;- as.numeric(str_split(str_remove_all(str_replace(sal, &amp;quot;à&amp;quot;, &amp;quot;-&amp;quot;), &amp;quot;[^0-9.-]&amp;quot;), &amp;quot;-&amp;quot;, simplify = TRUE))
      sal_low &amp;lt;- sort(range_sal)[1]
      sal_high &amp;lt;- sort(range_sal)[2]

      if(str_detect(sal, &amp;quot;an&amp;quot;)){
        sal_low &amp;lt;- floor(sal_low/12)
        sal_high &amp;lt;- floor(sal_high/12)
      }
    }
  }
  return(c(as.numeric(sal_low), as.numeric(sal_high), others))
}

# Function to tidy the location of the job (Remote/Hybrid/Onsite) + homogenize 
# location and zip code
tidy_location &amp;lt;- function(final_df){
  final_df$Job_type &amp;lt;- ifelse(final_df$Location == &amp;quot;Télétravail&amp;quot;, &amp;quot;Full Remote&amp;quot;, ifelse(str_detect(final_df$Location, &amp;quot;Télétravail&amp;quot;), &amp;quot;Hybrid&amp;quot;, &amp;quot;On site&amp;quot;))
  final_df$Loc_possibility &amp;lt;- ifelse(str_detect(final_df$Location, &amp;quot;lieu&amp;quot;), &amp;quot;Plusieurs lieux&amp;quot;, NA)
  stopwords &amp;lt;- c(&amp;quot;Télétravail à&amp;quot;, &amp;quot;Télétravail&amp;quot;, &amp;quot;à&amp;quot;, &amp;quot;hybride&amp;quot;)
  final_df$Loc_tidy &amp;lt;- str_remove_all(final_df$Location, paste(stopwords, collapse = &amp;quot;|&amp;quot;))
  final_df$Loc_tidy &amp;lt;- str_remove_all(final_df$Loc_tidy, &amp;quot;[+].*&amp;quot;)
  final_df$Loc_tidy &amp;lt;- str_trim(final_df$Loc_tidy)
  final_df$Loc_tidy &amp;lt;-  sapply(final_df$Loc_tidy,
                               function(x){
                                 if(!is.na(suppressWarnings(as.numeric(substr(x, 1, 5))))){
                                   return(paste(substr(x, 7, 30), paste0(&amp;#39;(&amp;#39;, substr(final_df$Loc_tidy[2], 1, 2), &amp;#39;)&amp;#39;)))
                                 }else{
                                   return(x)
                                 }})
  return(final_df)
}

# Function to keep only certain words in text
keep_words &amp;lt;- function(text, keep) {
  words &amp;lt;- strsplit(text, &amp;quot; &amp;quot;)[[1]]
  txt &amp;lt;- paste(words[words %in% keep], collapse = &amp;quot; &amp;quot;)
  return(txt)
}

# Homogenize the job title and class them in a few categories
clean_job_title &amp;lt;- function(job_titles){
  job_titles &amp;lt;- tolower(job_titles)
  job_titles &amp;lt;- gsub(&amp;quot;[[:punct:]]&amp;quot;, &amp;quot; &amp;quot;, job_titles, perl=TRUE)

  words_to_keep &amp;lt;- c(&amp;quot;data&amp;quot;, &amp;quot;scientist&amp;quot;, &amp;quot;junior&amp;quot;, &amp;quot;senior&amp;quot;, &amp;quot;engineer&amp;quot;, &amp;quot;nlp&amp;quot;,
                     &amp;quot;analyst&amp;quot;, &amp;quot;analytics&amp;quot;, &amp;quot;analytic&amp;quot;, &amp;quot;science&amp;quot;, &amp;quot;sciences&amp;quot;,
                     &amp;quot;computer&amp;quot;, &amp;quot;vision&amp;quot;, &amp;quot;ingenieur&amp;quot;, &amp;quot;données&amp;quot;, &amp;quot;analyste&amp;quot;,
                     &amp;quot;analyses&amp;quot;, &amp;quot;lead&amp;quot;, &amp;quot;leader&amp;quot;, &amp;quot;dataminer&amp;quot;, &amp;quot;mining&amp;quot;, &amp;quot;chief&amp;quot;,
                     &amp;quot;miner&amp;quot;, &amp;quot;analyse&amp;quot;, &amp;#39;head&amp;#39;)
  job_titles_c &amp;lt;- unlist(sapply(job_titles, function(x){keep_words(x, words_to_keep)}, USE.NAMES = F))
  job_titles_c &amp;lt;- unlist(sapply(job_titles_c, function(x){paste(unique(unlist(str_split(x, &amp;quot; &amp;quot;))), collapse = &amp;quot; &amp;quot;)}, USE.NAMES = F))
  table(job_titles_c)

  data_analytics_ind &amp;lt;-  job_titles_c %in% c(&amp;quot;analyses data&amp;quot;, &amp;quot;analyst data&amp;quot;, &amp;quot;analyste data&amp;quot;, &amp;quot;analyste data scientist&amp;quot;, &amp;quot;data analyse&amp;quot;,
                                             &amp;quot;analyste données&amp;quot;, &amp;quot;analytic data scientist&amp;quot;, &amp;quot;analytics data&amp;quot;, &amp;quot;analytics data engineer&amp;quot;, &amp;quot;data analyst engineer&amp;quot;,
                                             &amp;quot;data analyst données&amp;quot;, &amp;quot;data analyst scientist&amp;quot;, &amp;quot;data analyst scientist données&amp;quot;, &amp;quot;data analyste&amp;quot;, &amp;quot;data analyst analytics&amp;quot;,
                                             &amp;quot;data analytics&amp;quot;, &amp;quot;data analytics engineer&amp;quot;, &amp;quot;data engineer analyst&amp;quot;, &amp;quot;data scientist analyst&amp;quot;, &amp;quot;data scientist analytics&amp;quot;)
  job_titles_c[data_analytics_ind] &amp;lt;- &amp;quot;data analyst&amp;quot;

  data_analytics_j_ind &amp;lt;-  job_titles_c %in% c(&amp;quot;junior data analyst&amp;quot;, &amp;quot;junior data analytics&amp;quot;, &amp;quot;junior data scientist analyst&amp;quot;)
  job_titles_c[data_analytics_j_ind] &amp;lt;- &amp;quot;data analyst junior&amp;quot;

  data_scientist_ind &amp;lt;- job_titles_c %in% c(&amp;quot;data computer science&amp;quot;, &amp;quot;data science&amp;quot;, &amp;quot;data science scientist&amp;quot;, &amp;quot;data sciences&amp;quot;,
                                            &amp;quot;data sciences scientist&amp;quot;, &amp;quot;data scientist données&amp;quot;, &amp;quot;data scientist sciences&amp;quot;,
                                            &amp;quot;données data scientist&amp;quot;, &amp;quot;scientist data&amp;quot;, &amp;quot;science données&amp;quot;, &amp;quot;scientist data&amp;quot;,
                                            &amp;quot;scientist data science&amp;quot;, &amp;quot;computer data science&amp;quot;, &amp;quot;data science données&amp;quot;, &amp;quot;data scientist science&amp;quot;)
  job_titles_c[data_scientist_ind] &amp;lt;- &amp;quot;data scientist&amp;quot;

  data_scientist_j_ind &amp;lt;- job_titles_c %in% c(&amp;quot;junior data scientist&amp;quot;)
  job_titles_c[data_scientist_j_ind] &amp;lt;- &amp;quot;data scientist junior&amp;quot;

  data_engineer_ind &amp;lt;- job_titles_c %in% c(&amp;quot;data engineer scientist&amp;quot;, &amp;quot;data science engineer&amp;quot;, &amp;quot;data miner&amp;quot;, &amp;quot;data scientist engineer&amp;quot;,
                                           &amp;quot;dataminer&amp;quot;, &amp;quot;engineer data scientist&amp;quot;, &amp;quot;senior data scientist engineer&amp;quot;, &amp;quot;ingenieur data scientist&amp;quot;)
  job_titles_c[data_engineer_ind] &amp;lt;- &amp;quot;data engineer&amp;quot;

  nlp_data_scientist_ind &amp;lt;- job_titles_c %in% c(&amp;quot;data scientist nlp&amp;quot;, &amp;quot;nlp data science&amp;quot;,
                                                &amp;quot;nlp data scientist&amp;quot;, &amp;quot;senior data scientist nlp&amp;quot;)
  job_titles_c[nlp_data_scientist_ind] &amp;lt;- &amp;quot;data scientist NLP&amp;quot;

  cv_data_scientist_ind &amp;lt;- job_titles_c %in% c(&amp;quot;computer vision data scientist&amp;quot;, &amp;quot;data science computer vision&amp;quot;,
                                               &amp;quot;data scientist computer vision&amp;quot;)
  job_titles_c[cv_data_scientist_ind] &amp;lt;- &amp;quot;data scientist CV&amp;quot;

  lead_data_scientist_ind &amp;lt;- job_titles_c %in% c(&amp;quot;chief data&amp;quot;, &amp;quot;chief data scientist&amp;quot;, &amp;quot;data scientist leader&amp;quot;, &amp;quot;lead data scientist&amp;quot;,
                                                 &amp;quot;data chief scientist&amp;quot;, &amp;quot;lead data scientist senior&amp;quot;, &amp;quot;head data science&amp;quot;)
  job_titles_c[lead_data_scientist_ind] &amp;lt;- &amp;quot;data scientist lead or higher&amp;quot;
  senior_data_scientist_ind &amp;lt;- job_titles_c %in% c(&amp;quot;senior data scientist&amp;quot;)
  job_titles_c[senior_data_scientist_ind] &amp;lt;- &amp;quot;data scientist senior&amp;quot;

  senior_data_analytics_ind &amp;lt;- job_titles_c %in% c(&amp;quot;senior analytics data scientist&amp;quot;, &amp;quot;senior data analyst&amp;quot;, &amp;quot;senior data scientist analytics&amp;quot;)
  job_titles_c[senior_data_analytics_ind] &amp;lt;- &amp;quot;data analyst senior&amp;quot;


  lead_data_analyst_ind &amp;lt;- job_titles_c %in% c(&amp;quot;lead data analyst senior&amp;quot;, &amp;quot;lead data analyst&amp;quot;)
  job_titles_c[lead_data_analyst_ind] &amp;lt;- &amp;quot;data analyst lead&amp;quot;
  return(job_titles_c)
}

# Function to clean the full job description before word annotation
clean_job_desc &amp;lt;- function(text){
  text &amp;lt;- tolower(text)
  text &amp;lt;- str_replace_all(text, &amp;quot;\n&amp;quot;, &amp;quot; &amp;quot;)
  text &amp;lt;- str_remove(text, pattern = &amp;quot;dé.*du poste &amp;quot;)
  text &amp;lt;- str_remove(text, pattern = &amp;quot;analyse de recr.*&amp;quot;)
  text &amp;lt;- gsub(&amp;quot;(?!&amp;amp;)[[:punct:]+’+…+»+«]&amp;quot;, &amp;quot; &amp;quot;, text, perl=TRUE)

  language &amp;lt;- textcat(text)

  if(language == &amp;quot;french&amp;quot;){
    text &amp;lt;- str_replace_all(text, &amp;quot;œ&amp;quot;, &amp;quot;oe&amp;quot;)
    stopwords &amp;lt;- c(&amp;quot;détails&amp;quot;, &amp;quot;poste&amp;quot;, &amp;quot;description&amp;quot;, &amp;quot;informations&amp;quot;, &amp;quot;complémentaires&amp;quot;, &amp;quot;c&amp;quot;, generate_stoplist(language = &amp;quot;French&amp;quot;))
  }else{
    stopwords &amp;lt;- c(&amp;quot;description&amp;quot;, generate_stoplist(language = &amp;quot;English&amp;quot;))
  }

  text &amp;lt;- str_replace_all(text, paste(stopwords, collapse = &amp;quot; | &amp;quot;), &amp;quot; &amp;quot;)
  text &amp;lt;- str_replace_all(text, paste(stopwords, collapse = &amp;quot; | &amp;quot;), &amp;quot; &amp;quot;)
  text &amp;lt;- str_replace_all(text, paste(stopwords, collapse = &amp;quot; | &amp;quot;), &amp;quot; &amp;quot;)

  return(c(language, text))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analysis of the top R packages</title>
      <link>https://aureliencallens.github.io/2022/07/19/analysis-of-the-top-r-packages/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2022/07/19/analysis-of-the-top-r-packages/</guid>
      <description>
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/vis/vis-network.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/vis/vis-network.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/visNetwork-binding/visNetwork.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;After a little while coding in Python every day for my work, I needed to make a break and perform some R analysis! Since the beginning of my postdoc, I haven’t followed the last trends concerning R packages. In this post, I am going to analyze some data about R packages to see what are the most downloaded packages during the past weeks. I will also visualize all the relationships between the R packages by looking at their required dependencies.&lt;/p&gt;
&lt;p&gt;Let’s import the packages required for this analysis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(magrittr)
library(cranlogs)
library(igraph)
library(visNetwork)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;how-to-find-the-most-popular-r-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to find the most popular R packages ?&lt;/h1&gt;
&lt;p&gt;The first thing is to gather the data about the number of downloads for each package. Luckily for us, there is a package called &lt;em&gt;cranlogs&lt;/em&gt; that does just what we need ! With a simple line of command we can collect data about the 50 packages with the most downloads in the last month, we can then plot the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;popular_pckg &amp;lt;- cran_top_downloads(&amp;quot;last-month&amp;quot;, 50)

popular_pckg %&amp;gt;%
  mutate(package = fct_reorder(package, desc(count))) %&amp;gt;% 
  ggplot(aes(x = package, y = count)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) + 
  scale_x_discrete(expand = expansion(mult = c(0, 0.02))) +
  theme_bw() +
  xlab(&amp;quot;&amp;quot;) +
  ylab(&amp;quot;Downloads&amp;quot;)+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 9, face = &amp;quot;bold&amp;quot;),
        plot.title = element_text(size = 14),
        legend.position = &amp;quot;none&amp;quot;) +labs(x = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2022-07-19-analysis-of-the-top-r-packages.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I thought this graph will be harder to make because of the availability of the data but with the right package everything can be done !&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualisation-of-the-dependencies-between-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualisation of the dependencies between packages&lt;/h1&gt;
&lt;p&gt;Once I saw the above graph, I was wondering about all the dependencies between these packages and I wanted to know which one was the most “connected”. To answer this question, I need more data, especially about the required dependencies of each package. After some research, I found out that data about package (including description and dependencies) can be extracted with a function in the &lt;em&gt;tools&lt;/em&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pkg &amp;lt;- tools::CRAN_package_db()[, c(&amp;#39;Package&amp;#39;, &amp;#39;Description&amp;#39;, &amp;#39;Imports&amp;#39;)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this function extract the data for all the packages and I want to perform the analysis only on the top 100 popular packages. So I decided to couple the function of the &lt;em&gt;cranlog&lt;/em&gt; package with the database I collected with the &lt;code&gt;CRAN_package_db()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Can be quite long hence the parallel map
plan(multisession, workers = 12)
monthly_dl &amp;lt;- future_map(df_pkg$Package, function(x){sum(cran_downloads(x, &amp;#39;last-month&amp;#39;)$count)})
df_pkg$monthly_dl &amp;lt;- unlist(monthly_dl)
# write_csv(df_pkg, &amp;#39;R_pkg_dl.csv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then filter by number of downloads and keep only the top 100:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_pkg &amp;lt;- df_pkg %&amp;gt;% 
  distinct(Package, .keep_all= TRUE) %&amp;gt;% 
  arrange(monthly_dl) %&amp;gt;% 
  top_n(100, monthly_dl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, it is time to prepare the data for a graph visualization. To make a graph, we need two tables. The first one must contain all the relationships between nodes (in our case nodes are packages), it has two columns : ‘from’ and ‘to’. The second table contains only one column with the names of the nodes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;import_cleaning &amp;lt;- function(text){
  text &amp;lt;- gsub(&amp;#39;\\s*\\([^\\)]+\\)&amp;#39;, &amp;#39;&amp;#39;, text)
  text &amp;lt;- gsub(&amp;#39;\\n&amp;#39;, &amp;#39; &amp;#39;, text)
  text &amp;lt;- gsub(&amp;#39; &amp;#39;, &amp;#39;&amp;#39;, text, fixed = TRUE)
  text &amp;lt;- str_split(text, &amp;#39;,&amp;#39;)
  return(text)
}

import_cleaning(df_pkg$Imports[2])

test &amp;lt;- df_pkg %&amp;gt;% 
  mutate(cleaned_imports = import_cleaning(Imports))

df_target &amp;lt;- function(x,y){
  df &amp;lt;- expand.grid(from=x, to=unlist(y))
  return(df)}

for(i in 1:nrow(test)){
  if(i == 1){
    df_res = df_target(test$Package[i], test$cleaned_imports[i])
  }else{
      df_res = rbind(df_res, df_target(test$Package[i], test$cleaned_imports[i]))
  }
}

links &amp;lt;- df_res %&amp;gt;% 
  filter(!is.na(to) | (to == &amp;quot;&amp;quot;))

nodes &amp;lt;- tibble(id=as.character(unique(unlist(df_res))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the two matrices are made, we can interactively visualize the graph network with &lt;em&gt;visNetwork&lt;/em&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;visNetwork(nodes, links) %&amp;gt;%
    visIgraphLayout(type = &amp;quot;full&amp;quot;) %&amp;gt;%
    visNodes(
        shape = &amp;quot;dot&amp;quot;,
        color = list(
            background = &amp;quot;#0085AF&amp;quot;,
            border = &amp;quot;#013848&amp;quot;,
            highlight = &amp;quot;#FF8000&amp;quot;
        ),
        scaling = list(min=2,
                       max = 10),
        shadow = list(enabled = TRUE, size = 10)
    ) %&amp;gt;%
    visEdges(
      arrows=&amp;#39;to&amp;#39;,
        shadow = FALSE,
        color = list(color = &amp;quot;#0085AF&amp;quot;, highlight = &amp;quot;#C62F4B&amp;quot;)
    ) %&amp;gt;%
    visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T)) %&amp;gt;% 
    visLayout(randomSeed = 11)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;visNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;nodes&#34;:{&#34;id&#34;:[&#34;DBI&#34;,&#34;rvest&#34;,&#34;xlsx&#34;,&#34;sp&#34;,&#34;plotly&#34;,&#34;RcppEigen&#34;,&#34;pbkrtest&#34;,&#34;png&#34;,&#34;uuid&#34;,&#34;rematch2&#34;,&#34;dbplyr&#34;,&#34;desc&#34;,&#34;base64enc&#34;,&#34;plyr&#34;,&#34;mime&#34;,&#34;prettyunits&#34;,&#34;readxl&#34;,&#34;highr&#34;,&#34;rprojroot&#34;,&#34;tzdb&#34;,&#34;labeling&#34;,&#34;gtable&#34;,&#34;munsell&#34;,&#34;jquerylib&#34;,&#34;fastmap&#34;,&#34;farver&#34;,&#34;rappdirs&#34;,&#34;tinytex&#34;,&#34;car&#34;,&#34;rstudioapi&#34;,&#34;yaml&#34;,&#34;viridisLite&#34;,&#34;isoband&#34;,&#34;evaluate&#34;,&#34;xml2&#34;,&#34;units&#34;,&#34;hms&#34;,&#34;broom&#34;,&#34;jpeg&#34;,&#34;colorspace&#34;,&#34;lme4&#34;,&#34;generics&#34;,&#34;purrr&#34;,&#34;vroom&#34;,&#34;pkgconfig&#34;,&#34;nloptr&#34;,&#34;readr&#34;,&#34;RColorBrewer&#34;,&#34;htmltools&#34;,&#34;fs&#34;,&#34;utf8&#34;,&#34;rJava&#34;,&#34;testthat&#34;,&#34;digest&#34;,&#34;rmarkdown&#34;,&#34;sass&#34;,&#34;bslib&#34;,&#34;R6&#34;,&#34;xfun&#34;,&#34;lubridate&#34;,&#34;Rcpp&#34;,&#34;scales&#34;,&#34;Hmisc&#34;,&#34;data.table&#34;,&#34;fansi&#34;,&#34;curl&#34;,&#34;openssl&#34;,&#34;zoo&#34;,&#34;ps&#34;,&#34;knitr&#34;,&#34;callr&#34;,&#34;withr&#34;,&#34;tidyverse&#34;,&#34;tidyr&#34;,&#34;tidyselect&#34;,&#34;cpp11&#34;,&#34;crayon&#34;,&#34;pkgdown&#34;,&#34;processx&#34;,&#34;jsonlite&#34;,&#34;ellipsis&#34;,&#34;httr&#34;,&#34;stringi&#34;,&#34;magrittr&#34;,&#34;tibble&#34;,&#34;pillar&#34;,&#34;stringr&#34;,&#34;lifecycle&#34;,&#34;textshaping&#34;,&#34;ragg&#34;,&#34;rgl&#34;,&#34;rgeos&#34;,&#34;vctrs&#34;,&#34;dplyr&#34;,&#34;glue&#34;,&#34;cli&#34;,&#34;sf&#34;,&#34;devtools&#34;,&#34;rlang&#34;,&#34;ggplot2&#34;,null,&#34;selectr&#34;,&#34;xlsxjars&#34;,&#34;grDevices&#34;,&#34;utils&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;lattice&#34;,&#34;grid&#34;,&#34;tools&#34;,&#34;htmlwidgets&#34;,&#34;lazyeval&#34;,&#34;crosstalk&#34;,&#34;promises&#34;,&#34;Matrix&#34;,&#34;MASS&#34;,&#34;methods&#34;,&#34;numDeriv&#34;,&#34;parallel&#34;,&#34;assertthat&#34;,&#34;blob&#34;,&#34;cellranger&#34;,&#34;abind&#34;,&#34;mgcv&#34;,&#34;nnet&#34;,&#34;quantreg&#34;,&#34;maptools&#34;,&#34;nlme&#34;,&#34;backports&#34;,&#34;splines&#34;,&#34;boot&#34;,&#34;minqa&#34;,&#34;bit64&#34;,&#34;clipr&#34;,&#34;brio&#34;,&#34;pkgload&#34;,&#34;praise&#34;,&#34;waldo&#34;,&#34;latticeExtra&#34;,&#34;cluster&#34;,&#34;rpart&#34;,&#34;foreign&#34;,&#34;gridExtra&#34;,&#34;htmlTable&#34;,&#34;viridis&#34;,&#34;askpass&#34;,&#34;dtplyr&#34;,&#34;forcats&#34;,&#34;googledrive&#34;,&#34;googlesheets4&#34;,&#34;haven&#34;,&#34;modelr&#34;,&#34;reprex&#34;,&#34;downlit&#34;,&#34;memoise&#34;,&#34;whisker&#34;,&#34;systemfonts&#34;,&#34;classInt&#34;,&#34;s2&#34;,&#34;pkgbuild&#34;,&#34;rcmdcheck&#34;,&#34;remotes&#34;,&#34;roxygen2&#34;,&#34;rversions&#34;,&#34;sessioninfo&#34;],&#34;x&#34;:[0.386457563185495,-0.209544515485143,0.541047694476809,0.337579595002297,-0.15375301048213,0.384321005848065,0.225574547168811,-0.763580522677827,-0.53296131211388,0.264436651653297,0.0737917940800772,-0.324352857950663,0.158271193550516,0.670093190078233,-0.307916385941493,0.734179246720498,0.20791691191607,-0.0712632507674694,-0.696125153800372,-0.255329594109032,0.0945639243467313,0.526502568852197,0.0538702155799515,-0.0967571318930509,0.0476503908958459,-0.290574501616033,-0.534922341631374,-0.153956878112058,0.406621830954043,-0.29878925249601,-0.268147016988015,-0.32919118357942,0.393947721600689,-0.184549648778358,-0.212436067610972,0.585639558768406,-0.0187328500316927,0.130636984337608,-0.30217852589036,0.22648698228787,0.380183746515616,0.221581713058898,-0.0945889516994241,-0.0665737909120777,0.166403369481109,0.417513663226184,-0.032697211107562,-0.273693788279883,0.0532294770112041,-0.326691368752584,0.237193646564678,0.856100235769082,-0.242444689973535,-0.0725864394027208,-0.0897584938128452,-0.266928788846343,-0.0947622715364383,-0.153783120191336,-0.0345628674810706,0.180557899329029,0.403024491946714,-0.126818723457706,0.5291618843136,0.185466678963278,0.238576526538908,-0.728183140631224,-0.779973985463301,0.361471546312692,-0.209900085083992,-0.0317327551249718,-0.248890863965528,-0.0169748745963975,-0.0415149093304951,-0.0202612979703686,0.00901431258157959,1,-0.0264603107610911,-0.286350999349902,-0.220569441952127,-0.187898675811517,-0.112399411918016,-0.384272769229265,0.101642057841186,-0.00923011046910349,0.025122874459405,0.063459132861478,0.0226921720181037,-0.110554537997695,-0.948624273306784,-0.718257973161049,-0.00568367248081125,0.313369509050134,-0.0519719185836822,0.0482829442808488,0.0675080468681686,-0.121854633416273,0.344170661329352,-0.299278966628783,-0.0746862829545499,0.187750969704254,-0.0481324372716931,-0.399723484866724,0.816299890194279,0.218630601739407,0.106177228683935,0.125542561814993,0.249988101242616,0.499095412965317,0.435078197534759,-0.0457127589127149,-0.180889562681238,-0.392075018275946,-0.47609550443714,-0.445227224004289,0.537682898095351,0.351304802458883,0.0617813161374929,0.246488798408767,0.320367709175466,0.30150658675907,0.143308818444642,0.419623186247672,0.618237972341979,0.504532953073644,0.639304631168706,0.65349231174949,0.55987775348312,0.471825847785384,0.414115027368835,0.472082159636044,0.347747830431689,0.546874903128462,-0.188333109125767,-0.0773546818420742,-0.574211289408827,-0.472783011937467,-0.601422239933286,-0.584681217908274,0.804977383174812,0.854881091625112,0.831180575949758,0.856395528098377,0.791057025383752,0.765202659331969,0.822486301630695,-1,0.0421446409857009,-0.0725354812567434,0.102450792867618,-0.196576957167383,-0.131199250751859,0.00827460620785669,-0.0520146030628683,-0.597487467886719,-0.514083660190582,-0.625795632187973,-0.917206892735193,0.673206486579979,0.691098567007708,-0.533645972155654,-0.636627828627995,-0.575964083382927,-0.554771805428217,-0.47734458800951,-0.619376062350072],&#34;y&#34;:[-0.376582869121508,-0.398951878295112,-0.206237036236983,0.052948592007013,0.00460342078076459,-0.159371867171528,0.0470959284859178,0.734138613529798,0.871517472561515,-0.5419889676815,-0.33803668270972,-0.190046445358737,0.17074833223135,-0.451063975670746,0.00758753737487106,-1,-0.471896616437729,0.432950662875889,-0.146351411712139,-0.636091498667435,0.240001435957712,-0.00840406945816641,0.186483859622117,0.274301876999692,0.447764096875386,0.412827035121424,0.313322908464758,0.389687541809218,0.205622319324374,-0.542888092730679,0.164453357088491,0.219852743886424,-0.0790198236170336,0.0419010036938008,-0.313461514321644,-0.310216188638168,-0.397020752440457,-0.268678570057359,0.955286042260105,0.194664069708546,0.330991615441971,-0.378832088843713,-0.243694721187652,-0.355997673279183,-0.399009485818769,0.628552522291966,-0.342915156447199,0.269750722670728,0.107322864222573,-0.0838553508744814,-0.693899071289692,-0.273087886943925,-0.194462730099049,-0.0513460150718271,0.088192741535787,0.0751537736676968,0.0403973266349467,-0.141556329171388,0.243934098262388,-0.605493618955002,-0.266302688792167,0.0948529470960133,0.140942181477251,0.0956412957554662,-0.25648413937357,-0.26081605223673,-0.335241012077671,0.0947298143147361,-0.0376984428999569,0.137170537286727,-0.161154190402051,-0.127583593578011,-0.452595028876187,-0.262681976092465,-0.379118045708484,-0.755668758847009,-0.221267660853506,-0.0903769294246142,-0.0889203290223447,-0.130405080590519,-0.374151975372798,-0.235956947625526,0.033002327288925,-0.177413462083141,-0.287420072288616,-0.378581556514767,-0.116656055931081,-0.267799559936146,0.16922390334137,0.0997282218018463,0.0327831165377941,-0.0192563234665462,-0.294006089710639,-0.22415904053209,-0.280813231930088,-0.320796654310692,-0.140916287698948,-0.299417449341477,-0.193351734623789,-0.0963177788481836,1,-0.678802934696719,-0.347367043820736,-0.0270772168020018,-0.123441124334529,-0.0236015737586579,0.104883742556678,0.247218241369793,0.0579404360055764,-0.0104052183567368,0.238008539030753,0.286420459917831,0.158321223960414,0.221042835748384,-0.0692134538905717,0.175422110725514,-0.125926270409502,0.37990588461653,0.354498182283786,-0.613191620646399,-0.653344101389674,-0.717574008029819,0.436479615259521,0.0555041978630559,0.277639195860031,0.384635994151028,0.456523072211169,0.444432971725482,-0.49511088791794,0.590677794225397,0.611096327772316,0.585317015444846,-0.655195073330833,-0.642438351342685,-0.290222568006941,-0.352948489902007,-0.210404633957181,-0.125690444455695,0.239259325029257,0.190574509419829,0.061450106547752,0.120559499514448,0.161923922768571,0.331150453955103,0.293074233519843,-0.379441016037974,-0.744945429834416,-0.808050849635277,-0.771713668935451,-0.761092241459022,-0.768269305666766,-0.796149880560864,-0.750879281905125,0.0555171842495394,-0.19999005069526,-0.0120392829781844,0.233565280281424,-0.25356044923125,-0.174519035640793,-0.505197993049675,-0.398241101166873,-0.443024621999615,-0.557383254926031,-0.560663646130983,-0.484396506940254],&#34;label&#34;:[&#34;DBI&#34;,&#34;rvest&#34;,&#34;xlsx&#34;,&#34;sp&#34;,&#34;plotly&#34;,&#34;RcppEigen&#34;,&#34;pbkrtest&#34;,&#34;png&#34;,&#34;uuid&#34;,&#34;rematch2&#34;,&#34;dbplyr&#34;,&#34;desc&#34;,&#34;base64enc&#34;,&#34;plyr&#34;,&#34;mime&#34;,&#34;prettyunits&#34;,&#34;readxl&#34;,&#34;highr&#34;,&#34;rprojroot&#34;,&#34;tzdb&#34;,&#34;labeling&#34;,&#34;gtable&#34;,&#34;munsell&#34;,&#34;jquerylib&#34;,&#34;fastmap&#34;,&#34;farver&#34;,&#34;rappdirs&#34;,&#34;tinytex&#34;,&#34;car&#34;,&#34;rstudioapi&#34;,&#34;yaml&#34;,&#34;viridisLite&#34;,&#34;isoband&#34;,&#34;evaluate&#34;,&#34;xml2&#34;,&#34;units&#34;,&#34;hms&#34;,&#34;broom&#34;,&#34;jpeg&#34;,&#34;colorspace&#34;,&#34;lme4&#34;,&#34;generics&#34;,&#34;purrr&#34;,&#34;vroom&#34;,&#34;pkgconfig&#34;,&#34;nloptr&#34;,&#34;readr&#34;,&#34;RColorBrewer&#34;,&#34;htmltools&#34;,&#34;fs&#34;,&#34;utf8&#34;,&#34;rJava&#34;,&#34;testthat&#34;,&#34;digest&#34;,&#34;rmarkdown&#34;,&#34;sass&#34;,&#34;bslib&#34;,&#34;R6&#34;,&#34;xfun&#34;,&#34;lubridate&#34;,&#34;Rcpp&#34;,&#34;scales&#34;,&#34;Hmisc&#34;,&#34;data.table&#34;,&#34;fansi&#34;,&#34;curl&#34;,&#34;openssl&#34;,&#34;zoo&#34;,&#34;ps&#34;,&#34;knitr&#34;,&#34;callr&#34;,&#34;withr&#34;,&#34;tidyverse&#34;,&#34;tidyr&#34;,&#34;tidyselect&#34;,&#34;cpp11&#34;,&#34;crayon&#34;,&#34;pkgdown&#34;,&#34;processx&#34;,&#34;jsonlite&#34;,&#34;ellipsis&#34;,&#34;httr&#34;,&#34;stringi&#34;,&#34;magrittr&#34;,&#34;tibble&#34;,&#34;pillar&#34;,&#34;stringr&#34;,&#34;lifecycle&#34;,&#34;textshaping&#34;,&#34;ragg&#34;,&#34;rgl&#34;,&#34;rgeos&#34;,&#34;vctrs&#34;,&#34;dplyr&#34;,&#34;glue&#34;,&#34;cli&#34;,&#34;sf&#34;,&#34;devtools&#34;,&#34;rlang&#34;,&#34;ggplot2&#34;,null,&#34;selectr&#34;,&#34;xlsxjars&#34;,&#34;grDevices&#34;,&#34;utils&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;lattice&#34;,&#34;grid&#34;,&#34;tools&#34;,&#34;htmlwidgets&#34;,&#34;lazyeval&#34;,&#34;crosstalk&#34;,&#34;promises&#34;,&#34;Matrix&#34;,&#34;MASS&#34;,&#34;methods&#34;,&#34;numDeriv&#34;,&#34;parallel&#34;,&#34;assertthat&#34;,&#34;blob&#34;,&#34;cellranger&#34;,&#34;abind&#34;,&#34;mgcv&#34;,&#34;nnet&#34;,&#34;quantreg&#34;,&#34;maptools&#34;,&#34;nlme&#34;,&#34;backports&#34;,&#34;splines&#34;,&#34;boot&#34;,&#34;minqa&#34;,&#34;bit64&#34;,&#34;clipr&#34;,&#34;brio&#34;,&#34;pkgload&#34;,&#34;praise&#34;,&#34;waldo&#34;,&#34;latticeExtra&#34;,&#34;cluster&#34;,&#34;rpart&#34;,&#34;foreign&#34;,&#34;gridExtra&#34;,&#34;htmlTable&#34;,&#34;viridis&#34;,&#34;askpass&#34;,&#34;dtplyr&#34;,&#34;forcats&#34;,&#34;googledrive&#34;,&#34;googlesheets4&#34;,&#34;haven&#34;,&#34;modelr&#34;,&#34;reprex&#34;,&#34;downlit&#34;,&#34;memoise&#34;,&#34;whisker&#34;,&#34;systemfonts&#34;,&#34;classInt&#34;,&#34;s2&#34;,&#34;pkgbuild&#34;,&#34;rcmdcheck&#34;,&#34;remotes&#34;,&#34;roxygen2&#34;,&#34;rversions&#34;,&#34;sessioninfo&#34;]},&#34;edges&#34;:{&#34;from&#34;:[&#34;rvest&#34;,&#34;rvest&#34;,&#34;rvest&#34;,&#34;rvest&#34;,&#34;rvest&#34;,&#34;rvest&#34;,&#34;rvest&#34;,&#34;xlsx&#34;,&#34;xlsx&#34;,&#34;xlsx&#34;,&#34;xlsx&#34;,&#34;sp&#34;,&#34;sp&#34;,&#34;sp&#34;,&#34;sp&#34;,&#34;sp&#34;,&#34;sp&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;plotly&#34;,&#34;RcppEigen&#34;,&#34;RcppEigen&#34;,&#34;RcppEigen&#34;,&#34;RcppEigen&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;pbkrtest&#34;,&#34;rematch2&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;dbplyr&#34;,&#34;desc&#34;,&#34;desc&#34;,&#34;desc&#34;,&#34;desc&#34;,&#34;plyr&#34;,&#34;mime&#34;,&#34;readxl&#34;,&#34;readxl&#34;,&#34;readxl&#34;,&#34;highr&#34;,&#34;labeling&#34;,&#34;labeling&#34;,&#34;gtable&#34;,&#34;munsell&#34;,&#34;munsell&#34;,&#34;jquerylib&#34;,&#34;tinytex&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;car&#34;,&#34;isoband&#34;,&#34;isoband&#34;,&#34;evaluate&#34;,&#34;xml2&#34;,&#34;units&#34;,&#34;hms&#34;,&#34;hms&#34;,&#34;hms&#34;,&#34;hms&#34;,&#34;hms&#34;,&#34;hms&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;broom&#34;,&#34;colorspace&#34;,&#34;colorspace&#34;,&#34;colorspace&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;lme4&#34;,&#34;generics&#34;,&#34;purrr&#34;,&#34;purrr&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;vroom&#34;,&#34;pkgconfig&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;readr&#34;,&#34;htmltools&#34;,&#34;htmltools&#34;,&#34;htmltools&#34;,&#34;htmltools&#34;,&#34;htmltools&#34;,&#34;htmltools&#34;,&#34;fs&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;testthat&#34;,&#34;digest&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;rmarkdown&#34;,&#34;sass&#34;,&#34;sass&#34;,&#34;sass&#34;,&#34;sass&#34;,&#34;sass&#34;,&#34;bslib&#34;,&#34;bslib&#34;,&#34;bslib&#34;,&#34;bslib&#34;,&#34;bslib&#34;,&#34;bslib&#34;,&#34;xfun&#34;,&#34;xfun&#34;,&#34;lubridate&#34;,&#34;Rcpp&#34;,&#34;Rcpp&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;scales&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;Hmisc&#34;,&#34;data.table&#34;,&#34;fansi&#34;,&#34;fansi&#34;,&#34;openssl&#34;,&#34;zoo&#34;,&#34;zoo&#34;,&#34;zoo&#34;,&#34;zoo&#34;,&#34;ps&#34;,&#34;knitr&#34;,&#34;knitr&#34;,&#34;knitr&#34;,&#34;knitr&#34;,&#34;knitr&#34;,&#34;knitr&#34;,&#34;knitr&#34;,&#34;callr&#34;,&#34;callr&#34;,&#34;callr&#34;,&#34;withr&#34;,&#34;withr&#34;,&#34;withr&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyverse&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyr&#34;,&#34;tidyselect&#34;,&#34;tidyselect&#34;,&#34;tidyselect&#34;,&#34;tidyselect&#34;,&#34;tidyselect&#34;,&#34;crayon&#34;,&#34;crayon&#34;,&#34;crayon&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;pkgdown&#34;,&#34;processx&#34;,&#34;processx&#34;,&#34;processx&#34;,&#34;ellipsis&#34;,&#34;httr&#34;,&#34;httr&#34;,&#34;httr&#34;,&#34;httr&#34;,&#34;httr&#34;,&#34;stringi&#34;,&#34;stringi&#34;,&#34;stringi&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;tibble&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;pillar&#34;,&#34;stringr&#34;,&#34;stringr&#34;,&#34;stringr&#34;,&#34;lifecycle&#34;,&#34;lifecycle&#34;,&#34;textshaping&#34;,&#34;ragg&#34;,&#34;ragg&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgl&#34;,&#34;rgeos&#34;,&#34;rgeos&#34;,&#34;rgeos&#34;,&#34;vctrs&#34;,&#34;vctrs&#34;,&#34;vctrs&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;dplyr&#34;,&#34;glue&#34;,&#34;cli&#34;,&#34;cli&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;sf&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;devtools&#34;,&#34;rlang&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;,&#34;ggplot2&#34;],&#34;to&#34;:[&#34;httr&#34;,&#34;lifecycle&#34;,&#34;magrittr&#34;,&#34;rlang&#34;,&#34;selectr&#34;,&#34;tibble&#34;,&#34;xml2&#34;,&#34;rJava&#34;,&#34;xlsxjars&#34;,&#34;grDevices&#34;,&#34;utils&#34;,&#34;utils&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;grDevices&#34;,&#34;lattice&#34;,&#34;grid&#34;,&#34;tools&#34;,&#34;scales&#34;,&#34;httr&#34;,&#34;jsonlite&#34;,&#34;magrittr&#34;,&#34;digest&#34;,&#34;viridisLite&#34;,&#34;base64enc&#34;,&#34;htmltools&#34;,&#34;htmlwidgets&#34;,&#34;tidyr&#34;,&#34;RColorBrewer&#34;,&#34;dplyr&#34;,&#34;vctrs&#34;,&#34;tibble&#34;,&#34;lazyeval&#34;,&#34;rlang&#34;,&#34;crosstalk&#34;,&#34;purrr&#34;,&#34;data.table&#34;,&#34;promises&#34;,&#34;Matrix&#34;,&#34;Rcpp&#34;,&#34;stats&#34;,&#34;utils&#34;,&#34;broom&#34;,&#34;dplyr&#34;,&#34;magrittr&#34;,&#34;MASS&#34;,&#34;Matrix&#34;,&#34;methods&#34;,&#34;numDeriv&#34;,&#34;parallel&#34;,&#34;knitr&#34;,&#34;tibble&#34;,&#34;assertthat&#34;,&#34;blob&#34;,&#34;cli&#34;,&#34;DBI&#34;,&#34;dplyr&#34;,&#34;glue&#34;,&#34;lifecycle&#34;,&#34;magrittr&#34;,&#34;methods&#34;,&#34;pillar&#34;,&#34;purrr&#34;,&#34;R6&#34;,&#34;rlang&#34;,&#34;tibble&#34;,&#34;tidyselect&#34;,&#34;utils&#34;,&#34;vctrs&#34;,&#34;withr&#34;,&#34;cli&#34;,&#34;R6&#34;,&#34;rprojroot&#34;,&#34;utils&#34;,&#34;Rcpp&#34;,&#34;tools&#34;,&#34;cellranger&#34;,&#34;tibble&#34;,&#34;utils&#34;,&#34;xfun&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;grid&#34;,&#34;colorspace&#34;,&#34;methods&#34;,&#34;htmltools&#34;,&#34;xfun&#34;,&#34;abind&#34;,&#34;MASS&#34;,&#34;mgcv&#34;,&#34;nnet&#34;,&#34;pbkrtest&#34;,&#34;quantreg&#34;,&#34;grDevices&#34;,&#34;utils&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;maptools&#34;,&#34;lme4&#34;,&#34;nlme&#34;,&#34;grid&#34;,&#34;utils&#34;,&#34;methods&#34;,&#34;methods&#34;,&#34;Rcpp&#34;,&#34;ellipsis&#34;,&#34;lifecycle&#34;,&#34;methods&#34;,&#34;pkgconfig&#34;,&#34;rlang&#34;,&#34;vctrs&#34;,&#34;backports&#34;,&#34;dplyr&#34;,&#34;ellipsis&#34;,&#34;generics&#34;,&#34;glue&#34;,&#34;methods&#34;,&#34;purrr&#34;,&#34;rlang&#34;,&#34;stringr&#34;,&#34;tibble&#34;,&#34;tidyr&#34;,&#34;ggplot2&#34;,&#34;graphics&#34;,&#34;grDevices&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;grid&#34;,&#34;splines&#34;,&#34;utils&#34;,&#34;parallel&#34;,&#34;MASS&#34;,&#34;lattice&#34;,&#34;boot&#34;,&#34;nlme&#34;,&#34;minqa&#34;,&#34;nloptr&#34;,&#34;methods&#34;,&#34;magrittr&#34;,&#34;rlang&#34;,&#34;bit64&#34;,&#34;crayon&#34;,&#34;cli&#34;,&#34;glue&#34;,&#34;hms&#34;,&#34;lifecycle&#34;,&#34;methods&#34;,&#34;rlang&#34;,&#34;stats&#34;,&#34;tibble&#34;,&#34;tzdb&#34;,&#34;vctrs&#34;,&#34;tidyselect&#34;,&#34;withr&#34;,&#34;utils&#34;,&#34;cli&#34;,&#34;clipr&#34;,&#34;crayon&#34;,&#34;hms&#34;,&#34;lifecycle&#34;,&#34;methods&#34;,&#34;R6&#34;,&#34;rlang&#34;,&#34;tibble&#34;,&#34;utils&#34;,&#34;vroom&#34;,&#34;utils&#34;,&#34;digest&#34;,&#34;grDevices&#34;,&#34;base64enc&#34;,&#34;rlang&#34;,&#34;fastmap&#34;,&#34;methods&#34;,&#34;brio&#34;,&#34;callr&#34;,&#34;cli&#34;,&#34;crayon&#34;,&#34;desc&#34;,&#34;digest&#34;,&#34;ellipsis&#34;,&#34;evaluate&#34;,&#34;jsonlite&#34;,&#34;lifecycle&#34;,&#34;magrittr&#34;,&#34;methods&#34;,&#34;pkgload&#34;,&#34;praise&#34;,&#34;processx&#34;,&#34;ps&#34;,&#34;R6&#34;,&#34;rlang&#34;,&#34;utils&#34;,&#34;waldo&#34;,&#34;withr&#34;,&#34;utils&#34;,&#34;bslib&#34;,&#34;evaluate&#34;,&#34;htmltools&#34;,&#34;jquerylib&#34;,&#34;jsonlite&#34;,&#34;knitr&#34;,&#34;methods&#34;,&#34;stringr&#34;,&#34;tinytex&#34;,&#34;tools&#34;,&#34;utils&#34;,&#34;xfun&#34;,&#34;yaml&#34;,&#34;fs&#34;,&#34;rlang&#34;,&#34;htmltools&#34;,&#34;R6&#34;,&#34;rappdirs&#34;,&#34;grDevices&#34;,&#34;htmltools&#34;,&#34;jsonlite&#34;,&#34;sass&#34;,&#34;jquerylib&#34;,&#34;rlang&#34;,&#34;stats&#34;,&#34;tools&#34;,&#34;generics&#34;,&#34;methods&#34;,&#34;utils&#34;,&#34;farver&#34;,&#34;labeling&#34;,&#34;lifecycle&#34;,&#34;munsell&#34;,&#34;R6&#34;,&#34;RColorBrewer&#34;,&#34;rlang&#34;,&#34;viridisLite&#34;,&#34;methods&#34;,&#34;latticeExtra&#34;,&#34;cluster&#34;,&#34;rpart&#34;,&#34;nnet&#34;,&#34;foreign&#34;,&#34;gtable&#34;,&#34;grid&#34;,&#34;gridExtra&#34;,&#34;data.table&#34;,&#34;htmlTable&#34;,&#34;viridis&#34;,&#34;htmltools&#34;,&#34;base64enc&#34;,&#34;methods&#34;,&#34;grDevices&#34;,&#34;utils&#34;,&#34;askpass&#34;,&#34;utils&#34;,&#34;graphics&#34;,&#34;grDevices&#34;,&#34;lattice&#34;,&#34;utils&#34;,&#34;evaluate&#34;,&#34;highr&#34;,&#34;methods&#34;,&#34;stringr&#34;,&#34;yaml&#34;,&#34;xfun&#34;,&#34;tools&#34;,&#34;processx&#34;,&#34;R6&#34;,&#34;utils&#34;,&#34;graphics&#34;,&#34;grDevices&#34;,&#34;stats&#34;,&#34;broom&#34;,&#34;cli&#34;,&#34;crayon&#34;,&#34;dbplyr&#34;,&#34;dplyr&#34;,&#34;dtplyr&#34;,&#34;forcats&#34;,&#34;googledrive&#34;,&#34;googlesheets4&#34;,&#34;ggplot2&#34;,&#34;haven&#34;,&#34;hms&#34;,&#34;httr&#34;,&#34;jsonlite&#34;,&#34;lubridate&#34;,&#34;magrittr&#34;,&#34;modelr&#34;,&#34;pillar&#34;,&#34;purrr&#34;,&#34;readr&#34;,&#34;readxl&#34;,&#34;reprex&#34;,&#34;rlang&#34;,&#34;rstudioapi&#34;,&#34;rvest&#34;,&#34;stringr&#34;,&#34;tibble&#34;,&#34;tidyr&#34;,&#34;xml2&#34;,&#34;dplyr&#34;,&#34;ellipsis&#34;,&#34;glue&#34;,&#34;lifecycle&#34;,&#34;magrittr&#34;,&#34;purrr&#34;,&#34;rlang&#34;,&#34;tibble&#34;,&#34;tidyselect&#34;,&#34;utils&#34;,&#34;vctrs&#34;,&#34;ellipsis&#34;,&#34;glue&#34;,&#34;purrr&#34;,&#34;rlang&#34;,&#34;vctrs&#34;,&#34;grDevices&#34;,&#34;methods&#34;,&#34;utils&#34;,&#34;bslib&#34;,&#34;callr&#34;,&#34;crayon&#34;,&#34;desc&#34;,&#34;digest&#34;,&#34;downlit&#34;,&#34;fs&#34;,&#34;httr&#34;,&#34;jsonlite&#34;,&#34;magrittr&#34;,&#34;memoise&#34;,&#34;purrr&#34;,&#34;ragg&#34;,&#34;rlang&#34;,&#34;rmarkdown&#34;,&#34;tibble&#34;,&#34;whisker&#34;,&#34;withr&#34;,&#34;xml2&#34;,&#34;yaml&#34;,&#34;ps&#34;,&#34;R6&#34;,&#34;utils&#34;,&#34;rlang&#34;,&#34;curl&#34;,&#34;jsonlite&#34;,&#34;mime&#34;,&#34;openssl&#34;,&#34;R6&#34;,&#34;tools&#34;,&#34;utils&#34;,&#34;stats&#34;,&#34;ellipsis&#34;,&#34;fansi&#34;,&#34;lifecycle&#34;,&#34;magrittr&#34;,&#34;methods&#34;,&#34;pillar&#34;,&#34;pkgconfig&#34;,&#34;rlang&#34;,&#34;utils&#34;,&#34;vctrs&#34;,&#34;cli&#34;,&#34;crayon&#34;,&#34;ellipsis&#34;,&#34;fansi&#34;,&#34;glue&#34;,&#34;lifecycle&#34;,&#34;rlang&#34;,&#34;utf8&#34;,&#34;utils&#34;,&#34;vctrs&#34;,&#34;glue&#34;,&#34;magrittr&#34;,&#34;stringi&#34;,&#34;glue&#34;,&#34;rlang&#34;,&#34;systemfonts&#34;,&#34;systemfonts&#34;,&#34;textshaping&#34;,&#34;graphics&#34;,&#34;grDevices&#34;,&#34;stats&#34;,&#34;utils&#34;,&#34;htmlwidgets&#34;,&#34;htmltools&#34;,&#34;knitr&#34;,&#34;jsonlite&#34;,&#34;magrittr&#34;,&#34;R6&#34;,&#34;base64enc&#34;,&#34;mime&#34;,&#34;utils&#34;,&#34;stats&#34;,&#34;graphics&#34;,&#34;cli&#34;,&#34;glue&#34;,&#34;rlang&#34;,&#34;generics&#34;,&#34;glue&#34;,&#34;lifecycle&#34;,&#34;magrittr&#34;,&#34;methods&#34;,&#34;R6&#34;,&#34;rlang&#34;,&#34;tibble&#34;,&#34;tidyselect&#34;,&#34;utils&#34;,&#34;vctrs&#34;,&#34;pillar&#34;,&#34;methods&#34;,&#34;glue&#34;,&#34;utils&#34;,&#34;classInt&#34;,&#34;DBI&#34;,&#34;graphics&#34;,&#34;grDevices&#34;,&#34;grid&#34;,&#34;magrittr&#34;,&#34;Rcpp&#34;,&#34;s2&#34;,&#34;stats&#34;,&#34;tools&#34;,&#34;units&#34;,&#34;utils&#34;,&#34;callr&#34;,&#34;cli&#34;,&#34;desc&#34;,&#34;ellipsis&#34;,&#34;fs&#34;,&#34;httr&#34;,&#34;lifecycle&#34;,&#34;memoise&#34;,&#34;pkgbuild&#34;,&#34;pkgload&#34;,&#34;rcmdcheck&#34;,&#34;remotes&#34;,&#34;rlang&#34;,&#34;roxygen2&#34;,&#34;rstudioapi&#34;,&#34;rversions&#34;,&#34;sessioninfo&#34;,&#34;stats&#34;,&#34;testthat&#34;,&#34;tools&#34;,&#34;utils&#34;,&#34;withr&#34;,&#34;utils&#34;,&#34;digest&#34;,&#34;glue&#34;,&#34;grDevices&#34;,&#34;grid&#34;,&#34;gtable&#34;,&#34;isoband&#34;,&#34;MASS&#34;,&#34;mgcv&#34;,&#34;rlang&#34;,&#34;scales&#34;,&#34;stats&#34;,&#34;tibble&#34;,&#34;withr&#34;]},&#34;nodesToDataframe&#34;:true,&#34;edgesToDataframe&#34;:true,&#34;options&#34;:{&#34;width&#34;:&#34;100%&#34;,&#34;height&#34;:&#34;100%&#34;,&#34;nodes&#34;:{&#34;physics&#34;:false,&#34;shape&#34;:&#34;dot&#34;,&#34;color&#34;:{&#34;background&#34;:&#34;#0085AF&#34;,&#34;border&#34;:&#34;#013848&#34;,&#34;highlight&#34;:&#34;#FF8000&#34;},&#34;shadow&#34;:{&#34;enabled&#34;:true,&#34;size&#34;:10},&#34;scaling&#34;:{&#34;min&#34;:2,&#34;max&#34;:10}},&#34;manipulation&#34;:{&#34;enabled&#34;:false},&#34;edges&#34;:{&#34;smooth&#34;:false,&#34;arrows&#34;:&#34;to&#34;,&#34;color&#34;:{&#34;color&#34;:&#34;#0085AF&#34;,&#34;highlight&#34;:&#34;#C62F4B&#34;},&#34;shadow&#34;:false},&#34;physics&#34;:{&#34;stabilization&#34;:false},&#34;interaction&#34;:{&#34;hover&#34;:true,&#34;zoomSpeed&#34;:1},&#34;layout&#34;:{&#34;randomSeed&#34;:11}},&#34;groups&#34;:null,&#34;width&#34;:null,&#34;height&#34;:null,&#34;idselection&#34;:{&#34;enabled&#34;:false,&#34;style&#34;:&#34;width: 150px; height: 26px&#34;,&#34;useLabels&#34;:true,&#34;main&#34;:&#34;Select by id&#34;},&#34;byselection&#34;:{&#34;enabled&#34;:false,&#34;style&#34;:&#34;width: 150px; height: 26px&#34;,&#34;multiple&#34;:false,&#34;hideColor&#34;:&#34;rgba(200,200,200,0.5)&#34;,&#34;highlight&#34;:false},&#34;main&#34;:null,&#34;submain&#34;:null,&#34;footer&#34;:null,&#34;background&#34;:&#34;rgba(0, 0, 0, 0)&#34;,&#34;igraphlayout&#34;:{&#34;type&#34;:&#34;full&#34;},&#34;tooltipStay&#34;:300,&#34;tooltipStyle&#34;:&#34;position: fixed;visibility:hidden;padding: 5px;white-space: nowrap;font-family: verdana;font-size:14px;font-color:#000000;background-color: #f5f4ed;-moz-border-radius: 3px;-webkit-border-radius: 3px;border-radius: 3px;border: 1px solid #808074;box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.2);&#34;,&#34;highlight&#34;:{&#34;enabled&#34;:true,&#34;hoverNearest&#34;:true,&#34;degree&#34;:1,&#34;algorithm&#34;:&#34;all&#34;,&#34;hideColor&#34;:&#34;rgba(200,200,200,0.5)&#34;,&#34;labelOnly&#34;:true},&#34;collapse&#34;:{&#34;enabled&#34;:false,&#34;fit&#34;:false,&#34;resetHighlight&#34;:true,&#34;clusterOptions&#34;:null,&#34;keepCoord&#34;:true,&#34;labelSuffix&#34;:&#34;(cluster)&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Do not hesitate to move, zoom in or select packages to see their dependencies !&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can R and Shiny make me a better fisherman? Part 4</title>
      <link>https://aureliencallens.github.io/2022/04/12/r-shiny-fishing-part4/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2022/04/12/r-shiny-fishing-part4/</guid>
      <description>


&lt;p&gt;In this post, I explore the data I have collected during the last year with the updated version of the application (presented &lt;a href=&#34;https://aureliencallens.github.io/2021/06/01/r-shiny-fishing-part3/&#34;&gt;here&lt;/a&gt;). This quick exploratory analysis is performed with two packages I really enjoy: &lt;em&gt;Plotly&lt;/em&gt; and &lt;em&gt;shiny&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For reminder, my new application store the data in three csv files. The first one contains variables related to the fishing conditions. The second one contains information about my catches and finally the third one contains information about the characteristics of the lures I used during the session.&lt;/p&gt;
&lt;div id=&#34;shiny-to-explore-fishing-data-by-session&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny to explore fishing data by session&lt;/h2&gt;
&lt;p&gt;I coded a small shiny application that provide a summary of the tide and river flow conditions, the lure changes and catches for each session. Don’t hesitate to explore my fishing data!&lt;/p&gt;
&lt;div class=&#34;wrap&#34; style=&#34;width: 100%; height: 1500px;
    padding: 0; overflow: hidden;&#34;&gt;
&lt;iframe style=&#34;width: 800px;
    height: 1550px;
    border: 0;
    -ms-transform: scale(0.25);
    -moz-transform: scale(0.25);
    -o-transform: scale(0.25);
    -webkit-transform: scale(0.25);
    transform: scale(0.95);
    -ms-transform-origin: 0 0;
    -moz-transform-origin: 0 0;
    -o-transform-origin: 0 0;
    -webkit-transform-origin: 0 0;
    transform-origin: 0 0;&#34; src=&#34;https://aureliencallens.shinyapps.io/Fishing_analysis/&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;code-of-the-shiny-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code of the shiny application&lt;/h2&gt;
&lt;p&gt;Here is the code of the plotly graphs in the application:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plotly)
library(tidyverse)


#&amp;#39; For the tide plot
#&amp;#39; 
#&amp;#39; @param dat first dataframe with session characteristics
#&amp;#39; @param n_ses the id (number) of the session
#&amp;#39; @param temporal_range number of hours to display (before and after the session)
#&amp;#39; @return A plotly object
plot_tide_ses &amp;lt;- function(dat, n_ses, temporal_range = 4){

  dat_t &amp;lt;- dat %&amp;gt;% 
    filter(Session == n_ses) %&amp;gt;% 
    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))
  dat_tide &amp;lt;- as.data.frame(dat_t$Tide_ts)
  dat_tide$hour &amp;lt;- as.POSIXct(dat_tide$hour, origin = &amp;quot;1970-01-01&amp;quot;)
  dat_tide$Water &amp;lt;- as.numeric(as.character(dat_tide$Water))
  
  plot_ly(data = dat_tide, 
          x = ~ hour, 
          y = ~ Water, 
          mode = &amp;#39;lines&amp;#39;) %&amp;gt;%  
    layout(shapes = list(
      list(type = &amp;#39;line&amp;#39;,
           x0 = as.POSIXct(dat_t$Beg),
           x1 = as.POSIXct(dat_t$Beg),
           y0 = min(dat_tide$Water),
           y1 = max(dat_tide$Water),
           line = list(dash = &amp;#39;dot&amp;#39;, width = 1)),
      list(type = &amp;#39;line&amp;#39;,
           x0 =  as.POSIXct(dat_t$End),
           x1 = as.POSIXct(dat_t$End),
           y0 = min(dat_tide$Water),
           y1 = max(dat_tide$Water),
           line = list(dash = &amp;#39;dot&amp;#39;, width = 1))),
      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*temporal_range ,
                                        as.POSIXct(dat_t$End) + 3600*temporal_range )),
                   title = &amp;quot;&amp;quot;),
      yaxis = list(title = &amp;quot;Tide level&amp;quot;))
}

#&amp;#39; For the river flow plot
#&amp;#39; 
#&amp;#39; @param dat first dataframe with session characteristics
#&amp;#39; @param n_ses the id (number) of the session
#&amp;#39; @param past_days number of previous to display (before the session)
#&amp;#39; @return A plotly object
plot_flow_ses &amp;lt;- function(dat, n_ses, past_days = 4){
  dat_t &amp;lt;- dat %&amp;gt;% 
    filter(Session == n_ses) %&amp;gt;% 
    mutate(Flow_ts = list(eval(parse(text = Ts_flow))))
  
  dat_flow &amp;lt;- as.data.frame(dat_t$Flow_ts)
  dat_flow$Date &amp;lt;- as.POSIXct(dat_flow$Date, origin = &amp;quot;1970-01-01&amp;quot;)
  dat_flow$Nive &amp;lt;- as.numeric(as.character(dat_flow$Nive))
  dat_flow$Adour &amp;lt;- as.numeric(as.character(dat_flow$Adour))
  
  
  dat_flow &amp;lt;- dat_flow %&amp;gt;% 
    pivot_longer(cols = c(Nive, Adour), 
                 names_to = &amp;quot;River&amp;quot;,
                 values_to = &amp;quot;Flow&amp;quot;)
  
  plot_ly(data = dat_flow, 
          x = ~ Date,
          y = ~ Flow, 
          color = ~ River, 
          mode = &amp;#39;lines&amp;#39;) %&amp;gt;%  
    layout(shapes = list(
      list(type=&amp;#39;line&amp;#39;,
           x0 = as.POSIXct(dat_t$Beg),
           x1 = as.POSIXct(dat_t$Beg),
           y0 = min(dat_flow$Flow),
           y1 = max(dat_flow$Flow),
           line = list(dash = &amp;#39;dot&amp;#39;, width = 1))),
      xaxis = list(range = as.POSIXct(c(as.POSIXct(dat_t$Beg) - 3600*24*past_days,
                                        as.POSIXct(dat_t$End) )),
                   title = &amp;quot;&amp;quot;))
}

#&amp;#39; Function to prepare the dataset for the plot of lure change and catch
#&amp;#39; 
#&amp;#39; @param lure third dataframe with lure changes (hours) and characteristics
#&amp;#39; @param session first dataframe with session characteristics
#&amp;#39; @param ses_n the id (number) of the session
#&amp;#39; @return A dataframe
start_end_fonction &amp;lt;- function(lure, session, ses_n){
  dat_ses &amp;lt;- session %&amp;gt;% 
    filter(Session == ses_n)
  
  dat_lure &amp;lt;- lure %&amp;gt;% 
    filter(n_ses == ses_n)
  
  startdates &amp;lt;- dat_lure$time
  enddates &amp;lt;- c(startdates[-1], dat_ses$End)
  
  data.frame(change = length(startdates):1, 
             start = as.POSIXct(startdates),
             end = as.POSIXct(enddates),
             type = dat_lure$type_lure,
             text = paste(dat_lure$color_lure, dat_lure$length_lure))
}

#&amp;#39; For the plot of lure change and catch
#&amp;#39; 
#&amp;#39; @param lure third dataframe with lure changes (hours) and characteristics
#&amp;#39; @param caught second dataframe with fish caught characteristics
#&amp;#39; @param session first dataframe with session characteristics
#&amp;#39; @param n_ses the id (number) of the session
#&amp;#39; @return A plotly object
lure_change &amp;lt;- function(lure, caught, dat, n_ses){
  
  df &amp;lt;- start_end_fonction(lure, dat, n_ses)
  
  catch &amp;lt;- caught %&amp;gt;% 
    filter(n_ses == n_ses)
  
  dat_t &amp;lt;- dat %&amp;gt;% 
    filter(Session == n_ses) %&amp;gt;% 
    mutate(Tide_ts = list(eval(parse(text = Ts_tide))))
  dat_tide &amp;lt;- as.data.frame(dat_t$Tide_ts)
  dat_tide$hour &amp;lt;- as.POSIXct(dat_tide$hour, origin = &amp;quot;1970-01-01&amp;quot;)
  dat_tide$Water &amp;lt;- as.numeric(as.character(dat_tide$Water))
  
  plot_ly() %&amp;gt;% 
    add_segments(data = df,
                 x = ~ start,
                 xend = ~ end,
                 y = ~ change,
                 yend = ~ change,
                 color = ~ type,
                 #text = ~ text,
                 size = I(5),
                 alpha = 0.8) %&amp;gt;%
    add_segments(x = as.POSIXct(catch$time),
                 xend = as.POSIXct(catch$time),
                 y = min(df$change),
                 yend = max(df$change),
                 line = list(color = &amp;quot;red&amp;quot;, dash = &amp;quot;dash&amp;quot;),
                 name = &amp;#39;Fish caught&amp;#39;) %&amp;gt;%
    add_trace(data = dat_tide, 
              x = ~ hour, 
              y = ~ Water, 
              mode = &amp;#39;lines&amp;#39;, 
              yaxis = &amp;quot;y2&amp;quot;,
              name = &amp;quot;Water level&amp;quot;,
              alpha = 0.4,
              hoverinfo = &amp;#39;skip&amp;#39;
    ) %&amp;gt;% 
    layout(xaxis = list(range = c(df$start[1] - 1000 , df$end[nrow(df)] + 1000),
                        title = &amp;quot;&amp;quot;),
           yaxis = list(title = &amp;quot;&amp;quot;, zeroline = FALSE, showline = FALSE,
                        showticklabels = FALSE, showgrid = FALSE ),
           yaxis2 = list(overlaying = &amp;quot;y&amp;quot;, side = &amp;quot;right&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the code of this simple yet informative application:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)
library(shinyWidgets)
library(shinydashboard)
library(plotly)
library(tidyverse)
source(&amp;#39;plot_functions.R&amp;#39;)
dat &amp;lt;- read_csv(&amp;quot;session1.csv&amp;quot;)
caught &amp;lt;- read_csv(&amp;quot;catch1.csv&amp;quot;)
lure &amp;lt;- read_csv(&amp;quot;lure.csv&amp;quot;)

# In order to save the tide and flow time series I parse the data in the dataframe
# The following line is used to transform the parsed text into usable values
dat_t &amp;lt;- dat %&amp;gt;% 
  mutate(Tide_ts = list(eval(parse(text = Ts_tide))),
         Flow_ts = list(eval(parse(text = Ts_flow))))

body &amp;lt;- dashboardBody(fluidPage(
  # Application title
  h1(&amp;quot;Exploratory analysis of fishing data&amp;quot;,
  align = &amp;quot;center&amp;quot;,
  style = &amp;quot;padding: 40px;  text-align: center;  background: #605ca8;  color: white;  font-size: 40px;&amp;quot;),
  br(),
  # Dropdown menu to select the fishing session
  fluidRow(align = &amp;quot;center&amp;quot;,
           pickerInput(inputId = &amp;#39;Ses&amp;#39;,
                       label = h3(&amp;#39;Select a fishing session:&amp;#39;),
                       choices = unique(dat$Session[-1]),
                       options = list(
                         style = &amp;quot;btn-primary&amp;quot;),
                       choicesOpt = list(
                         style = rep_len(&amp;quot;font-size: 75%; line-height: 1.6;&amp;quot;, 4)
                       ))),
  br(),
  br(),
  # Key figures of the session
  fluidRow(
    valueBoxOutput(&amp;quot;progressD&amp;quot;, width = 4),
    valueBoxOutput(&amp;quot;progressF&amp;quot;, width = 4),
    valueBoxOutput(&amp;quot;progressL&amp;quot;, width = 4)),
  br(),
  
  br(),
  # Graphs of the tide and river flow of recent days
  fluidRow(
    box(title = &amp;quot;Tidal water level&amp;quot;, status = &amp;quot;primary&amp;quot;, 
        plotlyOutput(&amp;quot;TidePlot&amp;quot;), width = 6),
    box(title = &amp;quot;River flow&amp;quot;, status = &amp;quot;primary&amp;quot;,
        plotlyOutput(&amp;quot;FlowPlot&amp;quot;), width = 6)),
  br(),
  # Graph lure changes during the session + catch
  fluidRow(
    box(title = &amp;quot;Lures tested and fish capture&amp;quot;, status = &amp;quot;warning&amp;quot;, 
        plotlyOutput(&amp;quot;LurePlot&amp;quot;), width=12))
))

ui &amp;lt;- dashboardPage(
  
  dashboardHeader(disable = TRUE),
  
  dashboardSidebar(disable = TRUE),
  
  body
)


# Define server logic required to draw a histogram
server &amp;lt;- function(input, output) {
  
  # Duration
  output$progressD &amp;lt;- renderValueBox({
    Duration = as.integer(difftime(as.POSIXct(dat$End[dat$Session == input$Ses]), as.POSIXct(dat$Beg[dat$Session == input$Ses]), units = &amp;#39;mins&amp;#39;))
    valueBox(tags$p(&amp;quot;Duration&amp;quot;, style = &amp;quot;font-size: 80%;&amp;quot;),
             tags$p(paste(Duration, &amp;quot;min&amp;quot;), style = &amp;quot;font-size: 150%; font-weight: bold;&amp;quot;),
             icon = icon(&amp;quot;clock&amp;quot;), color = &amp;quot;purple&amp;quot;)
  })
  
  # Number of fish
  
  output$progressF &amp;lt;- renderValueBox({
    fish_caught = as.integer(caught %&amp;gt;% filter(n_ses == input$Ses) %&amp;gt;% nrow())
    valueBox(tags$p(&amp;quot;Fish caught&amp;quot;, style = &amp;quot;font-size: 80%;&amp;quot;), tags$p(fish_caught, style = &amp;quot;font-size: 150%;font-weight: bold;&amp;quot;),
             icon = icon(&amp;quot;trophy&amp;quot;), color = &amp;quot;purple&amp;quot;)
  })
  
  # Number of lures tried
  
  output$progressL &amp;lt;- renderValueBox({
    Lure = as.integer(lure %&amp;gt;% filter(n_ses == input$Ses) %&amp;gt;% nrow())
    valueBox(tags$p(&amp;quot;Lure tried&amp;quot;, style = &amp;quot;font-size: 80%;&amp;quot;), tags$p(Lure, style = &amp;quot;font-size: 150%;font-weight: bold;&amp;quot;),
             icon = icon(&amp;quot;fish&amp;quot;), color = &amp;quot;purple&amp;quot;)
  })
  
  output$TidePlot &amp;lt;- renderPlotly({
    # generate plot depending on session
    plot_tide_ses(dat, input$Ses, 4)
  })
  output$FlowPlot &amp;lt;- renderPlotly({
    # generate plot depending on session
    plot_flow_ses(dat_t, input$Ses, 4)
  })
  output$LurePlot &amp;lt;- renderPlotly({
    # generate plot depending on session
    lure_change(lure, caught, dat, input$Ses)
  })
  
}

# Run the application 
shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The end of my PhD journey</title>
      <link>https://aureliencallens.github.io/2021/10/02/the-end-of-a-journey/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2021/10/02/the-end-of-a-journey/</guid>
      <description>


&lt;p&gt;Two weeks ago, I defended my thesis entitled ‘Statistical learning for coastal risks assessment’ in front of an academic jury. The defense went extremely well and the jury and the auditory were genuinely interested by the work I had done during my thesis. This thesis was an amazing and enriching journey in terms of knowledge and research, but also in terms of collaboration and exchange. I now am excited to start a new journey and discover new subjects to work on !&lt;/p&gt;
&lt;p&gt;You can find my thesis manuscript here:
&lt;a href=&#34;http://www.theses.fr/2021PAUU3016&#34; target=&#34;_blank&#34;&gt; http://www.theses.fr/2021PAUU3016&lt;/a&gt;. If you don’t have the courage to read the entire manuscript (I understand :wink:) you can find below a short abstract of my thesis and the slides I presented during my PhD defense.&lt;/p&gt;
&lt;div id=&#34;abstract-of-my-thesis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract of my thesis&lt;/h2&gt;
&lt;p&gt;Over the last decades, the quantity of data related to coastal risk has greatly increased with the installation of numerous monitoring networks. In this era of big data, the use of statistical learning methods (SLM) in the development of local predictive models becomes more legitimate and justified. The objective of this thesis is to demonstrate how SLM can contribute to the improvement of coastal risk assessment tools and to the development of an early warning system which aims to reduce coastal flooding risk.&lt;/p&gt;
&lt;p&gt;Three methodologies have been developed and tested on real study sites. The first methodology aims to improve the local wave forecast made by spectral wave model with machine learning methods and data from monitoring networks. We showed that data assimilation with machine learning methods improve significantly the forecast of wave parameters especially the wave height and period. The second methodology concerns the creation of storm impact databases. Even though these databases are essential for the disaster risk reduction process they are rare and sparse. We therefore proposed a methodology based on a deep learning method (convolutional neural networks) to generate automatically qualitative storm impact data from images provided by video monitoring stations installed on the coast. The last methodology is about the development of a storm impact model with a statistical method (bayesian network) based exclusively on data acquired with diverse monitoring networks. With this methodology we were able to predict qualitatively the storm impact on our study site, the Grande Plage of Biarritz.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://aureliencallens.github.io/img_post/thesis_organization.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Organization of my PhD manuscript&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;my-presentation-for-the-phd-defense&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My presentation for the PhD defense&lt;/h2&gt;









    
    &lt;link href=&#34;https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css&#34; rel=&#34;stylesheet&#34;&gt;



    
    &lt;script src=&#34;https://code.jquery.com/jquery-3.2.1.slim.min.js&#34; crossorigin=&#34;anonymous&#34;&gt;&lt;/script&gt;



&lt;style&gt;
#slide-window {
    position: relative;
    width: 100%;
    height: 550px;
    overflow: hidden;
    top: 0px;
    left: 0px;
}

#slides-list {
    width: 100%;
    height: 550px;
    position: absolute;
    margin: 0px;
    padding: 0px;
    -webkit-transform: translate3d(0px, 0px, 0px);
    transform: translate3d(0px, 0px, 0px);
    transition: all 0.66s ease;
    -webkit-transition: all 0.66s ease;
}

.slide {
    list-style: none;
    position: relative;
    float: left;
    margin: 0;
    padding: 0;
    width: 100%;
    height: 550px;
    background: #ccc;
    text-align: center;
    line-height: 100%;
    background-size: cover;
    background-position: 50% 50%;
    color: #fff;
    -webkit-transform: translate3d(0px, 0px, 0px);
    -webkit-transform-style: preserve-3d;
    transform: scale(0.9, 0.9);
    -ms-transform: scale(0.9, 0.9);
    -webkit-transform: scale(0.9, 0.9);
}

.nav {
    position: relative;
    z-index: 9;
    top: 45%;
    cursor: pointer;
    color: #EAEAEA;
    opacity: 0.7;
    transition: all 0.66s ease;
    -webkit-transition: all 0.66s ease;
}

.nav:hover {
    opacity: 1.0;
}

#left {
    left: 3%;
    float: left;
}

#right {
    right: 3%;
    float: right;
}
&lt;/style&gt;


&lt;div id=&#34;slide-window&#34;&gt;
    &lt;ol id=&#34;slides-list&#34;&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_01.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_02.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_03.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_04.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_05.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_06.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_07.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_08.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_09.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_10.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_11.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_12.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_13.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_14.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_15.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_16.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_17.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_18.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_19.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_20.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_21.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_22.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_23.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_24.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_25.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_26.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_27.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_28.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_29.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_30.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_31.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_32.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_33.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_34.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_35.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_36.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_37.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_38.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_39.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_40.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_41.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_42.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_43.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_44.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_45.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_46.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_47.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_48.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_49.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_50.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_51.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_52.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_53.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
                &lt;li class=&#34;slide&#34; style=&#34;background:url(https://aureliencallens.github.io/Slides/Slide_54.jpeg) no-repeat ;background-size:contain&#34;&gt;&lt;/li&gt;
    &lt;/ol&gt;
    &lt;span class=&#34;nav fa fa-chevron-left fa-3x&#34; id=&#34;left&#34;&gt;&lt;/span&gt;
    &lt;span class=&#34;nav fa fa-chevron-right fa-3x&#34; id=&#34;right&#34;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;script&gt;
sliderJQuery = jQuery.noConflict();
sliderJQuery(function( $ ) {
    $.global = new Object();
    $.global.total = 0;

    $(document).ready(function () {
        var slideWindowWidth = $(&#39;#slide-window&#39;).width();
        var slideCount = $(&#39;#slides-list li&#39;).length;
        var totalSlidesWidth = slideCount * slideWindowWidth;

        $.global.item = 0;
        $.global.total = slideCount;

        $(&#39;.slide&#39;).css(&#39;width&#39;, slideWindowWidth + &#39;px&#39;);
        $(&#39;#slides-list&#39;).css(&#39;width&#39;, totalSlidesWidth + &#39;px&#39;);

        $(&#39;#left&#39;).click(function () {
            resetAutoSlide();
            performSlide(&#39;back&#39;);
        });

        $(&#39;#right&#39;).click(function () {
            resetAutoSlide();
            performSlide(&#39;forward&#39;);
        });

    });

    function performSlide(direction) {
        if (direction == &#39;back&#39;) {
            var nextSlideId = $.global.item - 1;
        }
        if (direction == &#39;forward&#39;) {
            var nextSlideId = $.global.item + 1;
        }

        if (nextSlideId == -1) {
             
            moveCss($.global.total - 1);
        } else if (nextSlideId == $.global.total) {
             
            moveCss(0);
        } else {
             
            moveCss(nextSlideId);
        }
    }

    function moveCss(nextSlideId) {
        var slideWindowWidth = $(&#39;#slide-window&#39;).width();
        var margin = slideWindowWidth * nextSlideId;

        $(&#39;#slides-list&#39;).css(&#39;transform&#39;, &#39;translate3d(-&#39; + margin + &#39;px,0px,0px)&#39;);

        $.global.item = nextSlideId;
    }

    
      var autoSlide = parseInt(&#34;0&#34;, 10);
      var autoSlideInterval;
      function resetAutoSlide(){
        if(autoSlide) {
          if(autoSlideInterval) {
            clearInterval(autoSlideInterval);
          }
          autoSlideInterval = setInterval(function(){
            performSlide(&#39;forward&#39;);
          }, autoSlide)
        }
      }
      resetAutoSlide();
});
&lt;/script&gt;

&lt;/div&gt;
&lt;div id=&#34;appendix-how-to-make-this-slide-show-from-a-pdf-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix: How to make this slide show from a pdf file&lt;/h2&gt;
&lt;p&gt;The first step is to transform the presentation file (pdf format) into a collection of images (jpeg format) inside a specific folder with the R package &lt;em&gt;pdftools&lt;/em&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(pdftools)
pdf_convert(&amp;quot;filename.pdf&amp;quot;, format = &amp;quot;jpeg&amp;quot;, pages = NULL, filenames = NULL, dpi = 300, opw = &amp;quot;&amp;quot;, upw = &amp;quot;&amp;quot;, verbose = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we add the short code “gallery-slider.html” presented in this &lt;a href=&#34;https://github.com/tbiering/hugo-slider-shortcode&#34;&gt;github repository&lt;/a&gt; in this folder: &lt;code&gt;Website_Folder/themes/beautifulhugo/layouts/shortcodes/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Finally we include the gallery slider our post with the following line of code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt;/* gallery-slider dir=&amp;quot;Slides/&amp;quot; width=&amp;quot;100%&amp;quot; height=&amp;quot;500px&amp;quot; */&amp;gt;}}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The directory containing the images must be placed in the &lt;code&gt;Website_Folder/static/&lt;/code&gt; folder.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can R and Shiny make me a better fisherman? Part 3</title>
      <link>https://aureliencallens.github.io/2021/06/01/r-shiny-fishing-part3/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2021/06/01/r-shiny-fishing-part3/</guid>
      <description>


&lt;p&gt;In this &lt;a href=&#34;https://aureliencallens.github.io/2020/09/12/r-shiny-fishing-part1/&#34;&gt;previous post&lt;/a&gt;, I presented the shiny application I developed to record data about my fishing session. In today’s post, I will present briefly the changes and updates I made to improve the application. Here are the main changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Weather API&lt;/strong&gt;: the API I was using (Dark Sky) stopped furnishing free data. I changed for &lt;a href=&#34;https://openweathermap.org/&#34;&gt;Openweathermap&lt;/a&gt; and I needed to update the functions to gather the same information as before.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;River flow&lt;/strong&gt;: the first version of the application did not collect any data about the flow of the river in which I am fishing. However I am convinced that the river flow before the fishing session might have an impact on the presence of seabass. I therefore created a web scrapping function that collect the flow of the Nive and the Adour (main fishing rivers).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Collecting lure data&lt;/strong&gt;: in the first version, data about fishing lures I used were only collected when a fish was caught. However I did not have data about how long I used the lure before catching a fish. The new version of the application now collects data about lure and how often I change lure.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;weather-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Weather API&lt;/h2&gt;
&lt;p&gt;Small changes were made to adapt the former weather function to the new weather API. As this new API do not furnish moon phase data, I decided to compute the moon phase with the &lt;em&gt;oce&lt;/em&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(httr)
library(jsonlite)
library(tidyverse)
library(rvest)
library(oce)

weather &amp;lt;- function(lat, lon, API_key){
  url &amp;lt;- paste0(&amp;quot;api.openweathermap.org/data/2.5/weather?lat=&amp;quot;, lat, &amp;quot;&amp;amp;lon=&amp;quot;, lon, &amp;quot;&amp;amp;appid=&amp;quot;, API_key, &amp;quot;&amp;amp;units=metric&amp;quot;)
  
  rep &amp;lt;- GET(url)
  
  table &amp;lt;- fromJSON(content(rep, &amp;quot;text&amp;quot;))
  
  # The weather API don&amp;#39;t provide moon phase so I compute it with Oce package
  moon_phase &amp;lt;- round(moonAngle(t = Sys.Date(),
                                longitude = as.numeric(lon),
                                latitude = as.numeric(lat))$illuminatedFraction,
                      3)
  
  
  current.weather.info &amp;lt;- data.frame(Air_temp = table$main$temp,
                                     Weather = table$weather$main,
                                     Atm_pres = table$main$pressure,
                                     Wind_str = table$wind$speed,
                                     Wind_dir = table$wind$deg,
                                     Cloud_cover = table$clouds$all,
                                     PrecipInt = ifelse(is.null(table$rains$`1h`), 0, table$rains$`1h`),  
                                     Moon = moon_phase)
  return(current.weather.info)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;river-flow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;River flow&lt;/h2&gt;
&lt;p&gt;I wrote functions to scrap information about the flows of the rivers in which I fish the most on a french website:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get and prepare the flow data
get_Qdata &amp;lt;- function(link){
  table &amp;lt;- fromJSON(content(GET(link), &amp;quot;text&amp;quot;))
  table &amp;lt;- table$Serie$ObssHydro
  table &amp;lt;- as.data.frame(table)
  table$DtObsHydro &amp;lt;- sub(&amp;quot;T&amp;quot;, &amp;quot; &amp;quot;, table$DtObsHydro)
  table$DtObsHydro &amp;lt;- substr(table$DtObsHydro, start = 1, stop = 19)
  ts &amp;lt;- data.frame(Date = seq.POSIXt(as.POSIXct(range(table$DtObsHydro)[1],&amp;#39;%m/%d/%y %H:%M:%S&amp;#39;), 
                                     as.POSIXct(range(table$DtObsHydro)[2],&amp;#39;%m/%d/%y %H:%M:%S&amp;#39;), by=&amp;quot;hour&amp;quot;))
  
  table$DtObsHydro &amp;lt;- as.POSIXct(table$DtObsHydro, format = &amp;quot;%Y-%m-%d %H:%M:%S&amp;quot;)
  
  table &amp;lt;- full_join(table, ts, by = c(&amp;quot;DtObsHydro&amp;quot; = &amp;quot;Date&amp;quot;)) %&amp;gt;% arrange(DtObsHydro)
  return(table)
}

# Main function to collect river flow 

river_flow &amp;lt;- function(){
  # Url of website to scrap:
  url_index &amp;lt;- &amp;quot;https://www.vigicrues.gouv.fr/services/station.json/index.php&amp;quot;
  
  rep &amp;lt;- GET(url_index)
  
  table_index &amp;lt;- fromJSON(content(rep, &amp;quot;text&amp;quot;))$Stations%&amp;gt;% 
    na.omit()
  
  # I need to add the flow of several rivers to get the flow of the rivers I am interested in:
  stations &amp;lt;- table_index %&amp;gt;% 
    filter(LbStationHydro %in% c(&amp;quot;Pontonx-sur-l&amp;#39;Adour&amp;quot;, &amp;quot;St-Pandelon&amp;quot;, &amp;quot;Artiguelouve&amp;quot;, &amp;quot;Escos&amp;quot;,
                                 &amp;quot;Aïcirits [St-Palais]&amp;quot;, &amp;quot;Cambo-les-Bains&amp;quot;))
  
  base_url &amp;lt;- &amp;quot;http://www.vigicrues.gouv.fr/services/observations.json?CdStationHydro=&amp;quot;
  height_url &amp;lt;- &amp;quot;&amp;amp;FormatDate=iso&amp;quot;
  Q_url &amp;lt;- &amp;quot;&amp;amp;GrdSerie=Q&amp;quot;
  
  stations &amp;lt;- stations %&amp;gt;% 
    mutate(WL_link = paste0(base_url, CdStationHydro, height_url),
           Q_link = paste0(WL_link, Q_url))
  
  data_Q &amp;lt;- lapply(stations$Q_link, 
                   function(x){get_Qdata(x)})
  
  data_Q &amp;lt;- suppressWarnings(Reduce(function(...) merge(..., all = TRUE, by = &amp;quot;DtObsHydro&amp;quot;),
                   data_Q))
  
  names(data_Q) &amp;lt;- c(&amp;quot;Date&amp;quot;, stations$LbStationHydro) 
  
  data_Q &amp;lt;- data_Q  %&amp;gt;% 
    mutate(hour_of_day = format(Date, &amp;quot;%Y-%m-%d %H&amp;quot;))
  
  
  data_Q &amp;lt;- aggregate(.~hour_of_day, data = data_Q, mean, na.rm = TRUE, na.action = na.pass)
  
  data_Q &amp;lt;- imputeTS::na_interpolation(data_Q, option = &amp;quot;linear&amp;quot;)
  
  final_data &amp;lt;- data_Q %&amp;gt;% 
    mutate(Adour = `Pontonx-sur-l&amp;#39;Adour` +  `Aïcirits [St-Palais]` + Artiguelouve + Escos + `St-Pandelon`,
           Date = as.POSIXct(hour_of_day, tryFormats = &amp;quot;%Y-%m-%d %H&amp;quot;)) %&amp;gt;% 
    select(Date, `Cambo-les-Bains`, Adour) %&amp;gt;% 
    rename(Nive = `Cambo-les-Bains`)
  
  Cur_flow &amp;lt;- data.frame(&amp;quot;Nive_c&amp;quot; = final_data[nrow(final_data), 2],
                         &amp;quot;Adour_c&amp;quot; = final_data[nrow(final_data), 3])
  
  
  final_data &amp;lt;- cbind(Cur_flow, final_data) %&amp;gt;% 
    nest(Ts_flow = c(Date, Nive, Adour)) %&amp;gt;% 
    mutate(Ts_flow = paste(Ts_flow))

  return(final_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny application&lt;/h2&gt;
&lt;p&gt;A simplified graph of the new application is showed below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://aureliencallens.github.io/img_post/graph_newapp.svg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Simplified workflow of the new version of application&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ui-side&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;UI side&lt;/h3&gt;
&lt;p&gt;The UI side did not change that much, I only removed the tab that displayed fishing data on a map because I wasn’t using this feature too much:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries 
library(shiny)
library(shinyWidgets)
library(googlesheets)
library(miniUI)
library(leaflet)
library(rdrop2)
Sys.setenv(TZ=&amp;quot;Europe/Paris&amp;quot;)

#Import the functions for weather API and webscrapping 
suppressMessages(source(&amp;quot;api_functions.R&amp;quot;))


# Load the dropbox token : 
token &amp;lt;&amp;lt;- readRDS(&amp;quot;token.rds&amp;quot;)

# Minipage for small screens
ui &amp;lt;- miniPage(tags$script(&amp;#39;$(document).ready(function () {
                           navigator.geolocation.getCurrentPosition(onSuccess, onError);

                           function onError (err) {
                           Shiny.onInputChange(&amp;quot;geolocation&amp;quot;, false);
                           }

                           function onSuccess (position) {
                           setTimeout(function () {
                           var coords = position.coords;
                           console.log(coords.latitude + &amp;quot;, &amp;quot; + coords.longitude);
                           Shiny.onInputChange(&amp;quot;geolocation&amp;quot;, true);
                           Shiny.onInputChange(&amp;quot;lat&amp;quot;, coords.latitude);
                           Shiny.onInputChange(&amp;quot;long&amp;quot;, coords.longitude);
                           }, 1100)
                           }
                           });&amp;#39;),
               
               gadgetTitleBar(&amp;quot;Catch them all&amp;quot;, left = NULL, right = NULL),
               
               miniTabstripPanel(
                 
                 miniTabPanel(&amp;quot;Session&amp;quot;, icon = icon(&amp;quot;sliders&amp;quot;),
                              
                              miniContentPanel(uiOutput(&amp;quot;UI_sess&amp;quot;, align = &amp;quot;center&amp;quot;),
                                               uiOutput(&amp;quot;UI&amp;quot;, align = &amp;quot;center&amp;quot;))
                              
                 )
               )
               
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;server-side&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Server side&lt;/h3&gt;
&lt;p&gt;Several changes were made in the server side to collect data about the lures I used. Now, each time I change my fishing lure, I fill a small form to collect the lure characteristics and it adds a line in a third csv file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;server &amp;lt;- function(input, output, session){
  
  observeEvent(input$go ,{
    
  # Read the csv file containing information about fishing session. If a session is running,
  # display the UI that allows the user to input data about the fish caught. If a session is not started,
  # display a button to start the session and small survey on lure characteristics.
    
    dat &amp;lt;&amp;lt;- drop_read_csv(&amp;quot;/app_peche/session1.csv&amp;quot;, header = T, stringsAsFactors = F, dtoken = token)
    
    # Reactive UI
    
    output$UI &amp;lt;- renderUI({
      
      if(!is.na(rev(dat$End)[1])){
        # We now indicate what type of lure we use at the beginning of the session:
        tagList(
          selectInput(&amp;quot;lure1&amp;quot;, 
                      label = &amp;quot;Type de leurre&amp;quot;,
                      choices = list(&amp;quot;Shad&amp;quot; = &amp;quot;shad&amp;quot;,
                                     &amp;quot;Slug&amp;quot; = &amp;quot;slug&amp;quot;,
                                     &amp;quot;Jerkbait&amp;quot; = &amp;quot;jerkbait&amp;quot;,
                                     &amp;quot;Casting jig&amp;quot; = &amp;quot;jig&amp;quot;,
                                     &amp;quot;Topwater&amp;quot; = &amp;quot;topwater&amp;quot;),
                      selected = &amp;quot;shad&amp;quot;,
                      selectize = FALSE),
          
          selectInput(&amp;quot;color_lure1&amp;quot;, 
                      label = &amp;quot;Couleur du leurre&amp;quot;,
                      choices = list(&amp;quot;Naturel&amp;quot; = &amp;quot;naturel&amp;quot;,
                                     &amp;quot;Sombre&amp;quot; = &amp;quot;sombre&amp;quot;,
                                     &amp;quot;Clair&amp;quot; = &amp;quot;clair&amp;quot;,
                                     &amp;quot;Flashy&amp;quot; = &amp;quot;flashy&amp;quot; ),
                      selected = &amp;quot;naturel&amp;quot;,
                      selectize = FALSE),
          
          selectInput(&amp;quot;length_lure1&amp;quot;,
                      label = &amp;quot;Taille du leurre&amp;quot;,
                      choices = list(&amp;quot;Petit&amp;quot; = &amp;quot;petit&amp;quot;,
                                     &amp;quot;Moyen&amp;quot; = &amp;quot;moyen&amp;quot;,
                                     &amp;quot;Grand&amp;quot; = &amp;quot;grand&amp;quot;),
                      selected = &amp;quot;petit&amp;quot;,
                      selectize = FALSE),
          
          actionButton(&amp;quot;go&amp;quot;,&amp;quot;Commencer session !&amp;quot;))
      }else{
        
        tagList(actionButton(&amp;quot;go&amp;quot;,&amp;quot;End session&amp;quot;))
      }
      
    })
    
    output$UI_sess &amp;lt;- renderUI({
      
      if(!is.na(rev(dat$End)[1])){
        
        tagList(textInput(&amp;quot;comments&amp;quot;, label = &amp;quot;Commentaire avant le début?&amp;quot;, value = &amp;quot;NA&amp;quot;))
        
      }else{
        input$catch
        input$lure
        tagList(
          
          selectInput(&amp;quot;lure_type&amp;quot;, 
                      label = &amp;quot;Type de leurre&amp;quot;,
                      choices = list(&amp;quot;Shad&amp;quot; = &amp;quot;shad&amp;quot;,
                                     &amp;quot;Slug&amp;quot; = &amp;quot;slug&amp;quot;,
                                     &amp;quot;Jerkbait&amp;quot; = &amp;quot;jerkbait&amp;quot;,
                                     &amp;quot;Casting jig&amp;quot; = &amp;quot;jig&amp;quot;,
                                     &amp;quot;Topwater&amp;quot; = &amp;quot;topwater&amp;quot;),
                      selected = &amp;quot;shad&amp;quot;,
                      selectize = FALSE),
          
          selectInput(&amp;quot;color_lure&amp;quot;, 
                      label = &amp;quot;Couleur du leurre&amp;quot;,
                      choices = list(&amp;quot;Naturel&amp;quot; = &amp;quot;naturel&amp;quot;,
                                     &amp;quot;Sombre&amp;quot; = &amp;quot;sombre&amp;quot;,
                                     &amp;quot;Clair&amp;quot; = &amp;quot;clair&amp;quot;,
                                     &amp;quot;Flashy&amp;quot; = &amp;quot;flashy&amp;quot; ),
                      selected = &amp;quot;naturel&amp;quot;,
                      selectize = FALSE),
          
          selectInput(&amp;quot;length_lure&amp;quot;,
                      label = &amp;quot;Taille du leurre&amp;quot;,
                      choices = list(&amp;quot;Petit&amp;quot; = &amp;quot;petit&amp;quot;,
                                     &amp;quot;Moyen&amp;quot; = &amp;quot;moyen&amp;quot;,
                                     &amp;quot;Grand&amp;quot; = &amp;quot;grand&amp;quot;),
                      selected = &amp;quot;petit&amp;quot;,
                      selectize = FALSE),
          
          actionButton(&amp;quot;lure&amp;quot;,
                       label = &amp;quot;Changer de leurre!&amp;quot;),
          
          br(), 
          br(), 
          
          h4(&amp;quot;Ajouter une capture&amp;quot;),
          
          selectInput(&amp;quot;species&amp;quot;, 
                      label = &amp;quot;Espèces&amp;quot;,
                      choices = list(&amp;quot;Bar&amp;quot; = &amp;quot;bar&amp;quot;,
                                     &amp;quot;Bar moucheté&amp;quot; = &amp;quot;bar_m&amp;quot;,
                                     &amp;quot;Alose&amp;quot; = &amp;quot;alose&amp;quot;,
                                     &amp;quot;Maquereau&amp;quot; = &amp;quot;maquereau&amp;quot;,
                                     &amp;quot;Chinchard&amp;quot; = &amp;quot;chinchard&amp;quot;),
                      selected = &amp;quot;bar&amp;quot;),
          
          sliderInput(&amp;quot;length&amp;quot;,
                      label = &amp;quot;Taille du poisson&amp;quot;,
                      value = 25, 
                      min = 0, 
                      max = 80, 
                      step = 1),
          
          actionButton(&amp;quot;catch&amp;quot;,&amp;quot;Rajoutez cette capture aux stats!&amp;quot;),
          
          br(), 
          br(), 
          
          textInput(&amp;quot;comments1&amp;quot;, label = h4(&amp;quot;Commentaire avant la fin ?&amp;quot;), value = &amp;quot;NA&amp;quot;)
        )
      }
    })
  }, ignoreNULL = F)
  
  
  #If the button is pushed, create the line to be added in the csv file. 
  
  observeEvent(input$go,{
    
    # Two outcomes depending if the session starts or ends. This gives the possibility 
    # to the user to add a comment before starting the session or after ending the session
    
    if(!is.na(rev(dat$End)[1])){
      
      #Tide + geoloc + Weather
      c_tide &amp;lt;- tide()
      geoloc &amp;lt;- c(input$lat,input$long)
      current.weather.info &amp;lt;- weather(lat = geoloc[1], lon = geoloc[2])
      river.flow &amp;lt;- river_flow()
      
      n_ses &amp;lt;- c(rev(dat$Session)[1] + 1)
      time_beg &amp;lt;- as.character(as.POSIXct(Sys.time()))
      comment &amp;lt;- input$comments
      dat.f &amp;lt;&amp;lt;- cbind(data.frame(n_ses,
                                 time_beg,
                                 NA,
                                 geoloc[2],
                                 geoloc[1]),
                      current.weather.info,
                      c_tide,
                      river.flow,
                      comment)
      names(dat.f) &amp;lt;- names(dat)
      print(dat.f)
      final_dat &amp;lt;- rbind(dat, dat.f)
      
      lure &amp;lt;- drop_read_csv(&amp;quot;/app_peche/lure.csv&amp;quot;,
                            header = T,
                            stringsAsFactors = F,
                            dtoken = token)
      
      new_lure &amp;lt;- data.frame(n_ses = n_ses,
                             time = as.character(as.POSIXct(Sys.time())),
                             type_lure = input$lure1,
                             color_lure = input$color_lure1,
                             length_lure = input$length_lure1)
      
      new_df &amp;lt;- rbind(lure, 
                      new_lure)
      
      write_csv(as.data.frame(new_df), &amp;quot;lure.csv&amp;quot;)
      drop_upload(&amp;quot;lure.csv&amp;quot;, path = &amp;quot;App_peche&amp;quot;, mode = &amp;quot;overwrite&amp;quot;, dtoken = token)
      

    }else{
      
      dat$End[nrow(dat)] &amp;lt;- as.character(as.POSIXct(Sys.time()))
      dat$Comments[nrow(dat)] &amp;lt;- paste(dat$Comments[nrow(dat)], &amp;quot;/&amp;quot;, input$comments1)
      final_dat &amp;lt;- dat 
    }
    
    # Write csv in temporary files of shiny server 
    write_csv(as.data.frame(final_dat), &amp;quot;session1.csv&amp;quot;)
    
    # Upload it to dropbox account 
    drop_upload(&amp;quot;session1.csv&amp;quot;, path = &amp;quot;App_peche&amp;quot;, mode = &amp;quot;overwrite&amp;quot;, dtoken = token)
  })
  
  # Add a line to the catch csv file whenever a fish is caught
  observeEvent(input$catch,{
    caugth &amp;lt;- drop_read_csv(&amp;quot;/app_peche/catch1.csv&amp;quot;, header = T, stringsAsFactors = F, dtoken = token)
    
    catch &amp;lt;- data.frame(n_ses = dat$Session[nrow(dat)],
                        time = as.character(as.POSIXct(Sys.time())),
                        species = input$species,
                        length = input$length)
    
    b &amp;lt;- rbind(caugth,catch)
    
    write_csv(as.data.frame(b), &amp;quot;catch1.csv&amp;quot;)
    drop_upload(&amp;quot;catch1.csv&amp;quot;, path = &amp;quot;App_peche&amp;quot;, mode = &amp;quot;overwrite&amp;quot;, dtoken = token)
  })
  
  
  observeEvent(input$lure,{
    lure &amp;lt;- drop_read_csv(&amp;quot;/app_peche/lure.csv&amp;quot;,
                          header = T,
                          stringsAsFactors = F,
                          dtoken = token)
    
    new_lure &amp;lt;- data.frame(n_ses = dat$Session[nrow(dat)],
                        time = as.character(as.POSIXct(Sys.time())),
                        type_lure = input$lure_type,
                        color_lure = input$color_lure,
                        length_lure = input$length_lure)
    
    new_df &amp;lt;- rbind(lure, 
               new_lure)
    
    write_csv(as.data.frame(new_df), &amp;quot;lure.csv&amp;quot;)
    drop_upload(&amp;quot;lure.csv&amp;quot;, path = &amp;quot;App_peche&amp;quot;, mode = &amp;quot;overwrite&amp;quot;, dtoken = token)
  })
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I have tested this new application during two fishing sessions and it has been working like a charm. I can’t wait to present you my findings at the end of this fishing season !&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Coastal risks and statistical learning: Analyzing Google trends with gtrendsR package</title>
      <link>https://aureliencallens.github.io/2021/03/03/coastal-risks-google-trends/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2021/03/03/coastal-risks-google-trends/</guid>
      <description>


&lt;p&gt;I am currently in the third year of my PhD and it is time for me to synthesize all the work I have done by writing my thesis. One important step is the introduction which presents the general context and the contributions of my thesis in the research world. For my introduction, I decided to include the Google trend analysis of specific terms related to my PhD subject. For information, the aim of my PhD is to demonstrate the potential contributions of statistical learning methods in the study of coastal risks.&lt;/p&gt;
&lt;p&gt;In this post, we are going to use the R package &lt;code&gt;gtrendsR&lt;/code&gt; to analyze the trends of specific words related to statistical learning such as “machine learning” or “data science” and also related to coastal risks: “coastal flood”, “storm surge”.&lt;/p&gt;
&lt;div id=&#34;analysis-of-specific-terms-for-the-whole-world&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis of specific terms for the whole world&lt;/h2&gt;
&lt;p&gt;In this first section, we are going to analyze the interest over time for several terms in the whole world. The &lt;code&gt;gtrends&lt;/code&gt; function extracts the “hits” variable for a given term. This variable is a normalized measure of the interest made by Google Trends. The number of search is normalized by geography and time range, then the resulting numbers are scaled on a range of 0 to 100 based on a topic’s proportion to all searches on all topics. All the details are given in the support of Google Trends (&lt;a href=&#34;https://support.google.com/trends/answer/4365533?hl=en&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We can look at the interest over time for “data science”, “deep learning” and “machine learning”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(gtrendsR)

words_stats &amp;lt;- c(&amp;quot;machine learning&amp;quot;,
           &amp;quot;deep learning&amp;quot;,
           &amp;quot;data science&amp;quot;)

res_stats &amp;lt;- lapply(words_stats, function(x){
  gtrends(x,
          geo = &amp;quot;&amp;quot;,
          time = &amp;quot;all&amp;quot;)})


interest_stats &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, 
                    lapply(res_stats, function(x){x$interest_over_time}))

interest_stats %&amp;gt;% 
  mutate(hits = as.numeric(ifelse(hits == &amp;quot;&amp;lt;1&amp;quot;, 0, hits))) %&amp;gt;% 
  ggplot(aes(x = date, y = hits)) + 
  geom_line() + 
  facet_grid(~ keyword) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2021-03-01-coastal-risks-google-trends.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this figure, we see the increasing interest for these terms over the last decade.&lt;/p&gt;
&lt;p&gt;We can do the same for terms related to coastal risks :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words_coastal &amp;lt;- c(&amp;quot;coastal risk&amp;quot;,
                   &amp;quot;coastal flood&amp;quot;,
                   &amp;quot;storm surge&amp;quot;)


res_coast &amp;lt;- lapply(words_coastal, function(x){
  gtrends(x,
          geo = &amp;quot;&amp;quot;,
          time = &amp;quot;all&amp;quot;)})


interest_coast &amp;lt;- do.call(&amp;quot;rbind&amp;quot;, lapply(res_coast, function(x){x$interest_over_time}))

interest_coast %&amp;gt;% 
  mutate(hits = as.numeric(ifelse(hits == &amp;quot;&amp;lt;1&amp;quot;, 0, hits))) %&amp;gt;% 
  ggplot(aes(x = date, y = hits)) + 
  geom_line() + 
  facet_grid(~ keyword) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2021-03-01-coastal-risks-google-trends.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Contrary to the interest for the statistical terms, the interest in terms related to coastal risk is more punctual. One of my hypothesis is that the interest in these topic increases after a large storm event has occurred in the world. We will investigate on this hypothesis at the end of the post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mapping-the-interest-of-storm-surge-and-data-science-in-the-world&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mapping the interest of “storm surge” and “data science” in the world&lt;/h2&gt;
&lt;p&gt;In addition to analyze the trend over time with &lt;code&gt;gtrendsR&lt;/code&gt;, we can also visualize the interest depending on the countries.&lt;/p&gt;
&lt;p&gt;Let’s start with “storm surge”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_world &amp;lt;- gtrends(&amp;quot;Storm surge&amp;quot;,
          geo = &amp;quot;&amp;quot;,
          time = &amp;quot;all&amp;quot;)


Int_country &amp;lt;- as_tibble(res_world$interest_by_country)

Int_country &amp;lt;- Int_country %&amp;gt;% 
  dplyr::rename(region = location)

WolrdMap = ggplot2::map_data(map = &amp;quot;world&amp;quot;)


Int_Merged &amp;lt;- Int_country %&amp;gt;% 
  dplyr::full_join(x = ., 
                   y = WolrdMap , 
                   by = &amp;quot;region&amp;quot;) %&amp;gt;% 
  mutate(hits = as.numeric(hits))


Int_Merged %&amp;gt;% 
  ggplot(aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, 
                   fill = hits), 
               colour = &amp;quot;grey40&amp;quot;,
               size = 0.2) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(7, &amp;quot;OrRd&amp;quot;),
                       na.value = &amp;quot;white&amp;quot;) +
  coord_cartesian(ylim = c(-55, 85)) + 
  theme(legend.position=&amp;quot;bottom&amp;quot;) +
  theme_bw() + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;) + 
  theme(panel.grid.major = element_line(size = 0.5, linetype = 2),
        panel.grid.minor = element_blank()) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2021-03-01-coastal-risks-google-trends.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see from this map that the number of Google searches for this term is quite low for many countries in comparison with other terms (white color means not enough search comparing to the total number of search). In general the interest is higher in countries that are more likely to be impacted by marine storms or hurricanes. The Republic of Philippines seems the most interested country in storm surge.&lt;/p&gt;
&lt;p&gt;We can then continue with the term “data science”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_world &amp;lt;- gtrends(&amp;quot;Data Science&amp;quot;,
          geo = &amp;quot;&amp;quot;,
          time = &amp;quot;all&amp;quot;)


Int_country &amp;lt;- as_tibble(res_world$interest_by_country)

Int_country &amp;lt;- Int_country %&amp;gt;% 
  dplyr::rename(region = location)

WolrdMap = ggplot2::map_data(map = &amp;quot;world&amp;quot;)


Int_Merged &amp;lt;- Int_country %&amp;gt;% 
  dplyr::full_join(x = ., 
                   y = WolrdMap , 
                   by = &amp;quot;region&amp;quot;) %&amp;gt;% 
  mutate(hits = as.numeric(hits))


Int_Merged %&amp;gt;% 
  ggplot(aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, 
                   fill = hits), 
               colour = &amp;quot;grey40&amp;quot;,
               size = 0.2) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(7, &amp;quot;OrRd&amp;quot;),
                       na.value = &amp;quot;white&amp;quot;) +
  coord_cartesian(ylim = c(-55, 85)) + 
  theme(legend.position=&amp;quot;bottom&amp;quot;) +
  theme_bw() + 
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;) + 
  theme(panel.grid.major = element_line(size = 0.5, linetype = 2),
        panel.grid.minor = element_blank()) +
  labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2021-03-01-coastal-risks-google-trends.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much more countries are interested by this topic with India being the country with the highest number of Google searchs about data science.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;focusing-on-the-trend-of-storm-surge-in-france&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Focusing on the trend of “storm surge” in France&lt;/h2&gt;
&lt;p&gt;Earlier in this post, I made the hypothesis that the number of Google search for terms like “storm surge” was related to the recent occurrence of a storm event. Let’s investigate this hypothesis for the term “Submersion marine” (marine submersion/ flood in English) in France.&lt;/p&gt;
&lt;p&gt;First, we extract the number of hits on this term for France by indicating &lt;code&gt;geo = &#34;FR&#34;&lt;/code&gt; in the &lt;code&gt;gtrends&lt;/code&gt; function. Then we create a data frame where we gather some severe storms that impacted France (&lt;a href=&#34;http://tempetes.meteo.fr/&#34; target=&#34;_blank&#34;&gt;Website listing the severe storms in France&lt;/a&gt;). Finally, we join all the information on the same graph (red dashed lines represent storm events):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res_fr &amp;lt;- gtrends(&amp;quot;submersion marine&amp;quot;,
          geo = &amp;quot;FR&amp;quot;,
          time = &amp;quot;all&amp;quot;)

df_storm &amp;lt;- as.data.frame(matrix(c(
  &amp;quot;2009-01-24 GMT&amp;quot;, &amp;quot;Klaus&amp;quot;,
  &amp;quot;2010-02-27 GMT&amp;quot;, &amp;quot;Xynthia&amp;quot;,
  &amp;quot;2011-12-16 GMT&amp;quot;, &amp;quot;Joachim&amp;quot;,
  &amp;quot;2014-02-14 GMT&amp;quot;, &amp;quot;Ulla&amp;quot;,
  &amp;quot;2014-11-04 GMT&amp;quot;, &amp;quot;Qendresa&amp;quot;,
  &amp;quot;2016-02-10 GMT&amp;quot;, &amp;quot;No name&amp;quot;,
  &amp;quot;2017-03-06 GMT&amp;quot;, &amp;quot;Zeus&amp;quot;,
  &amp;quot;2018-01-03 GMT&amp;quot;, &amp;quot;Eleanor&amp;quot;,
  &amp;quot;2019-12-13 GMT&amp;quot;, &amp;quot;No name&amp;quot;
  ), ncol = 2, byrow = T), stringsAsFactors = F)


names(df_storm) &amp;lt;- c(&amp;quot;date&amp;quot;, &amp;quot;Name of storm&amp;quot;)
df_storm$date &amp;lt;- as.POSIXct(df_storm$date, tz = &amp;quot;GMT&amp;quot;)


res_fr$interest_over_time %&amp;gt;% 
  mutate(hits = as.numeric(ifelse(hits == &amp;quot;&amp;lt;1&amp;quot;, 0, hits))) %&amp;gt;% 
  filter(date &amp;gt; as.POSIXct(&amp;quot;2006-01-24 GMT&amp;quot;, tz = &amp;quot;GMT&amp;quot;)) %&amp;gt;% 
  ggplot(aes(x = date, y = hits)) + 
  geom_line() +
  geom_vline(xintercept = df_storm$date, col = &amp;quot;red&amp;quot;, lty = 2, alpha = 0.5, lwd = 1) + 
  geom_label(aes(x=date, y = seq(38, 25, length.out=nrow(df_storm)), label =`Name of storm`), data = df_storm) +
  #geom_vline(xintercept = as.POSIXct(&amp;quot;2011-07-11 GMT&amp;quot;, tz = &amp;quot;GMT&amp;quot;), col = &amp;quot;blue&amp;quot;, lty = 2, alpha = 0.5) + 
  facet_grid(~ keyword) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2021-03-01-coastal-risks-google-trends.en_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this figure, we can see that after a storm event there is often a surge in interest in the term “marine submersion” in France. This confirms my hypothesis !&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This analysis was really interesting for me as it confirmed what I knew with numbers and graphs. Without a doubt, this analysis will be a great asset for my introduction, especially in the part where I will discuss about the growing interest in data science. I hope this post gave you the envy to reproduce this analysis for the terms of your choice !&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Webscraping Aliexpress with Rselenium</title>
      <link>https://aureliencallens.github.io/2020/11/18/2020-11-18-aliexpress_rselenium/</link>
      <pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2020/11/18/2020-11-18-aliexpress_rselenium/</guid>
      <description>


&lt;p&gt;Today, I am going to show you how to scrape product prices from Aliexpress website.&lt;/p&gt;
&lt;div id=&#34;a-few-words-on-web-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A few words on web scraping&lt;/h2&gt;
&lt;p&gt;Before diving into the subject, you should be aware that web scraping is not allowed on certain websites. To know if it is the case for the website you want to scrape, I invit you to check the &lt;em&gt;robots.txt&lt;/em&gt; page which should be located at the root of the website address. For Aliexpress this page is located here : &lt;a href=&#34;https://www.aliexpress.com/robots.txt&#34; target=&#34;_blank&#34;&gt;www.aliexpress.com/robots.txt .&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This page indicates that webscrapping and crawling are not allowed on several page categories such as &lt;code&gt;/bin/*&lt;/code&gt;, &lt;code&gt;/search/*&lt;/code&gt;, &lt;code&gt;/wholesale*&lt;/code&gt; for example. Fortunately for us, the &lt;code&gt;/item/*&lt;/code&gt; category, where the product pages are stored, can be scraped.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rselenium&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RSelenium&lt;/h2&gt;
&lt;div id=&#34;installation-for-ubuntu-18.04-lts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installation for Ubuntu 18.04 LTS&lt;/h3&gt;
&lt;p&gt;The installation for RSelenium was not as easy as expected and I encountered two errors.&lt;/p&gt;
&lt;p&gt;The first error I got after I installed the package and tried the function &lt;em&gt;Rsdriver&lt;/em&gt; was :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error in curl::curl_fetch_disk(url, x$path, handle = handle) :
Unrecognized content encoding type. libcurl understands deflate, gzip content encodings.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/ropensci/RSelenium/issues/186&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;, I installed the missing package : &lt;em&gt;stringi&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Once this error was addressed, I had a different one :&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error: Invalid or corrupt jarfile /home/aurelien/.local/share/binman_seleniumserver/generic/4.0.0-alpha-2/selenium-server-standalone-4.0.0-alpha-2.jar&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time the problem came from a corrupted file. Thanks to &lt;a href=&#34;https://stackoverflow.com/questions/20680229/invalid-or-corrupt-jarfile-usr-local-bin-selenium-server-standalone-2-38-0-jar&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt;, I knew that I just had to download this file &lt;em&gt;selenium-server-standalone-4.0.0-alpha-2.jar&lt;/em&gt; from the official &lt;a href=&#34;https://selenium-release.storage.googleapis.com/index.html?path=4.0/&#34; target=&#34;_blank&#34;&gt;selenium website&lt;/a&gt; and replace the corrupted file with it.&lt;/p&gt;
&lt;p&gt;I hope this will help some of you to install RSelenium with Ubuntu 18.04 LTS !&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;opening-a-web-browser&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Opening a web browser&lt;/h3&gt;
&lt;p&gt;After addressing the errors above, I can now open a firefox browser :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RSelenium)

#Open a firefox driver
rD &amp;lt;- rsDriver(browser = &amp;quot;firefox&amp;quot;) 
remDr &amp;lt;- rD[[&amp;quot;client&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logging-in-aliexpress&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logging in Aliexpress&lt;/h3&gt;
&lt;p&gt;The first step to scrape product prices on Aliexpress is to log in into your account:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_id &amp;lt;- &amp;quot;Your_mail_adress&amp;quot;
password &amp;lt;- &amp;quot;Your_password&amp;quot;

# Navigate to aliexpress login page 
remDr$navigate(&amp;quot;https://login.aliexpress.com/&amp;quot;)

# Fill the form with mail address
remDr$findElement(using = &amp;quot;id&amp;quot;, &amp;quot;fm-login-id&amp;quot;)$sendKeysToElement(list(log_id))

# Fill the form with password
remDr$findElement(using = &amp;#39;id&amp;#39;, &amp;quot;fm-login-password&amp;quot;)$sendKeysToElement(list(password))

#Submit the login form by clicking Submit button
remDr$findElement(&amp;quot;class&amp;quot;, &amp;quot;fm-button&amp;quot;)$clickElement()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;navigating-through-the-urls-and-scraping-the-prices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Navigating through the URLs and scraping the prices&lt;/h3&gt;
&lt;p&gt;Now we have to navigate through a vector containing the URL of the aliexpress products we are interested in. Then we extract the price of the product by using the xpath of the product price of the webpage. The xpath of the element you want to scrape can be found by using the developer tool of chrome or firefox ( &lt;a href=&#34;https://www.scrapingbee.com/blog/practical-xpath-for-web-scraping/&#34;&gt;tutorial here !&lt;/a&gt; ). Once the price is extracted we have to ensure this price is in numerical format by removing any special character (euro or dollar sign) and replace the comma by a point for the decimal separator. Here is the R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  url_list &amp;lt;- list(&amp;quot;https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Craws-Soft-Fishing-Lures-110mm-11-5g-Artificial-Bait-Soft-Bait-Craws-Lures/406467_32419930548.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ&amp;quot;,
            &amp;quot;https://fr.aliexpress.com/store/product/Maxcatch-Fishing-Lure-5Pcs-Lot-155mm-7-4g-3-colors-Swimbait-Artificial-Lizard-Soft-Fishing-Lures/406467_32613648610.html?spm=a2g0w.12010612.0.0.5deb64f7836LnZ&amp;quot;,
            &amp;quot;https://fr.aliexpress.com/store/product/Maxcatch-6Pcs-lot-Soft-Fishing-Lures-Minnow-Biat-95mm-6g-Jerkbait-Soft-Bait/406467_32419066106.html?spm=a2g0w.12010612.0.0.25fe5872CBqy0m&amp;quot;) 

# Allocate a vector to store the price of the products 
currentp &amp;lt;- c()
for(i in 1:length(url_list)){
  
  # Navigate to link [i]
  remDr$navigate(url_list[i])
  
  # Find the price with an xpath selector and findElement.  
  # Sometimes products can be removed and this could throw an error this is why we are using &amp;#39;try&amp;#39; to handle the potential errors
  
  current &amp;lt;- try(remDr$findElement(using = &amp;quot;xpath&amp;quot;,&amp;#39;//*[contains(concat( &amp;quot; &amp;quot;, @class, &amp;quot; &amp;quot; ), concat( &amp;quot; &amp;quot;, &amp;quot;product-price-value&amp;quot;, &amp;quot; &amp;quot; ))]&amp;#39;), silent = T)
  
  #If error : current price is NA 
  if(class(current) ==&amp;#39;try-error&amp;#39;){
    currentp[i] &amp;lt;- NA
  }else{
    # Get the price 
    text &amp;lt;- unlist(current$getElementText())
    
    #Remove euro sign
    text &amp;lt;- gsub(&amp;quot;[^A-Za-z0-9,;._-]&amp;quot;,&amp;quot;&amp;quot;,text)
    
    #Case when there is a range of price instead of one price + replace comma by point
    if(grepl(&amp;quot;-&amp;quot;, text)) {  
      pe &amp;lt;- sub(&amp;quot;-.*&amp;quot;,&amp;quot;&amp;quot;,text) %&amp;gt;% sub(&amp;quot;,&amp;quot;, &amp;quot;.&amp;quot;, ., fixed = TRUE)
      currentp[i] &amp;lt;-  as.numeric(pe)
    }else{
      currentp[i] &amp;lt;- as.numeric(sub(&amp;quot;,&amp;quot;, &amp;quot;.&amp;quot;, text, fixed = TRUE))
  }
  }
  
Sys.sleep(4)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Between each link it is advised to wait a few seconds with &lt;em&gt;Sys.sleep(4)&lt;/em&gt; to avoid being black-listed by the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;phantomjs-version&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Phantomjs version&lt;/h3&gt;
&lt;p&gt;If you execute the code above, you should see a firefox browser open and navigate through the list you provided. In case you don’t want an active window, you can replace firefox by phantomjs browser which is a headless browser (without a window).&lt;/p&gt;
&lt;p&gt;I don’t know why but using &lt;code&gt;rsDriver(browser = &#34;phantomjs&#34;)&lt;/code&gt; does not work for me. I found &lt;a href=&#34;https://cbelanger.netlify.app/post/web-scraping-in-r-selenium-firefox-and-phantomjs/&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; which propose to start the phantomjs browser with the wdman package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wdman)
library(RSelenium)
# start phantomjs instance
rPJS &amp;lt;- wdman::phantomjs(port = 4680L)

# is it alive?
rPJS$process$is_alive()

#connect selenium to it?
remDr &amp;lt;-  RSelenium::remoteDriver(browserName=&amp;quot;phantomjs&amp;quot;, port=4680L)

# open a browser
remDr$open()

remDr$navigate(&amp;quot;http://www.google.com/&amp;quot;)

# Screenshot of the headless browser to check if everything is working
remDr$screenshot(display = TRUE)

# Don&amp;#39;t forget to close the browser when you are finished ! 
remDr$close()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Once you have understand the basics of RSelenium and how to select elements inside HTML pages, it is really easy to write a script to scrape data on the web. This post was a short example to scrape the product price on Aliexpress pages but the script can be extended to scrape more data on each page such as the name of the item, its rating etc… It is even possible to automate this script to run daily in order to see price changes over time. As you see possibilities are endless!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can R and Shiny make me a better fisherman? Part 2</title>
      <link>https://aureliencallens.github.io/2020/09/25/r-shiny-fishing-part2/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2020/09/25/r-shiny-fishing-part2/</guid>
      <description>
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/proj4/proj4.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://aureliencallens.github.io/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://aureliencallens.github.io/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the previous blog article, I described in details how I built a shiny application that stores data about my fishing sessions. In this post, I will explore the data I have collected during the last year.&lt;/p&gt;
&lt;p&gt;To sum up, my application store the data in two csv files. The first one contains variables related to the fishing conditions at the beginning and at the end of the session such as :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time&lt;/li&gt;
&lt;li&gt;Longitude and Latitude&lt;/li&gt;
&lt;li&gt;Meteorological conditions (9 in total : Air temperature, Atmospheric pressure, Wind speed and direction …)&lt;/li&gt;
&lt;li&gt;Tide conditions (Current water level, Tide status and Time since the tide peak)&lt;/li&gt;
&lt;li&gt;Specific comments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second one contains information about my catches :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time when caught&lt;/li&gt;
&lt;li&gt;Time since the beginning of the session&lt;/li&gt;
&lt;li&gt;Species&lt;/li&gt;
&lt;li&gt;Length of the fish&lt;/li&gt;
&lt;li&gt;Fishing lure used&lt;/li&gt;
&lt;li&gt;Color of the fishing lure&lt;/li&gt;
&lt;li&gt;Length of the lure&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;importing-and-cleaning-my-fishing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importing and cleaning my fishing data&lt;/h2&gt;
&lt;p&gt;The first step of this analysis is to import both csv files.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Summary table of the first file:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(summarytools)
library(kableExtra)

# Change character variables to factor
session_data %&amp;lt;&amp;gt;% 
mutate_at(vars(Weather, Tide_status), as.factor)

# Make summary table 
session_data %&amp;gt;% 
  select(-c(Status, Comments)) %&amp;gt;% 
dfSummary(., plain.ascii = FALSE, style = &amp;quot;multiline&amp;quot;, 
          graph.magnif = 0.75, valid.col = FALSE) %&amp;gt;% 
  select(-c(text.graph, Missing)) %&amp;gt;% 
  knitr::kable(format = &amp;quot;html&amp;quot;, escape = F) %&amp;gt;%
  scroll_box(width = &amp;quot;100%&amp;quot;, height = &amp;quot;500px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; &#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
No
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Variable
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Stats / Values
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Freqs (% of Valid)
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Graph
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Session&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 19.5 (11)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
1 &amp;lt; 19.5 &amp;lt; 38&lt;br /&gt;
IQR (CV) : 19 (0.6)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAAx0lEQVR42u3cwQmEMBBA0WRJdVawW6FWYHt6nT0MiArin/9uEgx+iacE+9Zq+Dz9AIYaes6IF/MrP9hvz0biwPgfmpJb1htH7pyrtfXg6yizdA2lMZTGUBpDaQylMZTGUBpDaQylMZTGUBpDaQylKRM6rk/xtCXdA/yF7TRA6LF9tjJL11AaQ2kMpTGUxlAaQ2kMpTGUxlAaQ2kMpTGUxlAaQ2kMpTGUxlAaQ2kMpTGUxlCaHk8MvvMXIrl46LHD2lJllq6hNDtoUxGs2NNpeAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Time&lt;br /&gt;
[POSIXct, POSIXt]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min : 2019-04-09 19:23:09&lt;br /&gt;
med : 2019-06-21 08:13:07&lt;br /&gt;
max : 2020-09-13 22:12:40&lt;br /&gt;
range : 1y 5m 4d 2H 49M 31S
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
76 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABB0lEQVR42u2bUQ6CMBAFreF0nkBPKCfwevpbpQRsmrDMzvxB+Ogkbzdts5T3JQfXoxegqKJ9TPVDq17nlSK+l6OXvk29xGn781vj3etoh79JE11FaShKQ1EaitJQlIaiNBSloSgNRWkoSkNRGorSUJSGojQUpaEoDUVpKEojjeiO8ZtY9M49nU60d+4pTXQVpdFdo62mEHkQslt02RJiD0Kmia6iNBSloSiNNKKhTy/zwP/JQouO3H+lia6iNL5qdGTxR+OnGZ3t8LWfNNFVlEbwDcN+Wo30Ud1hYUS32mia6CpKY2iNRr7UHioaeV+VJrqK0ih1/3jCjmn1zqjA3FZJE11FaXwAiI0bDp6R7IsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Long&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : -1.5 (0)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
-1.5 &amp;lt; -1.5 &amp;lt; -1.5&lt;br /&gt;
IQR (CV) : 0 (0)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
69 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABA0lEQVR42u3cQQ6CMBQAUWs4HSfQE8oJvJ5ui0AwyP/gdGaHC+oLpRoClNelja5HfwGhQrfV1Rux5+sws/tbiRyx3nm3eS8b6j+2n4ljNzN1hdISSksoLaG0hNISSksoLaG0hNISSksorWagqRewp+VdvT8Y2k8+ibp638zUFUpLKC2htITSEkpLKC2htITSEkpLKC2htITSEkpLKC2htITSEkqrGejBN2t80z53rvwBdJ/nTkfQ/Gd28/o4okc+sxubi9FvDenvsZgb8V6ddmGLUd7Nb0sjjsc74aobMxtOCI1ZEFehlJ+cVWj+uRZTqQ/YA/bOn3rVLTDbYs38YRBK6w1cEB4ObwvYYwAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lat&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 43.5 (0)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
43.5 &amp;lt; 43.5 &amp;lt; 43.5&lt;br /&gt;
IQR (CV) : 0 (0)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
70 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA00lEQVR42u3bQQqDMBBA0aZ4Ok+gJ7Qn6PXsdlzIqJTG/vl/JwbxQQy4mLY+avTs/QJChV5riBf3+V5fB15latmKuGDIFvdqTO6/Tz6vzNYVSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNIqA73tyFZePr82h+G0P4aem14rs3WF0hJKq8upe2SwGQH9/mBzXpmtK5SWUFqbU7fHsf+rWrQtMGj8TWsw225lvlGhtD4BqxEAhUjPPAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Air_temp&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 19.1 (4.7)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
10.6 &amp;lt; 18.3 &amp;lt; 30.7&lt;br /&gt;
IQR (CV) : 6.4 (0.2)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
73 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABJ0lEQVR42u2bYQ6CMAxGmeF0nEBPKCfwevp3uqmjGZR9fe8fBAgvaZuug/ScYnDxfgFEEbUx5wfWfF0rN16Tt9o05a8wm5/yxvJx/PB2LAgTuoiqgagaiKoRRtTQMKxNDdTZuiVTZ9TSBy3FGd9uKUzoIqoGomogqgaiaiCqBqJqIKoGomogqgaiaiCqRqcdbxv1Ufg+Y25X0SPH3GFCF1E1/uRo285ZK32f1lW0b7nw/BopTOgiqgaiaiCqBqJqIKoGomo4j1JqlIu5HlOkE4rus5gLE7qIqnHCHC1pKU+1edQtu2oI0bby9PuqMKGLqBpD5GjJ9lH4oKLbx+phQhdRNd5y1HO361DREf6+txImdBFVI+X15y5WjPJlWhJz+0qY0EVUjRcKXyYHmYFvUQAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Weather&lt;br /&gt;
[factor]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1. Clear&lt;br /&gt;
2. Drizzle&lt;br /&gt;
3. Humid&lt;br /&gt;
4. Humid and Mostly Cloudy&lt;br /&gt;
5. Humid and Partly Cloudy&lt;br /&gt;
6. Mostly Cloudy&lt;br /&gt;
7. Overcast&lt;br /&gt;
8. Partly Cloudy&lt;br /&gt;
9. Possible Light Rain&lt;br /&gt;
10. Windy and Mostly Cloudy
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
\25 (32.9%)&lt;br /&gt;
 1 ( 1.3%)&lt;br /&gt;
 1 ( 1.3%)&lt;br /&gt;
 2 ( 2.6%)&lt;br /&gt;
 1 ( 1.3%)&lt;br /&gt;
\18 (23.7%)&lt;br /&gt;
 8 (10.5%)&lt;br /&gt;
\17 (22.4%)&lt;br /&gt;
 2 ( 2.6%)&lt;br /&gt;
 1 ( 1.3%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAADEAAAC3CAQAAAARfH5SAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA/0lEQVR42u3cwQ2DMAyF4bjqdEzQTlgmYD04IQVUiJvECL3+78aFT8hRjFGEzSk6j3BBhHjmF59uhXnbAZHS0AWYNlcatYCAgFAldtvgVHeX01i+f/drstlefnktxnkMeFvQWFEQEBAQf07Q9WqIC54CAgICAiIgGl1vMxSvPe9lNbc6yu4phk4fIE+IiEBAQEDcmeALpzPMehAQEBAChGBLisnXWe+3lCbD5qMm5XcWjVpAQECoEgxizjhbUssHSVdLaquQxqKFgICAqA5dz5nmQazcDxsHMU/tNBYtBAQERHXoes5w1AQCAgJCgKDrOcNREwgICAgBwvhxym2IBWG8OB3nstqxAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Atm_pres&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 1017.7 (6.2)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
996.8 &amp;lt; 1017.3 &amp;lt; 1028.6&lt;br /&gt;
IQR (CV) : 7.2 (0)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
70 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA9klEQVR42u3cQQ7CIBQAUTE9nSfQE+oJvJ5ucVEISAqdP7MzxtaXfJLWEtPnEqPr7C8gVGhfW/5iznp9VU57T71Hzj+49R5kZLfCe+9B5wgzukJpCaUllJZQWkJpCaUllJZQWkJpCaUllJZQWkJpCaUllJZQWkJpCaUVBnrI9pvaTiIM9JidROXCjK5QWkJpCaW1xH7d3moXIo9sZ/KpoS0XImFGNwz0BKM75pbgBND9ddhyOxBmdIXSEkpLKC2htITSEkpLKK0w0J8b7/4/CljhwWAD9J/He/MfDJYLM7phoClfXM/lV1pb+S/1CWbbLczoCqX1BaYUF629GKdEAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wind_str&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 10.4 (6.2)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
3.1 &amp;lt; 8.8 &amp;lt; 45.9&lt;br /&gt;
IQR (CV) : 5.7 (0.6)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
73 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA/UlEQVR42u3bXRLBMBRAYTFdnRWwQl2B7fEa3E6o/sS557wxHvKRptVMy/2Qo+PeAxAqdF5D/SI6XseJg/hc9h56u3qIQ/vjp+C9296Gr0szdYXSEkpLKC2htITSEkpLKC2htITSEkpLKC2htITSEkpLKK000A82guOinfCed8FnQ9/3wfveBU8zdYXSEkpLKC2htITSEkpLKC2htITSEkpLKK2n+7oj+BnLlxvY/3Zbejb0t3replgU2vN8SLMYCaUllFYa6KKnl6j4amv7s+vq0F4ehU8zddNAV5+62xSvBJdqJYBA29fZpf4qrrD/o/UvWmC2ydIsRkJpPQAcNhhjjiifFAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wind_dir&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 204.2 (107.2)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
6 &amp;lt; 233.5 &amp;lt; 358&lt;br /&gt;
IQR (CV) : 200.8 (0.5)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
65 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABAUlEQVR42u2bUQrCMBAFjfR0nkBPaE/g9fQ3CCuhWreZnflrQ2kGXiC8pu15qsE5ewKKKrqNpb847npdP0zt2qKRfmA5TcIluP8YfL5MdBWloSgNRWkoSkNRGorSUJSGojRSGoa4GIlrkSlFo2JktBbZQpnoKkpDURqK0lCUhqI0FKWhKA1FaQw2DNtOhRyJ4Srl21Mh2dSM7pp+omq/GbxFNzug+72/THQVpaEoDUVpKEqjjOgPPgTn75D/JBrvT7N3zj1loqsoDUVpKEpDURqK0lCUhqI0yohO89d+TNxw3LpP1ADRsR6jTHTLiLY+4Pcp+rxx+jXaYG4hZaKrKI0Xi14bsbsk3owAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cloud_cover&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 0.4 (0.3)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
0 &amp;lt; 0.4 &amp;lt; 1&lt;br /&gt;
IQR (CV) : 0.7 (0.8)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
43 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABAUlEQVR42u2bXQ7CIBAGxfR0PYGesD2B19NXjNUaoCKzM2/9oWGSb7OkgXQ/xeDcewKKKlrGlF8smwV7Sb0nWUo+8en50fzy8q33bBsRJrqK0lCUhqI0FKWhKA1FaShKQ1EaitJQlIaiNBSloSgNRWkoSkNRGorSUJSGojQUpaEojan+E59Yq7ZR1o3+qWjtNsp2mzDDRDeMaMPorlXnEI4Yfc2quWmN9qzHvdFhohtG9Ivotuxmfy06b9wb70iB0T2SHo2oi2hdMZSNDhNdRWkoSiOMaHF7qeuFA4mOdtI0THQVpaEoDUVpKEoj5Su5ZbBl3R75n/oEc3tLmOgqSuMBOxEh/7VPo8kAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Preci_prob&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 0 (0.1)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
0 &amp;lt; 0 &amp;lt; 0.5&lt;br /&gt;
IQR (CV) : 0 (2.2)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA3UlEQVR42u3cQQrCMBQAUSM5XU+gJ6wn6PV0G/EXQyrEzp/ZWVPIgzR0k5bnJUfX2RMQKnSs2v5Ywwf2VmZPcrR24vX9r+Vj8DZ7tj8qzdIVSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksoLaG0hNISSksorTTQ+n3IA3GqtAO6BNfOd6o0zdIVSisNtGMziov24n/eiYehZzvfn2bppoEOL92++t+qjr1/xXffm7tLO2KFffNnF0ouzTMqlNYLgyQUslOrargAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Preci_int&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 0 (0.1)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
0 &amp;lt; 0 &amp;lt; 0.7&lt;br /&gt;
IQR (CV) : 0 (2.7)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
29 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAAxUlEQVR42u3cQQ5AMBQAUZWezgk4ISdwPba1oCmhMZ23E5V0Gix/2Lo29LU3YKih98T0Ys58sGOovd0y6Xbj8dZw8dhae9+PNPPqGkpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSGEpjKI2hNLFk8fLSBJkvBlkUhZ6Po1gzoyreGWSRO/gpOcCQrs2NEPmb01CyZn5GhtLs40EPrJ6Djx8AAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Moon&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 0.6 (0.3)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
0 &amp;lt; 0.6 &amp;lt; 1&lt;br /&gt;
IQR (CV) : 0.5 (0.5)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
32 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABEklEQVR42u2b2w2DMAwAScV0naCdsEzQ9drfIKDhkfI43/0RKZJPMpHtQPo0MbgdHYCiiq6jzR+u8L52E0E+0nAtX2qby3EfWXsXd4VJXUVpKEpDURqK0lCUhqI0FKWhKI0wosVRypIZzZGMxfnMYpwxM1o3o9mbYZT9GMOkrqI0FKWhKA1FaShKQ1EaB9x4dwu+IKjXDB5ytT/e+JUarW2ESV1FaShKQ1EaYUR7BcOSmmXItt27ipbHwL8Z2/3femc+YVJXURqK0lCURhjRk/8lUa/aOrlovboqTOoqSkNRGorSUJSGojQUpbG6ezn3uLqi6FkG03MJk7qK0kj5mfK62AFTIv8dJMHcJgmTuorS+AJcnR9YpC8V/AAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Water_level&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 2.8 (1)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
1.1 &amp;lt; 2.8 &amp;lt; 4.3&lt;br /&gt;
IQR (CV) : 1.9 (0.4)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
65 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA/ElEQVR42u3aQQ6CMBBAUWo4nSfQE8oJvJ5u64KJFaTlz/87Q2J4yQChpbymHF16n4BQob811z9GvF6X8KRuJTpaH5yn4buuHnk2/Eua0RVKSygtobSE0hJKSygtobSE0hJKSygtobSE0mpYqd+yOdC/pi2JfTYH+pRmdIXSEkrr4667jLjl/Q9o9AA5wyMkKs3oCqUllJZQWkJpCaWVBnqCz1ij4vete7Vgd3Lo929bjm5rW17aj1gT3g0aD1H/F/o0o5sGOsRd94hFuSGgR1zBaUZXKC2htITSEkpLKC2htITSEkorDbTUyzUP2AdV9ZZEgdlWSzO6Qmm9AR3nGQphvDmeAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tide_status&lt;br /&gt;
[factor]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1. Dead&lt;br /&gt;
2. Down&lt;br /&gt;
3. Up
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
\11 (14.5%)&lt;br /&gt;
\28 (36.8%)&lt;br /&gt;
\37 (48.7%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAEMAAAA7CAQAAACKsv9UAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAAn0lEQVRYw+2WUQ5AMBAFVXo6J+CEnMD1+N2SVptYXpqZPxFM+lb2hWNQYPxbAI070V6sj4Myh/c+bV8V01tT8cHd7TREQkEDDTTQQKMDjctq81teZYLd7d8WQrvoRUIR0UhmY3NIpa6vNbWvdmpHXiQUNNBAAw00OtCgfdG+cri3rzyLScW5feVJfwaRUNBAAw000OhAQ7F9/YdIKGhYTgyGEaS0KaAfAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tide_time&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 186.8 (125.3)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
5 &amp;lt; 175 &amp;lt; 375&lt;br /&gt;
IQR (CV) : 255 (0.7)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
47 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABB0lEQVR42u2cUQ6CMBAFwXA6T6AnlBN4Pf2txpJmRWlnZ/6ANOkk2/BYCvNjysHp6AkoqmiMpTzod72uG1O7zLUr5YVlGoRz5fy9cXya0lWUhqI0FKWhKI00oi8RMJYox+At636bKPslTekqSkNRGorSUJSGojQUpaEoDUVpKEpDURppRJtf7dd7vmN0fJtFR+/4pildRWkoSkNRGorS2GF35xrY/fr/2LiDaD0c9hQb05SuojQUpZFGtKuvJCJ35K1R1+Ju3ZVorGHTNiZN6aYRPah0Y6txQNHP6+qXGThN6SpKQ1EaitJQlIaiNBSlkUZ0Lh8Mb/3+QyRE2TOaYW5V0pSuojSeJ48cCsu8CVkAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Summary table of the second file:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Change character variables to factor
catch_data %&amp;lt;&amp;gt;% 
mutate_at(vars(species, lure, colour, length_lure), as.factor)

# Make summary table 
catch_data %&amp;gt;% 
dfSummary(., plain.ascii = FALSE, style = &amp;quot;multiline&amp;quot;, 
          graph.magnif = 0.75, valid.col = FALSE) %&amp;gt;% 
  select(-c(text.graph, Missing)) %&amp;gt;% 
  knitr::kable(format = &amp;quot;html&amp;quot;, escape = F) %&amp;gt;%
  scroll_box(width = &amp;quot;100%&amp;quot;, height = &amp;quot;500px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:100%; &#34;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
No
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Variable
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Stats / Values
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Freqs (% of Valid)
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Graph
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n_ses&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 24.3 (13.2)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
1 &amp;lt; 28 &amp;lt; 37&lt;br /&gt;
IQR (CV) : 20 (0.5)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
18 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA8ElEQVR42u3cQQrCMBBAUSOezhPoCe0JvJ5u20JiMYYxf/7b1YX0QxrIWCyvUw7n6Bsw1NDvXNYXMz6vS+Om76USOqdr5fPn5irN0jWUxlAaQ2kMpTGUxlCazemldeS5lU9fNdLSfYLcHdOOHXki9N5ZmqVrKI2hNIbSGEpjKI2hNIbSGEpjKI2hNIbSBLx+0z+MniS0NYweN0BPs3QNpTGUxlAaQ2kMpTGUxlAaQ2kMpTGUxlCagXPdmEF1QGhtUB3zNneapWsozeFn9L+2loGhEb+A/VKapWsoTVnvMY/JN5y99V+IFFhbVZqlayjNGxHNGVPK0jg3AAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
time&lt;br /&gt;
[POSIXct, POSIXt]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min : 2019-04-09 19:23:29&lt;br /&gt;
med : 2020-05-25 21:20:55&lt;br /&gt;
max : 2020-09-04 21:33:17&lt;br /&gt;
range : 1y 4m 26d 2H 9M 48S
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
57 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA10lEQVR42u3csQ2DMBBA0VyU6ZggmTBMwHqkNRGnWKQA//uvwwXyl0xjC8d6q+F+9gQMNfSYR/sw6vc6JxN/RRI6rmlnbNk8lVm6htIYSmMojaE0htIYSmMojaE0htIYSmMojaE0htIYSrM5TcuO357R86pr+zo2/H38NqoyS9dQGkNpDKUxlMZQGkNpDKUxlMZQGkNpDKXp+h3kKhvb8x8/5nSFTrujZ2xsH59JmaVrKI2hNIbSGEpjKI2hNIbSGEoT7e7Ee9Q7RBLtFSIBa0uVWbqG0nwAbkkSVhHqu04AAAAldEVYdGRhdGU6Y3JlYXRlADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDB2a/EfAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwBzZJowAAAABJRU5ErkJggg==&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min_fishing&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 51.4 (43.4)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
0 &amp;lt; 44 &amp;lt; 150&lt;br /&gt;
IQR (CV) : 77 (0.8)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
35 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABC0lEQVR42u2cUQqDMBAFm5LT9QTtCfUEXq/9TaEpxighszN/IpEMvCVLjKb3LQb30RNQVNFj5PJi+VOwzzR6qu2UU87ftx6VIdvoOXcTJrqK0lCUhqI0FKWhKA1FaShKQ1EaitJQlIaiNHL/I1pZh2yTDxAds00eJrqK0lCUhqI0FKWhKI3dvW69FZ/jvMpu0dnPq4SJrqI0FKURRvSEPaM5VtgTROdYYcNEV1EaitJQlIaiNMKIXvp+dG3+lPG6vvlS0d9d8NY84oy+OUx0FaWhKA1FaQw5UHWE3lZiGtHeViJMdMOIThPdOvXqfRXVCxDdV71hohtGNJUBX2D//ClrNMHcqoSJrqI0PsMwHWcXHY73AAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
species&lt;br /&gt;
[factor]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1. bar&lt;br /&gt;
2. bar_m
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
\52 (91.2%)&lt;br /&gt;
 5 ( 8.8%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHIAAAAqCAQAAADFCcdsAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAAjklEQVRo3u2YQQqAMAwEE/F1vkBfqC/o9/QaRbGI0DLZueVSMjQ5bHw3PkPrBiQpyXrGWKygBV38QdJsat3bT5RTlWJcJUlBkhQkSUGSFCRJQZIULimkfHulczxGSFCctBAnE47rdvuVs9c91S+vlwHClqYYV0lSkCQFSVKQJAVJUpAkhXyXASopxlWSFA7QnAvHD/kqRgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
length&lt;br /&gt;
[numeric]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean (sd) : 29.1 (9.8)&lt;br /&gt;
min &amp;lt; med &amp;lt; max:&lt;br /&gt;
10 &amp;lt; 30 &amp;lt; 49&lt;br /&gt;
IQR (CV) : 12 (0.3)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
22 distinct values
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAHQAAABUCAQAAAD5P9uaAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAABFUlEQVR42u2a2w3CMBAEMUp1VAAVkgpoD/6QQRwyDo7t2Z2/RMpjpM3pfE66HzQ49n4Bi1q0jiU/qPle1y8XnVNfufzxS/VdnpyC87e+lm/IRNeiNCxKw6I0LErDojQsSsOiNCxKw6I0/jAziokHZ/uPzZqKjjQ2k4muRWlYlIZFaViURtPO6Fda7rUOJdqyaZSJrkVpyIgWFqN1+h/MiqvuSIvoGmSia1EaL9/o/CWnUHT+khMjE12L0hhsmRazdXtjGtGtZVImuhalISPaqRjt32x2Ev1cQ1s2mjLRtSiNaTqjmLiwXbLmECBa1hzKRNeiNCxKw6I0LErDojQsSkNGNOWLuStsfzRfjyaYW4hMdC1K4wF8iR25d7sLagAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
lure&lt;br /&gt;
[factor]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1. jerkbait&lt;br /&gt;
2. shad&lt;br /&gt;
3. slug&lt;br /&gt;
4. topwater
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
 2 ( 3.5%)&lt;br /&gt;
\38 (66.7%)&lt;br /&gt;
\16 (28.1%)&lt;br /&gt;
 1 ( 1.8%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAFcAAABOCAQAAAD/GEK0AAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAA0ElEQVRo3u3aQQrCMBSE4Rfp6XoCPaGewOvVRUFfg0poo5mBf1YVNx9hhAy1LOGU02gAXJ1M+cP1WeRzGQ17JVOm7VdzRETcRws/xqwMcOHChQsXLly4LaluZLp3sTUlbzXN3Zbvu85luEke7yUd79s1oZTtr8msDHDhwoULFy5cuC1hTXQnpmfnMhxdE79/o9FxTfyj92ZlgAsXLly4cOHCbQlrojsxPTuXYV0TSv9yqlOdrt67ia9c9cCFCxcuXLhw4e7gWq0J/XiXQT1m3Ad3ZBbKHMa9qgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
colour&lt;br /&gt;
[factor]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1. clair&lt;br /&gt;
2. naturel&lt;br /&gt;
3. sombre
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
\21 (36.8%)&lt;br /&gt;
\26 (45.6%)&lt;br /&gt;
\10 (17.5%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAEAAAAA7CAQAAABhhURXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAAnElEQVRYw+3YwQmAMAyF4UY6nRPohDqB6+k11UJF0j4Pf25WoR+8QIJ2Jm1N4vsBpOwftg4NsdjzzB/l8tUcfP3R/EIeAQAAAAAAkANuw6g9PKLL/AQetZ75cSyPQA4oemAflMHqMui8EdWqbHR5BAAAAAAAQA5gI5JHIAd82ohq/31CAO82othGlUcAAAAAAADkgH9tRIqSRwDgAnQaEaR5cS7eAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIyLTA5LTE0VDA5OjE3OjI5KzAyOjAwdmvxHwAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMAc2SaMAAAAASUVORK5CYII=&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
length_lure&lt;br /&gt;
[factor]
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1. moyen&lt;br /&gt;
2. petit
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
\32 (56.1%)&lt;br /&gt;
\25 (43.9%)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;img style=&#34;border:none;background-color:transparent;padding:0;max-width:max-content;&#34; src=&#34;data:image/png;base64,%20iVBORw0KGgoAAAANSUhEUgAAAEsAAAAqCAQAAABR726eAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAAHdElNRQfmCQ4LER3djOdVAAAAf0lEQVRYw+3WSwqAMAyE4UQ8nSfQE+oJej3dRkGoD5op/LMrlPLRZDG+m2KGbACs7xnjYU1dtMVvWGZTGqqcTqJDhAULlkJgwYKlkEuDKO9e+T0eK1Zurw91q4shbg2+a/aaW43bae3uig4RFixYCoEFC5ZCeminOhEdIqwnOQCHOAvHb8r6gAAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMi0wOS0xNFQwOToxNzoyOSswMjowMHZr8R8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjItMDktMTRUMDk6MTc6MjkrMDI6MDAHNkmjAAAAAElFTkSuQmCC&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;After cleaning and rearranging the data (code hidden below), we are ready to explore
graphically our data!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute mean conditions (between beg and end session) 

mean_weather_cond &amp;lt;- session_data %&amp;gt;% 
group_by(Session) %&amp;gt;% 
select(-c(Long, Lat, Water_level, Tide_time)) %&amp;gt;% 
summarise_if(is.numeric, mean) 


# Extract fixed conditions and comments + join with mean cond 

fixed_cond_com &amp;lt;- session_data %&amp;gt;% 
group_by(Session) %&amp;gt;% 
select(Session, Comments, Long, Lat, Weather) %&amp;gt;% 
mutate(Comments_parsed = paste(na.omit(Comments), collapse = &amp;quot;&amp;quot;)) %&amp;gt;% 
select(-Comments) %&amp;gt;% 
slice(1) %&amp;gt;% 
inner_join(mean_weather_cond, by = &amp;quot;Session&amp;quot;)

# Create end and beg variables for WL, Time , Tide_time, Tide_status

beg_end_vars &amp;lt;- session_data %&amp;gt;% 
select(Session, Status, Water_level, Time, Tide_time, Tide_status) %&amp;gt;% 
pivot_wider(names_from = Status,
values_from = c(Time, Water_level,  Tide_time, Tide_status))


# Assemble both file and calculate duration

dat_ses &amp;lt;-  inner_join(beg_end_vars,
fixed_cond_com,
by = &amp;quot;Session&amp;quot;)

# Calculate duration of the sessions

dat_ses %&amp;lt;&amp;gt;% 
mutate(duration = round(difftime(Time_end,  Time_beg,  units = &amp;quot;hours&amp;quot;),
digits = 1))

catch_cond &amp;lt;- full_join(dat_ses,
catch_data, by = c( &amp;quot;Session&amp;quot; = &amp;quot;n_ses&amp;quot; )) %&amp;gt;% 
mutate(Session = factor(Session, levels = 1:length(dat_ses$Session)))

catch_cond %&amp;lt;&amp;gt;%
mutate(Tide_status_ses = paste0(Tide_status_beg, &amp;quot;_&amp;quot;, Tide_status_end))

# Simplify the Tide status variable

catch_cond$Tide_status_ses &amp;lt;- sapply(catch_cond$Tide_status_ses , function(x){switch(x, 
&amp;quot;Up_Dead&amp;quot; = &amp;quot;Up&amp;quot;,
&amp;quot;Up_Up&amp;quot; = &amp;quot;Up&amp;quot;,
&amp;quot;Up_Down&amp;quot; = &amp;quot;Dead&amp;quot;,
&amp;quot;Down_Dead&amp;quot; = &amp;quot;Down&amp;quot;,
&amp;quot;Down_Up&amp;quot; = &amp;quot;Dead&amp;quot;,
&amp;quot;Down_Down&amp;quot;  = &amp;quot;Down&amp;quot;,
&amp;quot;Dead_Dead&amp;quot; = &amp;quot;Dead&amp;quot;,
&amp;quot;Dead_Up&amp;quot; = &amp;quot;Up&amp;quot;,
&amp;quot;Dead_Down&amp;quot; = &amp;quot;Down&amp;quot;
)}, USE.NAMES = F)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;graphical-exploration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graphical exploration&lt;/h2&gt;
&lt;div id=&#34;where-did-i-fish&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where did I fish ?&lt;/h3&gt;
&lt;p&gt;We can visualize the locations I fished the most by using the &lt;em&gt;leaflet&lt;/em&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate the number of fish caught by session 
fish_number &amp;lt;-  catch_cond  %&amp;gt;% na.omit() %&amp;gt;% group_by(Session) %&amp;gt;%  summarise(nb = length(Session))

# Dataframe with variables we want to show on the map
map_data &amp;lt;- catch_cond %&amp;gt;% 
group_by(Session) %&amp;gt;%
select(Session, Time_beg, Time_end, Long,
Lat, Water_level_beg, Tide_status_beg, Tide_time_beg, duration) 

map_data &amp;lt;- full_join(map_data, fish_number)

map_data$nb[is.na(map_data$nb)] &amp;lt;- 0

# Interactive map with Popup for each session
library(leaflet)

leaflet(map_data, width = &amp;quot;100%&amp;quot;) %&amp;gt;% addTiles() %&amp;gt;%
addPopups(lng = ~Long, lat = ~Lat, 
with(map_data, sprintf(&amp;quot;&amp;lt;b&amp;gt;Session %.0f : %.1f h&amp;lt;/b&amp;gt; &amp;lt;br/&amp;gt; %s &amp;lt;br/&amp;gt; %.0f  fish &amp;lt;br/&amp;gt; Water level: %.0f m, %s, %.0f min since last peak&amp;quot;,                                         Session, duration,  Time_beg, nb, Water_level_beg, Tide_status_beg, Tide_time_beg)), 
options = popupOptions(maxWidth = 100, minWidth = 50))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;https://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;https://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addPopups&#34;,&#34;args&#34;:[[43.4903501,43.4903501,43.4903501,43.4948349,43.4948349,43.4948349,43.4948349,43.4948349,43.4929536,43.4929536,43.4717306,43.4856133,43.4856133,43.4898314,43.4879548,43.4891374,43.4888454,43.4903375,43.4907003,43.4883211,43.4870525,43.4882129,43.4902466,43.4879819,43.4879819,43.489656,43.4667765,43.4854263,43.4888221,43.4884741,43.4854276,43.4854276,43.4854276,43.490646,43.490646,43.490646,43.4884784,43.4891888,43.4885291,43.4885291,43.4849954,43.4849954,43.4849954,43.4853693,43.4865331,43.4853571,43.4881548,43.4865984,43.4895741,43.4831239,43.4831239,43.4831239,43.4831239,43.4831239,43.4831239,43.4831239,43.484568,43.484568,43.484568,43.484568,43.484568,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4893757,43.4850086,43.4850086,43.4850086,43.488591],[-1.474845,-1.474845,-1.474845,-1.4702738,-1.4702738,-1.4702738,-1.4702738,-1.4702738,-1.4727054,-1.4727054,-1.4726262,-1.4762295,-1.4762295,-1.4645089,-1.4741375,-1.4619148,-1.4750443,-1.4523802,-1.471115,-1.4737693,-1.4741375,-1.4648611,-1.4522513,-1.4720217,-1.4720217,-1.4784552,-1.4835086,-1.4752964,-1.4768579,-1.4752235,-1.4752846,-1.4752846,-1.4752846,-1.4753465,-1.4753465,-1.4753465,-1.4801831,-1.4717195,-1.4740189,-1.4740189,-1.5000672,-1.5000672,-1.5000672,-1.4762834,-1.4757226,-1.4762743,-1.4748271,-1.4754897,-1.4745318,-1.4726297,-1.4726297,-1.4726297,-1.4726297,-1.4726297,-1.4726297,-1.4726297,-1.4745762,-1.4745762,-1.4745762,-1.4745762,-1.4745762,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.474431,-1.4765871,-1.4765871,-1.4765871,-1.4750231],[&#34;&lt;b&gt;Session 1 : 0.9 h&lt;\/b&gt; &lt;br/&gt; 2019-04-09 19:23:09 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 4 m, Up, 340 min since last peak&#34;,&#34;&lt;b&gt;Session 1 : 0.9 h&lt;\/b&gt; &lt;br/&gt; 2019-04-09 19:23:09 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 4 m, Up, 340 min since last peak&#34;,&#34;&lt;b&gt;Session 1 : 0.9 h&lt;\/b&gt; &lt;br/&gt; 2019-04-09 19:23:09 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 4 m, Up, 340 min since last peak&#34;,&#34;&lt;b&gt;Session 2 : 2.1 h&lt;\/b&gt; &lt;br/&gt; 2019-04-10 18:23:40 &lt;br/&gt; 5  fish &lt;br/&gt; Water level: 3 m, Up, 240 min since last peak&#34;,&#34;&lt;b&gt;Session 2 : 2.1 h&lt;\/b&gt; &lt;br/&gt; 2019-04-10 18:23:40 &lt;br/&gt; 5  fish &lt;br/&gt; Water level: 3 m, Up, 240 min since last peak&#34;,&#34;&lt;b&gt;Session 2 : 2.1 h&lt;\/b&gt; &lt;br/&gt; 2019-04-10 18:23:40 &lt;br/&gt; 5  fish &lt;br/&gt; Water level: 3 m, Up, 240 min since last peak&#34;,&#34;&lt;b&gt;Session 2 : 2.1 h&lt;\/b&gt; &lt;br/&gt; 2019-04-10 18:23:40 &lt;br/&gt; 5  fish &lt;br/&gt; Water level: 3 m, Up, 240 min since last peak&#34;,&#34;&lt;b&gt;Session 2 : 2.1 h&lt;\/b&gt; &lt;br/&gt; 2019-04-10 18:23:40 &lt;br/&gt; 5  fish &lt;br/&gt; Water level: 3 m, Up, 240 min since last peak&#34;,&#34;&lt;b&gt;Session 3 : 2.3 h&lt;\/b&gt; &lt;br/&gt; 2019-04-17 20:45:27 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 1 m, Down, 305 min since last peak&#34;,&#34;&lt;b&gt;Session 3 : 2.3 h&lt;\/b&gt; &lt;br/&gt; 2019-04-17 20:45:27 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 1 m, Down, 305 min since last peak&#34;,&#34;&lt;b&gt;Session 4 : 0.8 h&lt;\/b&gt; &lt;br/&gt; 2019-04-21 17:58:26 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Up, 320 min since last peak&#34;,&#34;&lt;b&gt;Session 5 : 2.0 h&lt;\/b&gt; &lt;br/&gt; 2019-04-22 18:18:23 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 4 m, Up, 305 min since last peak&#34;,&#34;&lt;b&gt;Session 5 : 2.0 h&lt;\/b&gt; &lt;br/&gt; 2019-04-22 18:18:23 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 4 m, Up, 305 min since last peak&#34;,&#34;&lt;b&gt;Session 6 : 1.0 h&lt;\/b&gt; &lt;br/&gt; 2019-04-26 18:30:25 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 110 min since last peak&#34;,&#34;&lt;b&gt;Session 7 : 0.6 h&lt;\/b&gt; &lt;br/&gt; 2019-05-06 18:38:02 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Dead, 15 min since last peak&#34;,&#34;&lt;b&gt;Session 8 : 0.8 h&lt;\/b&gt; &lt;br/&gt; 2019-05-10 19:37:30 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 3 m, Up, 265 min since last peak&#34;,&#34;&lt;b&gt;Session 9 : 1.6 h&lt;\/b&gt; &lt;br/&gt; 2019-05-10 20:23:30 &lt;br/&gt; 1  fish &lt;br/&gt; Water level: 4 m, Up, 315 min since last peak&#34;,&#34;&lt;b&gt;Session 10 : 1.0 h&lt;\/b&gt; &lt;br/&gt; 2019-05-16 16:08:35 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Dead, 30 min since last peak&#34;,&#34;&lt;b&gt;Session 11 : 0.9 h&lt;\/b&gt; &lt;br/&gt; 2019-05-21 19:18:50 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Dead, 15 min since last peak&#34;,&#34;&lt;b&gt;Session 12 : 0.6 h&lt;\/b&gt; &lt;br/&gt; 2019-05-23 19:45:32 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Up, 330 min since last peak&#34;,&#34;&lt;b&gt;Session 13 : 2.2 h&lt;\/b&gt; &lt;br/&gt; 2019-05-29 18:50:42 &lt;br/&gt; 1  fish &lt;br/&gt; Water level: 2 m, Down, 290 min since last peak&#34;,&#34;&lt;b&gt;Session 14 : 1.6 h&lt;\/b&gt; &lt;br/&gt; 2019-05-30 13:27:48 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 3 m, Up, 295 min since last peak&#34;,&#34;&lt;b&gt;Session 15 : 0.9 h&lt;\/b&gt; &lt;br/&gt; 2019-06-01 15:28:24 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Up, 335 min since last peak&#34;,&#34;&lt;b&gt;Session 16 : 0.8 h&lt;\/b&gt; &lt;br/&gt; 2019-06-01 21:28:06 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 1 m, Down, 325 min since last peak&#34;,&#34;&lt;b&gt;Session 16 : 0.8 h&lt;\/b&gt; &lt;br/&gt; 2019-06-01 21:28:06 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 1 m, Down, 325 min since last peak&#34;,&#34;&lt;b&gt;Session 17 : 1.2 h&lt;\/b&gt; &lt;br/&gt; 2019-06-02 14:26:22 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 3 m, Up, 235 min since last peak&#34;,&#34;&lt;b&gt;Session 18 : 1.0 h&lt;\/b&gt; &lt;br/&gt; 2019-06-07 19:20:32 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Up, 315 min since last peak&#34;,&#34;&lt;b&gt;Session 19 : 1.6 h&lt;\/b&gt; &lt;br/&gt; 2019-06-19 18:13:14 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Up, 340 min since last peak&#34;,&#34;&lt;b&gt;Session 20 : 1.1 h&lt;\/b&gt; &lt;br/&gt; 2019-06-22 20:38:02 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Dead, 370 min since last peak&#34;,&#34;&lt;b&gt;Session 21 : 0.8 h&lt;\/b&gt; &lt;br/&gt; 2019-06-26 20:13:34 &lt;br/&gt; 1  fish &lt;br/&gt; Water level: 2 m, Up, 115 min since last peak&#34;,&#34;&lt;b&gt;Session 22 : 0.2 h&lt;\/b&gt; &lt;br/&gt; 2019-06-27 20:05:43 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 2 m, Up, 45 min since last peak&#34;,&#34;&lt;b&gt;Session 22 : 0.2 h&lt;\/b&gt; &lt;br/&gt; 2019-06-27 20:05:43 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 2 m, Up, 45 min since last peak&#34;,&#34;&lt;b&gt;Session 22 : 0.2 h&lt;\/b&gt; &lt;br/&gt; 2019-06-27 20:05:43 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 2 m, Up, 45 min since last peak&#34;,&#34;&lt;b&gt;Session 23 : 3.3 h&lt;\/b&gt; &lt;br/&gt; 2019-06-28 18:55:31 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 2 m, Down, 295 min since last peak&#34;,&#34;&lt;b&gt;Session 23 : 3.3 h&lt;\/b&gt; &lt;br/&gt; 2019-06-28 18:55:31 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 2 m, Down, 295 min since last peak&#34;,&#34;&lt;b&gt;Session 23 : 3.3 h&lt;\/b&gt; &lt;br/&gt; 2019-06-28 18:55:31 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 2 m, Down, 295 min since last peak&#34;,&#34;&lt;b&gt;Session 24 : 1.4 h&lt;\/b&gt; &lt;br/&gt; 2019-07-13 19:06:20 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Down, 250 min since last peak&#34;,&#34;&lt;b&gt;Session 25 : 0.2 h&lt;\/b&gt; &lt;br/&gt; 2019-08-13 21:35:09 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 1 m, Down, 310 min since last peak&#34;,&#34;&lt;b&gt;Session 26 : 1.3 h&lt;\/b&gt; &lt;br/&gt; 2020-05-20 20:47:38 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 2 m, Down, 270 min since last peak&#34;,&#34;&lt;b&gt;Session 26 : 1.3 h&lt;\/b&gt; &lt;br/&gt; 2020-05-20 20:47:38 &lt;br/&gt; 2  fish &lt;br/&gt; Water level: 2 m, Down, 270 min since last peak&#34;,&#34;&lt;b&gt;Session 27 : 0.0 h&lt;\/b&gt; &lt;br/&gt; 2020-05-22 22:51:04 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 1 m, Down, 325 min since last peak&#34;,&#34;&lt;b&gt;Session 27 : 0.0 h&lt;\/b&gt; &lt;br/&gt; 2020-05-22 22:51:04 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 1 m, Down, 325 min since last peak&#34;,&#34;&lt;b&gt;Session 27 : 0.0 h&lt;\/b&gt; &lt;br/&gt; 2020-05-22 22:51:04 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 1 m, Down, 325 min since last peak&#34;,&#34;&lt;b&gt;Session 28 : 1.4 h&lt;\/b&gt; &lt;br/&gt; 2020-05-25 20:44:09 &lt;br/&gt; 1  fish &lt;br/&gt; Water level: 4 m, Down, 110 min since last peak&#34;,&#34;&lt;b&gt;Session 29 : 1.2 h&lt;\/b&gt; &lt;br/&gt; 2020-05-26 21:22:28 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Down, 95 min since last peak&#34;,&#34;&lt;b&gt;Session 30 : 1.3 h&lt;\/b&gt; &lt;br/&gt; 2020-06-01 20:55:54 &lt;br/&gt; 1  fish &lt;br/&gt; Water level: 2 m, Up, 70 min since last peak&#34;,&#34;&lt;b&gt;Session 31 : 2.3 h&lt;\/b&gt; &lt;br/&gt; 2020-06-08 19:56:35 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 4 m, Down, 40 min since last peak&#34;,&#34;&lt;b&gt;Session 32 : 2.0 h&lt;\/b&gt; &lt;br/&gt; 2020-06-15 20:30:55 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 50 min since last peak&#34;,&#34;&lt;b&gt;Session 33 : 1.1 h&lt;\/b&gt; &lt;br/&gt; 2020-06-30 20:59:48 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 105 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 34 : 2.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-16 21:58:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 35 : 2.6 h&lt;\/b&gt; &lt;br/&gt; 2020-07-17 21:51:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 20 min since last peak&#34;,&#34;&lt;b&gt;Session 35 : 2.6 h&lt;\/b&gt; &lt;br/&gt; 2020-07-17 21:51:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 20 min since last peak&#34;,&#34;&lt;b&gt;Session 35 : 2.6 h&lt;\/b&gt; &lt;br/&gt; 2020-07-17 21:51:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 20 min since last peak&#34;,&#34;&lt;b&gt;Session 35 : 2.6 h&lt;\/b&gt; &lt;br/&gt; 2020-07-17 21:51:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 20 min since last peak&#34;,&#34;&lt;b&gt;Session 35 : 2.6 h&lt;\/b&gt; &lt;br/&gt; 2020-07-17 21:51:40 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 20 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 36 : 1.5 h&lt;\/b&gt; &lt;br/&gt; 2020-07-18 23:25:07 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 80 min since last peak&#34;,&#34;&lt;b&gt;Session 37 : 1.6 h&lt;\/b&gt; &lt;br/&gt; 2020-09-04 20:50:17 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 3 m, Down, 125 min since last peak&#34;,&#34;&lt;b&gt;Session 37 : 1.6 h&lt;\/b&gt; &lt;br/&gt; 2020-09-04 20:50:17 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 3 m, Down, 125 min since last peak&#34;,&#34;&lt;b&gt;Session 37 : 1.6 h&lt;\/b&gt; &lt;br/&gt; 2020-09-04 20:50:17 &lt;br/&gt; 3  fish &lt;br/&gt; Water level: 3 m, Down, 125 min since last peak&#34;,&#34;&lt;b&gt;Session 38 : 0.6 h&lt;\/b&gt; &lt;br/&gt; 2020-09-13 21:33:41 &lt;br/&gt; 0  fish &lt;br/&gt; Water level: 2 m, Up, 50 min since last peak&#34;],null,null,{&#34;maxWidth&#34;:100,&#34;minWidth&#34;:50,&#34;autoPan&#34;:true,&#34;keepInView&#34;:false,&#34;closeButton&#34;:true,&#34;className&#34;:&#34;&#34;}]}],&#34;limits&#34;:{&#34;lat&#34;:[43.4667765,43.4948349],&#34;lng&#34;:[-1.5000672,-1.4522513]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As you see I fish mostly in the Nive river that is flowing through Bayonne city.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-it-is-best-to-fish&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;When it is best to fish ?&lt;/h3&gt;
&lt;div id=&#34;time-of-the-year&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Time of the year&lt;/h4&gt;
&lt;p&gt;The following graph shows the number of fish caught depending on the time of the year :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;catch_cond %&amp;gt;% 
group_by(Session, Time_beg, .drop = F) %&amp;gt;% 
na.omit() %&amp;gt;% 
summarise(n_catch = n()) %&amp;gt;% 
right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Time_beg&amp;quot;)])) %&amp;gt;% 
mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;%
ggplot(aes(y = n_catch, x =Time_beg)) +
geom_point( size = 2) + 
  theme_minimal() + labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Number of catch&amp;quot;) + scale_x_datetime(date_labels = &amp;quot;%d/%m/%y&amp;quot;, date_breaks = &amp;quot;3 months&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this graph we see that I didn’t go fishing during the autumn and winter of 2019, I don’t have any data. Unfortunately for me, autumn is known to be a great period for sea bass fishing, I must go fishing this year to compensate the lack of data in this season. During winter, fishing is really complicated because the large majority of sea bass are returned to the ocean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-of-the-day&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Time of the day&lt;/h4&gt;
&lt;p&gt;This graph shows the number of fish I catch depending on the hour of the day :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;catch_cond %&amp;gt;% 
group_by(Session, Time_beg, .drop = F) %&amp;gt;% 
na.omit() %&amp;gt;% 
summarise(n_catch = n()) %&amp;gt;% 
right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Time_beg&amp;quot;)])) %&amp;gt;% 
mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch ), 
hour = format(Time_beg, &amp;quot;%H&amp;quot;)) %&amp;gt;%
ggplot(aes(y = n_catch, x =hour)) +
geom_point( size = 2)  + labs(x = &amp;quot;Hour&amp;quot;, y = &amp;quot;Number of catch&amp;quot;)+
theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-6-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I mostly fish after work or during evenings. To draw relevant conclusions about the influence of the fishing hour, I have to go fishing at different hours of the day (in the morning for example).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-tide&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The tide&lt;/h4&gt;
&lt;p&gt;The tide is an important parameter for fishing in estuaries. Let’s see the effect of the tide current on my catches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggpubr)

gg1 &amp;lt;- catch_cond %&amp;gt;% 
  group_by(Session, Tide_status_ses, .drop = F)  %&amp;gt;%  
  drop_na() %&amp;gt;% 
  summarise(n_catch = n()) %&amp;gt;% 
  right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Tide_status_ses&amp;quot;)])) %&amp;gt;% 
  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;%
  ggplot(aes(y = n_catch, x = Tide_status_ses, fill = Tide_status_ses)) +
  geom_boxplot() +
  labs(x = &amp;quot;Status of tide current&amp;quot;, y = &amp;quot;Number of catch&amp;quot;) +
  theme_minimal()+ theme(legend.position=&amp;quot;None&amp;quot;)

gg2 &amp;lt;- catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;% 
  ggplot(aes(y = length,x = Tide_status_ses, fill = Tide_status_ses)) +
  geom_boxplot()+
  labs(x = &amp;quot;Status of tide current&amp;quot;, y = &amp;quot;Length of the fish&amp;quot;) +
  theme_minimal()+ theme(legend.position=&amp;quot;None&amp;quot;)

ggarrange(gg1, gg2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-7-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It seems that the status of the tide current does not influence the number of my catch but influences the length of the fish. I tend to catch bigger fish when the current is going down.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;does-the-moon-affect-my-fishing-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Does the moon affect my fishing results?&lt;/h3&gt;
&lt;p&gt;A widespread belief among fishermen is that the moon influences greatly the behavior of the fish. Data about the moon phase were available thanks to the weather API, I decided to record this variable to investigate if the belief was true. The two graphs show the number and length of fish depending on the phase of moon (0 corresponding to new moon and 1 to full moon):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg3 &amp;lt;- catch_cond %&amp;gt;% 
  group_by(Session, Moon, .drop = F) %&amp;gt;%  
  na.omit() %&amp;gt;% 
  summarise(n_catch = n()) %&amp;gt;% 
  right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Moon&amp;quot;)])) %&amp;gt;% 
  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;% 
  ggplot(aes(y = n_catch, x = Moon)) +
  geom_point( size = 2) +
  labs(x = &amp;quot;Moon phase&amp;quot;, y = &amp;quot;Number of catch&amp;quot;)+
  theme_minimal()

gg4 &amp;lt;- catch_cond %&amp;gt;% 
  ggplot(aes(y = length, x = Moon)) +
  geom_point( size = 2) +
  geom_smooth(method=&amp;quot;lm&amp;quot;, se=T) + 
  labs(x = &amp;quot;Moon phase&amp;quot;, y = &amp;quot;Length of the fish&amp;quot;)+
  theme_minimal()

ggarrange(gg3, gg4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The phase of the moon does not seem to influence the number of fish I catch during a session. However, I tend to catch bigger fish the closer we are to the full moon. To confirm this observation, I need to keep going fishing to get more data !&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;does-the-weather-affect-my-fishing-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Does the weather affect my fishing results?&lt;/h3&gt;
&lt;p&gt;We can look at the number of fish caught during different weather conditions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# precipitation probability 

gg5 &amp;lt;- catch_cond %&amp;gt;% 
  group_by(Session, Preci_prob, .drop = F) %&amp;gt;%  
  na.omit() %&amp;gt;% 
  summarise(n_catch = n()) %&amp;gt;% 
  right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Preci_prob&amp;quot;)])) %&amp;gt;% 
  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;% 
  ggplot(aes(y = n_catch, x = Preci_prob)) +
  geom_point()+
  labs(x = &amp;quot;Precipitation prob.&amp;quot;, y = &amp;quot;Number of catch&amp;quot;)+
  theme_minimal()

# Atm pressure 

gg6 &amp;lt;- catch_cond %&amp;gt;% 
  group_by(Session, Atm_pres, .drop = F) %&amp;gt;%  
  na.omit() %&amp;gt;% 
  summarise(n_catch = n()) %&amp;gt;% 
  right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Atm_pres&amp;quot;)])) %&amp;gt;% 
  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;% 
  ggplot(aes(y = n_catch, x = Atm_pres)) +
  geom_point() +
  labs(x = &amp;quot;Atm. pressure&amp;quot;, y = &amp;quot;Number of catch&amp;quot;)+
  theme_minimal()

#Air temp

gg7 &amp;lt;- catch_cond %&amp;gt;% 
  group_by(Session, Air_temp, .drop = F) %&amp;gt;%  
  na.omit() %&amp;gt;% 
  summarise(n_catch = n()) %&amp;gt;% 
  right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Air_temp&amp;quot;)])) %&amp;gt;% 
  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;% 
  ggplot(aes(y = n_catch, x = Air_temp)) +
  geom_point() +
  labs(x = &amp;quot;Air temp.&amp;quot;, y = &amp;quot;Number of catch&amp;quot;)+
  theme_minimal()


#Cloud cover

gg8 &amp;lt;- catch_cond %&amp;gt;% 
  group_by(Session, Cloud_cover, .drop = F) %&amp;gt;%  
  na.omit() %&amp;gt;% 
  summarise(n_catch = n()) %&amp;gt;% 
  right_join(unique(catch_cond[, c(&amp;quot;Session&amp;quot;, &amp;quot;Cloud_cover&amp;quot;)])) %&amp;gt;% 
  mutate(n_catch = ifelse(is.na(n_catch), 0, n_catch )) %&amp;gt;% 
  ggplot(aes(y = n_catch, x = Cloud_cover)) +
  geom_point() +
  labs(x = &amp;quot;Cloud cover&amp;quot;, y = &amp;quot;Number of catchh&amp;quot;)+
  theme_minimal()

ggarrange(gg5, gg6, gg7, gg8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And to their length :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg15 &amp;lt;- catch_cond %&amp;gt;% 
  ggplot(aes(y = length, x = Preci_prob)) +
  geom_point( size = 2) +
  labs( y = &amp;quot;Length of the fish&amp;quot;)+
  theme_minimal()

gg16 &amp;lt;- catch_cond %&amp;gt;% 
  ggplot(aes(y = length, x = Atm_pres)) +
  geom_point( size = 2) +
  labs( y = &amp;quot;Length of the fish&amp;quot;)+
  theme_minimal()

gg17 &amp;lt;- catch_cond %&amp;gt;% 
  ggplot(aes(y = length, x = Air_temp)) +
  geom_point( size = 2) +
  labs( y = &amp;quot;Length of the fish&amp;quot;)+
  theme_minimal()


gg18  &amp;lt;- catch_cond %&amp;gt;% 
  ggplot(aes(y = length, x = Cloud_cover)) +
  geom_point( size = 2) +
  labs( y = &amp;quot;Length of the fish&amp;quot;)+
  theme_minimal()

ggarrange(gg15, gg16, gg17, gg18)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-10-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because we have limited data and not all weather conditions are covered, it is difficult to draw any conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-the-best-lures-to-catch-fish&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What are the best lures to catch fish ?&lt;/h3&gt;
&lt;p&gt;Each time I catch a fish, I fill a form with my shiny application in order to record the characteristics of the lure used. There are different types of lures that have specific swimming patterns, different colors and size. We can represent the number of fish caught depending on the lure characteristics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(catch_cond$colour) &amp;lt;- c(&amp;quot;clear&amp;quot;, &amp;quot;natural&amp;quot;, &amp;quot;dark&amp;quot;)
levels(catch_cond$length_lure) &amp;lt;- c(&amp;quot;medium&amp;quot;, &amp;quot;small&amp;quot;)

gg9 &amp;lt;- catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;% 
  ggplot( aes(x=lure, fill = lure)) +
  geom_bar(stat=&amp;quot;count&amp;quot;, width=0.7)+
  labs(x = &amp;quot;Type of lure&amp;quot;, y = &amp;quot;&amp;quot;)+
  theme_minimal()+ 
  theme(legend.position=&amp;quot;None&amp;quot;)

gg10 &amp;lt;- catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;% 
  ggplot( aes(x=colour, fill = colour)) +
  geom_bar(stat=&amp;quot;count&amp;quot;, width=0.7)+
  labs(x = &amp;quot;Color of the lures&amp;quot;, y = &amp;quot;&amp;quot;)+
  theme_minimal()+
    scale_fill_brewer(palette=&amp;quot;BuPu&amp;quot;)+ 
  theme(legend.position=&amp;quot;None&amp;quot;)

gg11 &amp;lt;- catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;%
  ggplot( aes(x=length_lure, fill = length_lure)) +
  geom_bar(stat=&amp;quot;count&amp;quot;, width=0.7)+
  labs(x = &amp;quot;Size of the lure&amp;quot;, y = &amp;quot;&amp;quot;)+
    scale_fill_brewer(palette=&amp;quot;Dark2&amp;quot;)+
  theme_minimal()+ theme(legend.position=&amp;quot;None&amp;quot;)

annotate_figure(ggarrange(gg9, gg10, gg11, ncol = 3),
                left = text_grob(&amp;quot;Number of catch&amp;quot;, rot = 90)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-11-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do the same for the length of fish caught:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg12 &amp;lt;-catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;% 
  ggplot(aes(y = length, x = lure, fill=lure)) +
  geom_boxplot()+
  labs(x = &amp;quot;Type of lure&amp;quot;, y = &amp;quot;&amp;quot;)+
  theme_minimal()+ theme(legend.position=&amp;quot;None&amp;quot;)

gg13 &amp;lt;-catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;%
  ggplot(aes(y = length, x = colour, fill= colour)) +
  geom_boxplot()+
  labs(x = &amp;quot;Color of the lures&amp;quot;, y = &amp;quot;&amp;quot;)+
  theme_minimal()+
    scale_fill_brewer(palette=&amp;quot;BuPu&amp;quot;)+ theme(legend.position=&amp;quot;None&amp;quot;)

gg14 &amp;lt;-catch_cond %&amp;gt;% 
  na.omit() %&amp;gt;%
  ggplot(aes(y = length, x = length_lure, fill=length_lure)) +
  geom_boxplot()+
  labs(x = &amp;quot;Size of the lure&amp;quot;, y = &amp;quot;&amp;quot;)+
  theme_minimal()+
    scale_fill_brewer(palette=&amp;quot;Dark2&amp;quot;)+ theme(legend.position=&amp;quot;None&amp;quot;)

annotate_figure(ggarrange(gg12, gg13, gg14, ncol = 3),
                left = text_grob(&amp;quot;Length of fish&amp;quot;, rot = 90)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://aureliencallens.github.io/post/2020-09-25-ds-fishing-part2.en_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With these 6 graphs, we can see that the most successful types of lures for me are the shad and slug types. An honorable mention is the jerkbait type: it only accounts for 2 fish, but 2 big fish (median around 47cm). The colors that worked best for me were clear and natural. For the size of the lure, bigger lures tend to catch bigger fish in average. These conclusions must be taken with a grain of salt because we do not know the time spent with each lure before catching a fish. In addition, I tend to use the same types and colors of lures (habits), I should vary more.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Analyzing my fishing data was very interesting and it brought me some insights on my fishing style! I understood that I was fishing almost the same way with the same habits. Although it seems to be working for me, I have a biased view on how to catch European sea bass. I must use bigger lures to catch bigger fish and I must vary the types of lures used. Indeed, I fish most of the times with slug or shad lures, hence the higher number of fish caught with these types of lures.&lt;/p&gt;
&lt;p&gt;I will keep using the application to gather more data and have a better understanding on my fishing session. I will keep you updated on the results ! :wink:&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can R and Shiny make me a better fisherman? Part 1</title>
      <link>https://aureliencallens.github.io/2020/09/12/r-shiny-fishing-part1/</link>
      <pubDate>Sat, 12 Sep 2020 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2020/09/12/r-shiny-fishing-part1/</guid>
      <description>


&lt;p&gt;My favorite hobby, in addition to R coding of course, is fishing. Most of the time, I am fishing European sea bass (&lt;em&gt;Dicentrarchus labrax&lt;/em&gt;) in estuaries. The sea bass is a predatory fish that has a broad range of preys: crabs, sand eels, prawns, shrimps and other fish. To catch these predators, I don’t use live baits, I prefer to use artificial lures that imitate a specific prey.&lt;/p&gt;
&lt;p&gt;In theory, it is quite easy to catch a fish:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use a lure that imitate the current prey of the sea bass.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Animate the lure in a spot where the fish are active.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Catch a really big fish !&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In practice, it is an other story ! Indeed, the feeding activity, the position of the European sea bass in the estuary and their preys will vary depending on different parameters :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the characteristics of the riverbed, which will depend where I fish&lt;/li&gt;
&lt;li&gt;the time of the day : the sea bass is more active during dawn and dusk&lt;/li&gt;
&lt;li&gt;the current and water level associated with the tide. The water level in estuaries is constantly varying to greater or lesser degree due to the tide influence. It is also influenced by the river flow which can be higher in case of heavy rains.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you understand, there are many parameters potentially influencing the results of my fishing session. This is why I decided to create a shiny application to augment the number and the length of the fish caught during my sessions. To reach this objective, I need to better understand the activity, the position and the prey of the sea bass depending on the parameters described above.&lt;/p&gt;
&lt;div id=&#34;requirements-of-my-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Requirements of my application&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;It must store data about my fishing session :&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Information needed&lt;/th&gt;
&lt;th&gt;Description of the variables&lt;/th&gt;
&lt;th&gt;Where do I get the data ?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Time&lt;/td&gt;
&lt;td&gt;Time when a fish is caught, time since the beginning of the session&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Catch&lt;/td&gt;
&lt;td&gt;Species and length of the fish caught&lt;/td&gt;
&lt;td&gt;Geolocation from smartphone?&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Lures&lt;/td&gt;
&lt;td&gt;Type, length, color of lure used&lt;/td&gt;
&lt;td&gt;Weather API&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;It must record data about my catch and the artificial lures used :&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Information needed&lt;/th&gt;
&lt;th&gt;Description of the variables&lt;/th&gt;
&lt;th&gt;Where do I get the data ?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Time&lt;/td&gt;
&lt;td&gt;Time when a fish is caught, time since the beginning of the session&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Catch&lt;/td&gt;
&lt;td&gt;Species and length of the fish caught&lt;/td&gt;
&lt;td&gt;User input&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Lures&lt;/td&gt;
&lt;td&gt;Type, length, color of lure used&lt;/td&gt;
&lt;td&gt;User input&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It must be adapted to small screens because I will always use the application on my phone.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It must remain free.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;collecting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Collecting the data&lt;/h2&gt;
&lt;div id=&#34;getting-my-gps-location&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting my gps location&lt;/h3&gt;
&lt;p&gt;My gps location is collected by using a bit of Javascript in the header of the shiny application. This code has been developed by AugusT and is available on his &lt;a href=&#34;https://github.com/AugustT/shiny_geolocation&#34; target=&#34;_blank&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weather-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weather API&lt;/h3&gt;
&lt;p&gt;For the weather data, I found a free API called Dark Sky. I made a function that takes as input the coordinates of a place and the API user key and returns the current weather conditions in a dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(httr)
library(jsonlite)
library(tidyverse)
library(rvest)

weather &amp;lt;- function(x, API_key){
  url &amp;lt;- paste0(&amp;quot;https://api.darksky.net/forecast/&amp;quot;,API_key,
                &amp;quot;/&amp;quot;, x[1], &amp;quot;,&amp;quot;, x[2],
                &amp;quot;?units=ca&amp;amp;exclude=hourly,alerts,flags&amp;quot;)
  
  rep &amp;lt;- GET(url)
  
  table &amp;lt;- fromJSON(content(rep, &amp;quot;text&amp;quot;))
  
  current.weather.info &amp;lt;- with(table,
                               data.frame(Air_temp = currently$temperature,
                                     Weather = currently$summary,
                                     Atm_pres = currently$pressure,
                                     Wind_str = currently$windSpeed,
                                     Wind_dir = currently$windBearing,
                                     Cloud_cover = currently$cloudCover,
                                     PrecipProb = currently$precipProbability,
                                     PrecipInt = currently$precipIntensity,  
                                     Moon = daily$data$moonPhase[1]))
  return(current.weather.info)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;web-scrapping-for-tide-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Web scrapping for Tide data&lt;/h3&gt;
&lt;p&gt;I created a function to scrap information about the tide on a french website. The following function takes no argument and return the current water level, the tide status (going up or down) and time since the tide peak for the location I fish.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tide &amp;lt;- function(){
  
  # Set the current time and time zone 
  Sys.setenv(TZ=&amp;quot;Europe/Paris&amp;quot;)
  time &amp;lt;- as.POSIXct(Sys.time())
  url &amp;lt;- &amp;quot;https://services.data.shom.fr/hdm/vignette/grande/BOUCAU-BAYONNE?locale=en&amp;quot;
  
  # Read the web page that contains the tide data 
  text &amp;lt;- url %&amp;gt;% 
    read_html() %&amp;gt;%
    html_text()
  
  # Clean the html data to get a dataframe  with two cols Time and water level: 

  text &amp;lt;- as.character(sub(&amp;quot;.*var data = *(.*?) *\\;.*&amp;quot;, &amp;quot;\\1&amp;quot;, text))
  text &amp;lt;- unlist(str_split( substr(text, 1, nchar(text)-2), &amp;quot;\\],&amp;quot;))
  tidy_df &amp;lt;- data.frame(hour=NA,Water=NA)
  
  for(i in 1:length(text)){
    text_dat &amp;lt;- unlist(str_split(text[i], &amp;#39;&amp;quot;&amp;#39;))[c(2,3)]
    text_dat[1] &amp;lt;- substr(text_dat[1], 1, nchar(text_dat[1])-1)
    text_dat[2] &amp;lt;- as.numeric(substr(text_dat[2], 2, nchar(text_dat[2])))
    tidy_df[i,] &amp;lt;- text_dat
  }
  
  tidy_df$hour &amp;lt;- as.POSIXct(paste(format(Sys.time(),&amp;quot;%Y-%m-%d&amp;quot;), tidy_df$hour))
  
  # Some lines to get the tide status (going down or up) : 
  
  n_closest &amp;lt;- which(abs(tidy_df$hour - time) == min(abs(tidy_df$hour - time)))
  
  water_level &amp;lt;- as.numeric(tidy_df[n_closest, 2])
  
  all_decrea &amp;lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==
                      cummin(tidy_df$Water[(n_closest-6):(n_closest+6)] ))
  
  all_increa &amp;lt;- all(tidy_df$Water[(n_closest-6):(n_closest+6)] ==
                      cummax(tidy_df$Water[(n_closest-6):(n_closest+6)] ))
  
  maree &amp;lt;- ifelse(all_decrea, &amp;quot;Down&amp;quot;, ifelse(all_increa, &amp;quot;Up&amp;quot;, &amp;quot;Dead&amp;quot;))
  
  
  # Compute time since the last peak :
  
  last_peak &amp;lt;- max(cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &amp;gt; 0)$lengths)
                   [cumsum(rle(diff(as.numeric(tidy_df$Water), lag = 2) &amp;gt;0)$lengths) &amp;lt; n_closest])
  
  
  time_after &amp;lt;- as.numeric(difftime(tidy_df$hour[n_closest], tidy_df$hour[last_peak], units = &amp;quot;mins&amp;quot;))
  
  
  # Return the list with the results :
  
  return(list(Water_level = water_level,
              Maree = maree,
              Time_peak = time_after))
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-shiny-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The shiny application&lt;/h2&gt;
&lt;p&gt;The main problem I encountered while developing this application was data storage. Shinyapps.io
host freely your shiny application but there were some problems when I used the shiny application to modify the csv files.
The solution I found was to store the data in my dropbox account, you can find &lt;a href=&#34;https://shiny.rstudio.com/articles/persistent-data-storage.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; more details on the subject and alternatives solutions. I used the package &lt;em&gt;rdrop2&lt;/em&gt; to access and modify the data with the shiny application.&lt;/p&gt;
&lt;p&gt;Here are the main steps of this application :&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;When the application is started, it reads a csv file stored on my dropbox to see if a fishing session is running or not. If not the user can start a fishing session.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When starting a new session, a line with coordinates, weather conditions, and tide condition is added to the csv file previously mentioned.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a fish is caught, the user can fill out a form to store the data in a second csv file. This file contains : the time, the species and length of the fish and information about the fishing lure used (type, color, length).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The user can end the fishing session by pushing a button. This will register the ending time, weather conditions, and tide condition in the first csv file.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A simplified graph is showed below:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://aureliencallens.github.io/img_post/graph.svg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Simplified workflow of the application&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ui-side&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;UI side&lt;/h3&gt;
&lt;p&gt;The user interface of the application is built using the &lt;em&gt;miniUI&lt;/em&gt; package. This package
allows R user to develop shiny application adapted to small screens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries 
library(shiny)
library(shinyWidgets)
library(googlesheets)
library(miniUI)
library(leaflet)
library(rdrop2)
Sys.setenv(TZ=&amp;quot;Europe/Paris&amp;quot;)

#Import the functions for weather API and webscrapping 
suppressMessages(source(&amp;quot;api_functions.R&amp;quot;))


# Load the dropbox token : 
token &amp;lt;&amp;lt;- readRDS(&amp;quot;token.rds&amp;quot;)

# Minipage for small screens
ui &amp;lt;- miniPage(
  # Javascript that give user location (input$lat,input$long)
  tags$script(&amp;#39;$(document).ready(function () {
                           navigator.geolocation.getCurrentPosition(onSuccess, onError);
                           
                           function onError (err) {
                           Shiny.onInputChange(&amp;quot;geolocation&amp;quot;, false);
                           }
                           
                           function onSuccess (position) {
                           setTimeout(function () {
                           var coords = position.coords;
                           console.log(coords.latitude + &amp;quot;, &amp;quot; + coords.longitude);
                           Shiny.onInputChange(&amp;quot;geolocation&amp;quot;, true);
                           Shiny.onInputChange(&amp;quot;lat&amp;quot;, coords.latitude);
                           Shiny.onInputChange(&amp;quot;long&amp;quot;, coords.longitude);
                           }, 1100)
                           }
                           });&amp;#39;),
  
  gadgetTitleBar(&amp;quot;Catch them all&amp;quot;, left = NULL, right = NULL),
  
  miniTabstripPanel(
    #First panel depends if a fishing session is started or not 
    miniTabPanel(&amp;quot;Session&amp;quot;, icon = icon(&amp;quot;sliders&amp;quot;),
                 miniContentPanel(uiOutput(&amp;quot;UI_sess&amp;quot;, align = &amp;quot;center&amp;quot;),
                                  uiOutput(&amp;quot;UI&amp;quot;, align = &amp;quot;center&amp;quot;))
    ),
    # Second panel displays the location of the previous fishing session with the number of fish caught 
    miniTabPanel(&amp;quot;Map&amp;quot;, icon = icon(&amp;quot;map-o&amp;quot;),
                 miniContentPanel(scrollable = FALSE,padding = 0,
                                  div(style=&amp;quot;text-align:center&amp;quot;,
                                      prettyRadioButtons(&amp;quot;radio&amp;quot;, inline = TRUE, label = &amp;quot;&amp;quot;,
                                                         choices = list(&amp;quot;3 dernières sessions&amp;quot; = 1,
                                                                        &amp;quot;3 Meilleures Sessions&amp;quot; = 2,
                                                                        &amp;quot;Tout afficher&amp;quot; = 3), 
                                                         selected = 1)),
                                  leafletOutput(&amp;quot;map&amp;quot;, height = &amp;quot;93%&amp;quot;)
                 ))
  )
  
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;server-side&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Server side&lt;/h3&gt;
&lt;p&gt;The server side is mainly composed by observeEvent functions. The utility of each
observeEvent is provided in the script as commentary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;server &amp;lt;- function(input, output, session){
  source(&amp;quot;api_functions.R&amp;quot;)
  
  # Read the csv file containing information about fishing session. If a session is running,
  # display the UI that allows the user to input data about the fish caught. If a session is not started,
  # display a button to start the session.
  
  observeEvent(input$go ,{
    
    dat &amp;lt;&amp;lt;- drop_read_csv(&amp;quot;/app_peche/session.csv&amp;quot;, header = T, stringsAsFactors = F, dtoken = token) 
    
    output$UI&amp;lt;- renderUI({
      tagList(
        if(rev(dat$Status)[1] == &amp;quot;end&amp;quot;){
          actionButton(&amp;quot;go&amp;quot;,&amp;quot;Start session&amp;quot;)}
        else{
          actionButton(&amp;quot;go&amp;quot;,&amp;quot;End session&amp;quot;) 
        }
      )
    })
    
    output$UI_sess&amp;lt;- renderUI({
      if(rev(dat$Status)[1] == &amp;quot;end&amp;quot;){
        tagList(textInput(&amp;quot;comments&amp;quot;, label = h3(&amp;quot;Commentaires&amp;quot;), value = &amp;quot;NA&amp;quot;))
      }else{
        input$catch
        
        tagList(
          selectInput(&amp;quot;species&amp;quot;, label = h3(&amp;quot;Espèces&amp;quot;), 
                      choices = list(&amp;quot;Bar&amp;quot; = &amp;quot;bar&amp;quot;, 
                                     &amp;quot;Bar moucheté&amp;quot; = &amp;quot;bar_m&amp;quot;, 
                                     &amp;quot;Alose&amp;quot; = &amp;quot;alose&amp;quot;,
                                     &amp;quot;Alose Feinte&amp;quot; = &amp;quot;alose_f&amp;quot;,
                                     &amp;quot;Maquereau&amp;quot; = &amp;quot;maquereau&amp;quot;, 
                                     &amp;quot;Chinchard&amp;quot; = &amp;quot;chinchard&amp;quot;), selected = &amp;quot;bar&amp;quot;),
          
          sliderInput(&amp;quot;length&amp;quot;,label = h3(&amp;quot;Taille du poisson&amp;quot;),value=25,min=0,max=80, step=1),
          
          selectInput(&amp;quot;lure&amp;quot;, label = h3(&amp;quot;Type de leurre&amp;quot;), 
                      choices = list(&amp;quot;Shad&amp;quot; = &amp;quot;shad&amp;quot;,
                                     &amp;quot;Slug&amp;quot; = &amp;quot;slug&amp;quot;,
                                     &amp;quot;Jerkbait&amp;quot; = &amp;quot;jerkbait&amp;quot;,
                                     &amp;quot;Casting jig&amp;quot; = &amp;quot;jig&amp;quot;,
                                     &amp;quot;Topwater&amp;quot; = &amp;quot;topwater&amp;quot;), selectize = FALSE),
          
          selectInput(&amp;quot;color_lure&amp;quot;, label = h3(&amp;quot;Couleur du leurre&amp;quot;), 
                      choices = list(&amp;quot;Naturel&amp;quot; = &amp;quot;naturel&amp;quot;,
                                     &amp;quot;Sombre&amp;quot; = &amp;quot;sombre&amp;quot;,
                                     &amp;quot;Clair&amp;quot; = &amp;quot;clair&amp;quot;,
                                     &amp;quot;Flashy&amp;quot; = &amp;quot;flashy&amp;quot; ), selectize = FALSE),
          
          selectInput(&amp;quot;length_lure&amp;quot;, label = h3(&amp;quot;Taille du leurre&amp;quot;), 
                      choices = list(&amp;quot;Petit&amp;quot; = &amp;quot;petit&amp;quot;,
                                     &amp;quot;Moyen&amp;quot; = &amp;quot;moyen&amp;quot;,
                                     &amp;quot;Grand&amp;quot; = &amp;quot;grand&amp;quot;), selectize = FALSE),
          
          actionButton(&amp;quot;catch&amp;quot;,&amp;quot;Rajoutez cette capture aux stats!&amp;quot;),
          
          textInput(&amp;quot;comments1&amp;quot;, label = h3(&amp;quot;Commentaire avant la fin ?&amp;quot;), value = &amp;quot;NA&amp;quot;)
          
          
        )
        
        
      }
      
    })  
    
    
  }, ignoreNULL = F)
  
  #If the button is pushed, create the line to be added in the csv file. 
  
  observeEvent(input$go,{
    
    #Tide + geoloc + Weather
    c_tide &amp;lt;- unlist(tide())
    geoloc &amp;lt;- c(input$lat,input$long)
    current.weather.info &amp;lt;- weather(geoloc) 
    
    # Two outcomes depending if the session starts or ends. This gives the possibility 
    # to the user to add a comment before starting the session or after ending the session
    
    if(rev(dat$Status)[1] == &amp;quot;end&amp;quot;){
      
      n_ses &amp;lt;- c(rev(dat$Session)[1]+1)
      stat_ses &amp;lt;- c(&amp;quot;beg&amp;quot;)
      time_beg &amp;lt;- as.character(as.POSIXct(Sys.time()))
      comment &amp;lt;- input$comments
      dat.f &amp;lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment)
      names(dat.f)&amp;lt;-names(dat)
      a &amp;lt;- rbind(dat,dat.f)
      
    }else{
      
      n_ses &amp;lt;- c(rev(dat$Session)[1])
      stat_ses &amp;lt;- c(&amp;quot;end&amp;quot;)
      time_beg &amp;lt;- as.character(as.POSIXct(Sys.time()))
      comment1 &amp;lt;- input$comments1
      dat.f&amp;lt;- data.frame(n_ses, stat_ses, time_beg ,geoloc[2], geoloc[1], current.weather.info, c_tide[1], c_tide[2], c_tide[3], comment1)
      names(dat.f)&amp;lt;-names(dat)
      a &amp;lt;- rbind(dat,dat.f)
    }
    
    # Write csv in temporary files of shiny server 
    write_csv(as.data.frame(a), &amp;quot;session.csv&amp;quot;)
    
    # Upload it to dropbox account 
    drop_upload(&amp;quot;session.csv&amp;quot;, path = &amp;quot;App_peche&amp;quot;, mode = &amp;quot;overwrite&amp;quot;, dtoken = token)
  })
  
  
  # Add a line to the catch csv file whenever a fish is caught
  observeEvent(input$catch,{
    caugth &amp;lt;- drop_read_csv(&amp;quot;/app_peche/catch.csv&amp;quot;, header = T, stringsAsFactors = F, dtoken = token) 
    
    n_ses &amp;lt;- c(rev(dat$Session)[1])
    time &amp;lt;- as.POSIXct(Sys.time())
    time_after_beg &amp;lt;- round(as.numeric(difftime(time, rev(dat$Time)[1], units = &amp;quot;mins&amp;quot;)), digits = 0)
    
    catch &amp;lt;- data.frame(n_ses, 
                        time = as.character(time),
                        min_fishing = as.character(time_after_beg),
                        species = input$species,
                        length = input$length,
                        lure = input$lure,
                        colour = input$color_lure,
                        length_lure = input$length_lure)
    
    b &amp;lt;- rbind(caugth,catch)
    
    # Write csv in temporary files of shiny server 
    write_csv(as.data.frame(b), &amp;quot;catch.csv&amp;quot;)
    # Upload it to dropbox account 
    drop_upload(&amp;quot;catch.csv&amp;quot;, path = &amp;quot;App_peche&amp;quot;, mode = &amp;quot;overwrite&amp;quot;, dtoken = token)
  })
  
  # Create the map with the results of previous session depending on the choice of the user :
  
  observeEvent(input$radio,{
    
    output$map &amp;lt;- renderLeaflet({
      map_data &amp;lt;- map_choice(input$radio)
      leaflet(map_data) %&amp;gt;% addTiles() %&amp;gt;%
        addPopups(lng = ~Long,
                  lat = ~Lat, 
                  with(map_data,
                       sprintf(&amp;quot;&amp;lt;b&amp;gt;Session %.0f : %.1f h&amp;lt;/b&amp;gt; &amp;lt;br/&amp;gt; %s &amp;lt;br/&amp;gt; %.0f  poissons &amp;lt;br/&amp;gt; hauteur d&amp;#39;eau: %.0f m, %s, %.0f min après l&amp;#39;étal&amp;quot;,
                               n_ses,
                               duration,
                               Time,
                               nb,
                               Water_level,
                               Tide_status,
                               Tide_time)),
                  options = popupOptions(maxWidth = 100, minWidth = 50))
    })
    
  })
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-future-improvments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion and future improvments&lt;/h2&gt;
&lt;p&gt;You can find a dummy example of this application (not linked to the dropbox account)
&lt;a href=&#34;https://aureliencallens.shinyapps.io/Dummy_angler_app/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
I have been using this application for 1 year without any problems! The data I collected will be presented in the next post.&lt;/p&gt;
&lt;p&gt;In the coming months, I must find a new free API to replace the actual one. Indeed, the weather API I am using has been bought by Apple and the free requests will be stopped in the following year.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What do I do here ?</title>
      <link>https://aureliencallens.github.io/2020/09/01/first-post/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <author>aurelien.callens@gmail.com (Aurelien Callens)</author>
      <guid>https://aureliencallens.github.io/2020/09/01/first-post/</guid>
      <description>


&lt;p&gt;Here, I will present my projects and write articles on different subjects such as :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Statistical analysis&lt;/li&gt;
&lt;li&gt;Data visualization&lt;/li&gt;
&lt;li&gt;Shiny applications&lt;/li&gt;
&lt;li&gt;Web scrapping&lt;/li&gt;
&lt;li&gt;etc …&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these articles will include R code and examples.
Python users won’t be left aside as I am currently learning this language!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
